{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f50f6f3",
   "metadata": {},
   "source": [
    "This is a test line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e7d62",
   "metadata": {
    "id": "8f4e7d62"
   },
   "source": [
    "# Doctor-Patient ASR Baseline Model\n",
    "\n",
    "This notebook implements a baseline ASR model for doctor-patient conversations using the Hugging Face dataset `Shamus/United-Syn-Med`.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Data Loading**: Stream subset from HF dataset\n",
    "2. **Preprocessing**: Audio resampling + feature extraction\n",
    "3. **Training**: Fine-tune Whisper Small/Tiny\n",
    "4. **Evaluation**: Compute WER metrics\n",
    "5. **Inference**: Test on validation samples\n",
    "\n",
    "Target: ~2000 samples (~500MB) for baseline prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece06a5",
   "metadata": {
    "id": "0ece06a5"
   },
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4761114c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-08-18T07:11:48.645002Z",
     "iopub.status.busy": "2025-08-18T07:11:48.644069Z",
     "iopub.status.idle": "2025-08-18T07:11:59.741490Z",
     "shell.execute_reply": "2025-08-18T07:11:59.740689Z",
     "shell.execute_reply.started": "2025-08-18T07:11:48.644953Z"
    },
    "id": "4761114c",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d5921c83-01ce-409d-d7bb-b97a184c755c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
      "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.73.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.26.4)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2.4.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.6.15)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->soundfile) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->soundfile) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->soundfile) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->soundfile) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->soundfile) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install datasets transformers torch torchaudio accelerate evaluate jiwer tensorboard\n",
    "!pip install --upgrade huggingface_hub\n",
    "# Install additional audio dependencies if needed\n",
    "!pip install soundfile librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c6f6bce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T07:11:59.743541Z",
     "iopub.status.busy": "2025-08-18T07:11:59.743234Z",
     "iopub.status.idle": "2025-08-18T07:11:59.751034Z",
     "shell.execute_reply": "2025-08-18T07:11:59.750364Z",
     "shell.execute_reply.started": "2025-08-18T07:11:59.743518Z"
    },
    "id": "0c6f6bce",
    "outputId": "fa45d114-cf21-4282-f37d-c0b04e6a4814",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: openai/whisper-tiny\n",
      "Target samples: 700\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperFeatureExtractor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import evaluate\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'dataset_name': 'Shamus/United-Syn-Med',\n",
    "    'model_name': 'openai/whisper-tiny',  # Change to 'openai/whisper-tiny' for faster testing\n",
    "    'num_samples': 700,\n",
    "    'target_sample_rate': 16000,\n",
    "    'train_split_ratio': 0.8,\n",
    "    'output_dir': './whisper-medical-asr',\n",
    "    'max_audio_length': 30.0,  # seconds\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 3,\n",
    "    'learning_rate': 1e-5,\n",
    "    'warmup_steps': 500,\n",
    "    'eval_steps': 500,\n",
    "    'save_steps': 1000,\n",
    "    'gradient_accumulation_steps': 2\n",
    "}\n",
    "\n",
    "print(f\"Using model: {CONFIG['model_name']}\")\n",
    "print(f\"Target samples: {CONFIG['num_samples']}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dded15",
   "metadata": {
    "id": "21dded15"
   },
   "source": [
    "## Module 1: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10fc4022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T07:11:59.752291Z",
     "iopub.status.busy": "2025-08-18T07:11:59.752039Z",
     "iopub.status.idle": "2025-08-18T07:11:59.779183Z",
     "shell.execute_reply": "2025-08-18T07:11:59.778537Z",
     "shell.execute_reply.started": "2025-08-18T07:11:59.752269Z"
    },
    "id": "10fc4022",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Handles loading and streaming of the medical conversation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_name: str, num_samples: int, train_split_ratio: float = 0.8):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.num_samples = num_samples\n",
    "        self.train_split_ratio = train_split_ratio\n",
    "        self.raw_data = None\n",
    "\n",
    "    def load_dataset_subset(self) -> List[Dict]:\n",
    "        \"\"\"Load a subset of the dataset using direct parquet access to avoid audio decoding issues.\"\"\"\n",
    "        print(f\"Loading {self.num_samples} samples from {self.dataset_name}...\")\n",
    "\n",
    "        try:\n",
    "            # Method 1: Direct parquet loading to avoid audio decoding\n",
    "            print(\"Attempting direct parquet loading...\")\n",
    "            samples = self._load_from_parquet()\n",
    "            if samples:\n",
    "                self.raw_data = samples\n",
    "                return samples\n",
    "        except Exception as e:\n",
    "            print(f\"Parquet loading failed: {e}\")\n",
    "\n",
    "        try:\n",
    "            # Method 2: Use datasets library but process samples carefully\n",
    "            print(\"Attempting careful streaming with manual audio handling...\")\n",
    "            samples = self._load_with_manual_processing()\n",
    "            if samples:\n",
    "                self.raw_data = samples\n",
    "                return samples\n",
    "        except Exception as e:\n",
    "            print(f\"Manual processing failed: {e}\")\n",
    "\n",
    "        raise Exception(\"All loading methods failed\")\n",
    "\n",
    "    def _load_from_parquet(self) -> List[Dict]:\n",
    "        \"\"\"Load dataset directly from parquet files.\"\"\"\n",
    "        import pandas as pd\n",
    "        from huggingface_hub import list_repo_files\n",
    "\n",
    "        # Get parquet files\n",
    "        files = list_repo_files(self.dataset_name, repo_type=\"dataset\")\n",
    "        parquet_files = [f for f in files if f.endswith('.parquet') and 'train' in f]\n",
    "\n",
    "        if not parquet_files:\n",
    "            raise Exception(\"No parquet files found\")\n",
    "\n",
    "        print(f\"Found {len(parquet_files)} parquet files\")\n",
    "\n",
    "        samples = []\n",
    "        samples_per_file = self.num_samples // len(parquet_files) + 1\n",
    "\n",
    "        for file_idx, file_path in enumerate(parquet_files):\n",
    "            if len(samples) >= self.num_samples:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                print(f\"Loading from {file_path}...\")\n",
    "                file_url = f\"https://huggingface.co/datasets/{self.dataset_name}/resolve/main/{file_path}\"\n",
    "                df = pd.read_parquet(file_url, engine='pyarrow')\n",
    "\n",
    "                # Take only the samples we need from this file\n",
    "                remaining_samples = self.num_samples - len(samples)\n",
    "                df_subset = df.head(min(samples_per_file, remaining_samples))\n",
    "\n",
    "                for _, row in df_subset.iterrows():\n",
    "                    if len(samples) >= self.num_samples:\n",
    "                        break\n",
    "\n",
    "                    # Convert row to dict and handle audio separately\n",
    "                    sample = {}\n",
    "                    for col, value in row.items():\n",
    "                        if col == 'audio':\n",
    "                            # Keep audio as raw data structure\n",
    "                            if isinstance(value, dict):\n",
    "                                sample[col] = value\n",
    "                            else:\n",
    "                                # If it's a different format, create a dict structure\n",
    "                                sample[col] = {'bytes': value, 'path': None, 'sampling_rate': 16000}\n",
    "                        elif col == 'transcription':\n",
    "                            # Map transcription to text field\n",
    "                            sample['text'] = str(value) if value is not None else \"\"\n",
    "                        else:\n",
    "                            sample[col] = value\n",
    "\n",
    "                    # Ensure we have a text field\n",
    "                    if 'text' not in sample:\n",
    "                        sample['text'] = \"\"\n",
    "\n",
    "                    samples.append(sample)\n",
    "\n",
    "                print(f\"Loaded {len(samples)} samples so far...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def _load_with_manual_processing(self) -> List[Dict]:\n",
    "        \"\"\"Fallback method using datasets library with careful processing.\"\"\"\n",
    "        # Try loading without any schema constraints\n",
    "        dataset_stream = load_dataset(\n",
    "            self.dataset_name,\n",
    "            split='train',\n",
    "            streaming=True\n",
    "        )\n",
    "\n",
    "        samples = []\n",
    "        error_count = 0\n",
    "        max_errors = 10  # Allow some errors before giving up\n",
    "\n",
    "        iterator = iter(dataset_stream)\n",
    "\n",
    "        while len(samples) < self.num_samples and error_count < max_errors:\n",
    "            try:\n",
    "                sample = next(iterator)\n",
    "\n",
    "                # Process sample carefully\n",
    "                processed_sample = {}\n",
    "\n",
    "                for key, value in sample.items():\n",
    "                    if key == 'audio':\n",
    "                        # Try to keep audio without triggering decoding\n",
    "                        if hasattr(value, 'keys') and callable(getattr(value, 'keys')):\n",
    "                            # It's dict-like, extract raw data\n",
    "                            processed_sample[key] = {\n",
    "                                'bytes': getattr(value, 'bytes', None),\n",
    "                                'path': getattr(value, 'path', None),\n",
    "                                'sampling_rate': getattr(value, 'sampling_rate', 16000)\n",
    "                            }\n",
    "                        else:\n",
    "                            # Store as-is and hope for the best\n",
    "                            processed_sample[key] = value\n",
    "                    elif key == 'transcription':\n",
    "                        processed_sample['text'] = str(value) if value is not None else \"\"\n",
    "                    else:\n",
    "                        processed_sample[key] = value\n",
    "\n",
    "                # Ensure text field exists\n",
    "                if 'text' not in processed_sample:\n",
    "                    processed_sample['text'] = \"\"\n",
    "\n",
    "                samples.append(processed_sample)\n",
    "\n",
    "                if (len(samples) + 1) % 100 == 0:\n",
    "                    print(f\"Loaded {len(samples)} samples...\")\n",
    "\n",
    "            except StopIteration:\n",
    "                print(\"Reached end of dataset\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"Error processing sample {len(samples) + error_count}: {e}\")\n",
    "                if error_count >= max_errors:\n",
    "                    print(f\"Too many errors ({error_count}), stopping...\")\n",
    "                    break\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def create_train_val_split(self, samples: List[Dict]) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"Split the data into train and validation sets.\"\"\"\n",
    "        split_idx = int(len(samples) * self.train_split_ratio)\n",
    "\n",
    "        train_samples = samples[:split_idx]\n",
    "        val_samples = samples[split_idx:]\n",
    "\n",
    "        # Convert to HF datasets\n",
    "        train_dataset = Dataset.from_list(train_samples)\n",
    "        val_dataset = Dataset.from_list(val_samples)\n",
    "\n",
    "        print(f\"Train samples: {len(train_dataset)}\")\n",
    "        print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def inspect_sample(self, idx: int = 0) -> None:\n",
    "        \"\"\"Inspect a sample to understand the data structure.\"\"\"\n",
    "        if self.raw_data is None:\n",
    "            print(\"No data loaded. Run load_dataset_subset() first.\")\n",
    "            return\n",
    "\n",
    "        sample = self.raw_data[idx]\n",
    "        print(f\"Sample {idx} structure:\")\n",
    "        for key, value in sample.items():\n",
    "            if key == 'audio':\n",
    "                if isinstance(value, dict):\n",
    "                    print(f\"  {key}: dict with keys {list(value.keys())}\")\n",
    "                    for sub_key, sub_value in value.items():\n",
    "                        if sub_key == 'bytes' and sub_value is not None:\n",
    "                            print(f\"    {sub_key}: {len(sub_value)} bytes\")\n",
    "                        else:\n",
    "                            print(f\"    {sub_key}: {sub_value}\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {type(value)}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {str(value)[:100]}...\" if len(str(value)) > 100 else f\"  {key}: {value}\")\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(\n",
    "    dataset_name=CONFIG['dataset_name'],\n",
    "    num_samples=CONFIG['num_samples'],\n",
    "    train_split_ratio=CONFIG['train_split_ratio']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "OSgb7prEDy_O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T07:11:59.781015Z",
     "iopub.status.busy": "2025-08-18T07:11:59.780776Z",
     "iopub.status.idle": "2025-08-18T07:12:02.849742Z",
     "shell.execute_reply": "2025-08-18T07:12:02.849036Z",
     "shell.execute_reply.started": "2025-08-18T07:11:59.780996Z"
    },
    "id": "OSgb7prEDy_O",
    "outputId": "fbd15a9e-877a-4b8f-afe7-4c27badc571a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset structure safely...\n",
      "Examining dataset structure safely...\n",
      "Error getting dataset info: cannot import name 'get_dataset_infos' from 'datasets.utils.info_utils' (/usr/local/lib/python3.11/dist-packages/datasets/utils/info_utils.py)\n",
      "Trying direct parquet approach...\n",
      "Found 33 training parquet files\n",
      "Examining first file: data/train-00000-of-00033.parquet\n",
      "Parquet file structure:\n",
      "Columns: ['audio', 'transcription']\n",
      "Shape: (3, 2)\n",
      "  audio: object\n",
      "  transcription: object\n",
      "    Sample value: Durysta is a medication used to reduce eye pressure in patients with open-angle glaucoma or ocular h...\n"
     ]
    }
   ],
   "source": [
    "# Test cell to examine the original dataset structure without triggering audio decoding\n",
    "def check_dataset_structure_safe():\n",
    "    \"\"\"Check the actual structure of the dataset without triggering audio decoding.\"\"\"\n",
    "    print(\"Examining dataset structure safely...\")\n",
    "\n",
    "    try:\n",
    "        # Load dataset info without streaming to avoid audio decoding\n",
    "        from datasets import get_dataset_config_names, get_dataset_split_names\n",
    "        from datasets.utils.info_utils import get_dataset_infos\n",
    "\n",
    "        print(\"Dataset configs:\", get_dataset_config_names(CONFIG['dataset_name']))\n",
    "        print(\"Dataset splits:\", get_dataset_split_names(CONFIG['dataset_name']))\n",
    "\n",
    "        # Try to get dataset info\n",
    "        dataset_infos = get_dataset_infos(CONFIG['dataset_name'])\n",
    "        print(\"Dataset info keys:\", list(dataset_infos.keys()))\n",
    "\n",
    "        # If we have default config, print its features\n",
    "        if 'default' in dataset_infos:\n",
    "            features = dataset_infos['default'].features\n",
    "            print(\"Dataset features:\")\n",
    "            for name, feature in features.items():\n",
    "                print(f\"  {name}: {feature}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting dataset info: {e}\")\n",
    "\n",
    "        # Alternative: try loading parquet files directly\n",
    "        try:\n",
    "            print(\"Trying direct parquet approach...\")\n",
    "            import pandas as pd\n",
    "            from huggingface_hub import list_repo_files\n",
    "\n",
    "            # List files in the repository\n",
    "            files = list_repo_files(CONFIG['dataset_name'], repo_type=\"dataset\")\n",
    "            parquet_files = [f for f in files if f.endswith('.parquet') and 'train' in f]\n",
    "            print(f\"Found {len(parquet_files)} training parquet files\")\n",
    "\n",
    "            if parquet_files:\n",
    "                # Load just the first few rows of the first parquet file to check structure\n",
    "                first_file = parquet_files[0]\n",
    "                print(f\"Examining first file: {first_file}\")\n",
    "\n",
    "                # Load parquet file directly\n",
    "                file_url = f\"https://huggingface.co/datasets/{CONFIG['dataset_name']}/resolve/main/{first_file}\"\n",
    "                df_sample = pd.read_parquet(file_url, engine='pyarrow').head(3)\n",
    "\n",
    "                print(\"Parquet file structure:\")\n",
    "                print(f\"Columns: {list(df_sample.columns)}\")\n",
    "                print(f\"Shape: {df_sample.shape}\")\n",
    "\n",
    "                for col in df_sample.columns:\n",
    "                    print(f\"  {col}: {df_sample[col].dtype}\")\n",
    "                    if col != 'audio':  # Skip audio column to avoid issues\n",
    "                        print(f\"    Sample value: {str(df_sample[col].iloc[0])[:100]}...\")\n",
    "\n",
    "                return df_sample\n",
    "\n",
    "        except Exception as e2:\n",
    "            print(f\"Parquet approach also failed: {e2}\")\n",
    "            return None\n",
    "\n",
    "# Run the safe structure check\n",
    "print(\"Checking dataset structure safely...\")\n",
    "sample_structure = check_dataset_structure_safe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caa97aeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T07:12:02.850944Z",
     "iopub.status.busy": "2025-08-18T07:12:02.850737Z",
     "iopub.status.idle": "2025-08-18T07:13:18.960488Z",
     "shell.execute_reply": "2025-08-18T07:13:18.959670Z",
     "shell.execute_reply.started": "2025-08-18T07:12:02.850928Z"
    },
    "id": "caa97aeb",
    "outputId": "5866c26e-9821-460b-e55d-32ea4304fb7e",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 700 samples from Shamus/United-Syn-Med...\n",
      "Attempting direct parquet loading...\n",
      "Found 33 parquet files\n",
      "Loading from data/train-00000-of-00033.parquet...\n",
      "Loaded 22 samples so far...\n",
      "Loading from data/train-00001-of-00033.parquet...\n",
      "Loaded 44 samples so far...\n",
      "Loading from data/train-00002-of-00033.parquet...\n",
      "Loaded 66 samples so far...\n",
      "Loading from data/train-00003-of-00033.parquet...\n",
      "Loaded 88 samples so far...\n",
      "Loading from data/train-00004-of-00033.parquet...\n",
      "Loaded 110 samples so far...\n",
      "Loading from data/train-00005-of-00033.parquet...\n",
      "Loaded 132 samples so far...\n",
      "Loading from data/train-00006-of-00033.parquet...\n",
      "Loaded 154 samples so far...\n",
      "Loading from data/train-00007-of-00033.parquet...\n",
      "Loaded 176 samples so far...\n",
      "Loading from data/train-00008-of-00033.parquet...\n",
      "Loaded 198 samples so far...\n",
      "Loading from data/train-00009-of-00033.parquet...\n",
      "Loaded 220 samples so far...\n",
      "Loading from data/train-00010-of-00033.parquet...\n",
      "Loaded 242 samples so far...\n",
      "Loading from data/train-00011-of-00033.parquet...\n",
      "Loaded 264 samples so far...\n",
      "Loading from data/train-00012-of-00033.parquet...\n",
      "Loaded 286 samples so far...\n",
      "Loading from data/train-00013-of-00033.parquet...\n",
      "Loaded 308 samples so far...\n",
      "Loading from data/train-00014-of-00033.parquet...\n",
      "Loaded 330 samples so far...\n",
      "Loading from data/train-00015-of-00033.parquet...\n",
      "Loaded 352 samples so far...\n",
      "Loading from data/train-00016-of-00033.parquet...\n",
      "Loaded 374 samples so far...\n",
      "Loading from data/train-00017-of-00033.parquet...\n",
      "Loaded 396 samples so far...\n",
      "Loading from data/train-00018-of-00033.parquet...\n",
      "Loaded 418 samples so far...\n",
      "Loading from data/train-00019-of-00033.parquet...\n",
      "Loaded 440 samples so far...\n",
      "Loading from data/train-00020-of-00033.parquet...\n",
      "Loaded 462 samples so far...\n",
      "Loading from data/train-00021-of-00033.parquet...\n",
      "Loaded 484 samples so far...\n",
      "Loading from data/train-00022-of-00033.parquet...\n",
      "Loaded 506 samples so far...\n",
      "Loading from data/train-00023-of-00033.parquet...\n",
      "Loaded 528 samples so far...\n",
      "Loading from data/train-00024-of-00033.parquet...\n",
      "Loaded 550 samples so far...\n",
      "Loading from data/train-00025-of-00033.parquet...\n",
      "Loaded 572 samples so far...\n",
      "Loading from data/train-00026-of-00033.parquet...\n",
      "Loaded 594 samples so far...\n",
      "Loading from data/train-00027-of-00033.parquet...\n",
      "Loaded 616 samples so far...\n",
      "Loading from data/train-00028-of-00033.parquet...\n",
      "Loaded 638 samples so far...\n",
      "Loading from data/train-00029-of-00033.parquet...\n",
      "Loaded 660 samples so far...\n",
      "Loading from data/train-00030-of-00033.parquet...\n",
      "Loaded 682 samples so far...\n",
      "Loading from data/train-00031-of-00033.parquet...\n",
      "Loaded 700 samples so far...\n",
      "Sample 0 structure:\n",
      "  audio: dict with keys ['bytes', 'path']\n",
      "    bytes: 65613 bytes\n",
      "    path: drug-female-defa7fcb-89d7-4b25-8834-90888b201d25.mp3\n",
      "  text: Durysta is a medication used to reduce eye pressure in patients with open-angle glaucoma or ocular h...\n",
      "\n",
      "==================================================\n",
      "Testing audio decoding...\n",
      "Audio stored as bytes - decoding...\n",
      "Successfully decoded audio: shape=torch.Size([1, 178150]), sample_rate=24000\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset subset\n",
    "samples = data_loader.load_dataset_subset()\n",
    "\n",
    "# Inspect a sample to understand the structure\n",
    "data_loader.inspect_sample(0)\n",
    "\n",
    "# Test audio decoding on the first sample\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing audio decoding...\")\n",
    "try:\n",
    "    first_sample = samples[0]\n",
    "    audio_info = first_sample['audio']\n",
    "\n",
    "    # Try to decode the audio manually\n",
    "    import io\n",
    "    if 'bytes' in audio_info and audio_info['bytes'] is not None:\n",
    "        print(\"Audio stored as bytes - decoding...\")\n",
    "        audio_bytes = audio_info['bytes']\n",
    "        audio_file = io.BytesIO(audio_bytes)\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        print(f\"Successfully decoded audio: shape={waveform.shape}, sample_rate={sample_rate}\")\n",
    "    elif 'path' in audio_info:\n",
    "        print(f\"Audio path: {audio_info['path']}\")\n",
    "        waveform, sample_rate = torchaudio.load(audio_info['path'])\n",
    "        print(f\"Successfully loaded audio: shape={waveform.shape}, sample_rate={sample_rate}\")\n",
    "    else:\n",
    "        print(f\"Unexpected audio format: {audio_info}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error decoding audio: {e}\")\n",
    "    print(\"This might indicate an issue with the audio format.\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f518969",
   "metadata": {
    "id": "5f518969"
   },
   "source": [
    "## Module 2: Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3af9c017",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308,
     "referenced_widgets": [
      "0dddc4553e0549db97f215be1b53c51f",
      "ec4dcc28a67546ee8265ff2dcf24b2b6",
      "0f4671829e9149318d030838ca788766",
      "8c8e25c295f34225933d590ecf002f3a",
      "b713465adfce4da58ab44cdbbfe11e85",
      "a3ee6162d2134f53b95f156fc645963a",
      "709f36990648403f82bb2d8012fa005d",
      "af32c32d09104beea9d774e82b6b1fdb",
      "28cf4d3041b44138b05fde3a6c0c71db",
      "3d7cec6d6c30410aaa06cd380062c9a7",
      "10f7ee7868da4ec0ab1e57342d8abf1f",
      "c9b27b25160b40a589b20dcfc2d38de6",
      "927848144429471abfaa5671841c520e",
      "4d63aff65c524fbfbc2d35081a29f5d9",
      "9685febcf74c401db8e24574b84af2b3",
      "aab1012473c840ccb99b5508c1626d88",
      "a2fd054a4e0945f8a714d69cb61bb674",
      "adbcba351b8349bc9293ba950bfc6d59",
      "fc33cb13dd604d74a9b4023b133cb4da",
      "a8102832b23944f58faed6032e863aea",
      "93d4de194b0146e0bfbf1f9203a8efbc",
      "11d2d86b57c84318827ea42aa4b59b8f",
      "efb34333e20c4b719b6d30668c6952f1",
      "02317272e64c4fce899c59e381ee0690",
      "150f986d62bb41c9a56fd38e4c954020",
      "83ace04551a745a68f33c9c1105e3b01",
      "e81b51ef0132489a8d8c73fbe25d4692",
      "2170942122d74128ba878bbc098a20cd",
      "78f8f798727b4556ab94a40afe3702f7",
      "84c50442ccbe41868faad529f5bb2f3e",
      "7696c8d022c8457fba0fdaa1cdfd411d",
      "3e8dd3c7670340ad944cea975f7bc3bb",
      "3cb416067c7443e8b745e61d8d2e76dd",
      "252a6ceac04d4dadacb05eb742067031",
      "79018e6e9f2745749ad812a6f6713bc3",
      "e31be64fc0104355a83f09cc6a0031f1",
      "f8e29ea23cd64bb4a5c6d03ca2576fde",
      "d5300ea0814440b79fbf23f489a758ed",
      "61e3e744b3b44bd9bea0de1d8bd3bdcb",
      "275da4af86f6427d844bbace06cb7bf8",
      "6df02e70d3b04da8b134e4a26b657152",
      "1867ff7ca04e4a54987eefcf6cf7ec9b",
      "ea9152547d904d11ab2aeec6db3fdc89",
      "5e9ec97932474f0882e7e9300115695f",
      "8128a80de9714b66bba0f79c0c940071",
      "02935574cb384d6b84fad32f3d93f4c8",
      "d3414bec85964ba8a9a5270fd09dced7",
      "a40c1fab1563407b904db049f9450c0e",
      "94cd766edca34b319e87de8dfc7044b2",
      "e59c3e69e09f420cb6e1b80ffe223136",
      "01b6552018cc425ead85eae0e17fda06",
      "dc2c4395392b474f93e941b352e38ca4",
      "d4adf3b026b44ec789233e36a0f0c64c",
      "84cd0cb9060744f1892d1efd4bd7adbd",
      "eb472304e4e942c091f4230917bd4e9c",
      "05f78daa68a04dd4a2b4a84e5ccfebd3",
      "56750607047846f593672599988e32dc",
      "98f6bd36ee97497bbd9b7e006b10b538",
      "60fa01c6a764403aa74defec1f0c2bd0",
      "3c2dbfa45e0b4d9b9303621f433f6165",
      "f1cf0f30095741ff910247f5a84be592",
      "358261fd07324927b6a7455df91e736b",
      "78bf40d6a36e4ae4ac35990aa44f5d9a",
      "294e60cd05a04286a238ce7282503b9a",
      "8148517597314edb9ca2ab6179877954",
      "9dbc5ba6a7e64e2f84bb3613c0a38c63",
      "334ccf4dec2b47ffa3297075d90f9190",
      "a04d448a56e749b2a3fb7f193a22bcd2",
      "775960d2e6934bd899e40fad5e93a148",
      "7bae267dec7a4c9687fb19d6cd903e33",
      "8157314a37e54b3facfcc0a44142806f",
      "2234a1a88efc4d5fae411866a3adf66e",
      "6ba64c270f0f4f7ab02d0471153ff58a",
      "7690e3a451944633aeb89a3bc25a809d",
      "52587a0ecb9d4f16b96a5e284c5c1c7c",
      "b69023098a164d1a8d1e5628b01a09ec",
      "7923ada824f343f8a26d2fe5cd3999dd",
      "f77ba27784944863819ba100c650f793",
      "65bfd81001294311a03d464b29987164",
      "d37a192fc6274871b0c2230dab76db86",
      "948d4af818ce4da0a4499a758aeccc7f",
      "efabae23841d4f08a97feeed631c6ed8",
      "1b198dd6dd524741abe5df656b99a309",
      "4467cacde89c470ba36ceb991ccf804b",
      "62a8e5ea484341a289feab09fe608859",
      "8179b13c47ba46feb3b9cb009740587c",
      "bfde110d84074490891834a0bda7f1d3",
      "c23e74984a7549199308b25d6721d27d"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T07:13:18.961930Z",
     "iopub.status.busy": "2025-08-18T07:13:18.961593Z",
     "iopub.status.idle": "2025-08-18T07:13:20.103295Z",
     "shell.execute_reply": "2025-08-18T07:13:20.102482Z",
     "shell.execute_reply.started": "2025-08-18T07:13:18.961904Z"
    },
    "id": "3af9c017",
    "outputId": "15c2ae14-8a13-4fd8-e79f-f778f31fd26b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper processor for openai/whisper-tiny...\n",
      "Processor loaded. Target sample rate: 16000Hz\n"
     ]
    }
   ],
   "source": [
    "class AudioPreprocessor:\n",
    "    \"\"\"Handles audio preprocessing for Whisper model.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, target_sample_rate: int = 16000, max_audio_length: float = 30.0):\n",
    "        self.model_name = model_name\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.max_audio_length = max_audio_length\n",
    "\n",
    "        # Initialize Whisper components\n",
    "        print(f\"Loading Whisper processor for {model_name}...\")\n",
    "        self.processor = WhisperProcessor.from_pretrained(model_name)\n",
    "        self.feature_extractor = self.processor.feature_extractor\n",
    "        self.tokenizer = self.processor.tokenizer\n",
    "\n",
    "        print(f\"Processor loaded. Target sample rate: {self.target_sample_rate}Hz\")\n",
    "\n",
    "    def decode_audio_bytes(self, audio_info: Dict) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"Decode audio from bytes using torchaudio.\"\"\"\n",
    "        import io\n",
    "\n",
    "        if 'bytes' in audio_info and audio_info['bytes'] is not None:\n",
    "            # Load audio from bytes\n",
    "            audio_bytes = audio_info['bytes']\n",
    "            audio_file = io.BytesIO(audio_bytes)\n",
    "\n",
    "            # Use torchaudio to load the audio\n",
    "            waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "            # Convert to numpy and handle multi-channel audio\n",
    "            audio_array = waveform.numpy()\n",
    "            if len(audio_array.shape) > 1:\n",
    "                # Convert to mono by averaging channels\n",
    "                audio_array = np.mean(audio_array, axis=0)\n",
    "\n",
    "            return audio_array, sample_rate\n",
    "\n",
    "        elif 'path' in audio_info and audio_info['path'] is not None:\n",
    "            # Load audio from file path\n",
    "            waveform, sample_rate = torchaudio.load(audio_info['path'])\n",
    "\n",
    "            # Convert to numpy and handle multi-channel audio\n",
    "            audio_array = waveform.numpy()\n",
    "            if len(audio_array.shape) > 1:\n",
    "                # Convert to mono by averaging channels\n",
    "                audio_array = np.mean(audio_array, axis=0)\n",
    "\n",
    "            return audio_array, sample_rate\n",
    "\n",
    "        elif 'array' in audio_info:\n",
    "            # Already decoded audio\n",
    "            return np.array(audio_info['array']), audio_info.get('sampling_rate', 16000)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported audio format: {audio_info.keys()}\")\n",
    "\n",
    "    def resample_audio(self, audio_array: np.ndarray, original_sr: int) -> np.ndarray:\n",
    "        \"\"\"Resample audio to target sample rate.\"\"\"\n",
    "        if original_sr == self.target_sample_rate:\n",
    "            return audio_array\n",
    "\n",
    "        # Convert to tensor for resampling\n",
    "        audio_tensor = torch.from_numpy(audio_array).float()\n",
    "        if len(audio_tensor.shape) == 1:\n",
    "            audio_tensor = audio_tensor.unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        # Resample\n",
    "        resampler = torchaudio.transforms.Resample(\n",
    "            orig_freq=original_sr,\n",
    "            new_freq=self.target_sample_rate\n",
    "        )\n",
    "        resampled = resampler(audio_tensor)\n",
    "\n",
    "        return resampled.squeeze().numpy()\n",
    "\n",
    "    def trim_or_pad_audio(self, audio_array: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Trim or pad audio to max length.\"\"\"\n",
    "        max_samples = int(self.max_audio_length * self.target_sample_rate)\n",
    "\n",
    "        if len(audio_array) > max_samples:\n",
    "            # Trim to max length\n",
    "            return audio_array[:max_samples]\n",
    "        elif len(audio_array) < max_samples:\n",
    "            # Pad with zeros\n",
    "            padding = max_samples - len(audio_array)\n",
    "            return np.pad(audio_array, (0, padding), mode='constant')\n",
    "        else:\n",
    "            return audio_array\n",
    "\n",
    "    def preprocess_batch(self, batch: Dict) -> Dict:\n",
    "        \"\"\"Preprocess a batch of audio samples.\"\"\"\n",
    "        # Extract audio data\n",
    "        audio_data = []\n",
    "        texts = []\n",
    "\n",
    "        for i in range(len(batch['audio'])):\n",
    "            try:\n",
    "                # Handle audio - decode from bytes/path\n",
    "                audio_info = batch['audio'][i]\n",
    "                audio_array, sampling_rate = self.decode_audio_bytes(audio_info)\n",
    "\n",
    "                # Preprocess audio\n",
    "                audio_array = self.resample_audio(audio_array, sampling_rate)\n",
    "                audio_array = self.trim_or_pad_audio(audio_array)\n",
    "                audio_data.append(audio_array)\n",
    "\n",
    "                # Handle text (check multiple possible field names)\n",
    "                text_field = \"\"\n",
    "                for field in ['text', 'transcription', 'sentence', 'transcript']:\n",
    "                    if field in batch and i < len(batch[field]):\n",
    "                        text_field = batch[field][i]\n",
    "                        break\n",
    "\n",
    "                if text_field is None:\n",
    "                    text_field = \"\"  # Empty fallback\n",
    "                texts.append(str(text_field))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i}: {e}\")\n",
    "                # Skip this sample or use empty data\n",
    "                audio_data.append(np.zeros(int(self.max_audio_length * self.target_sample_rate)))\n",
    "                texts.append(\"\")\n",
    "\n",
    "        # Process with Whisper feature extractor\n",
    "        features = self.feature_extractor(\n",
    "            audio_data,\n",
    "            sampling_rate=self.target_sample_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        # Tokenize texts with proper padding and truncation for Whisper\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                texts,\n",
    "                max_length=448,  # Whisper's max sequence length\n",
    "                padding=\"max_length\",  # Force consistent padding\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids\n",
    "\n",
    "        # Replace padding token id with -100 so it's ignored in loss computation\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_features\": features[\"input_features\"],\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = AudioPreprocessor(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    target_sample_rate=CONFIG['target_sample_rate'],\n",
    "    max_audio_length=CONFIG['max_audio_length']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5GQv7CKJHsJ",
   "metadata": {
    "id": "a5GQv7CKJHsJ",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc0790e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202,
     "referenced_widgets": [
      "64b72284dc7c4fab9eff625817401828",
      "6abb6151ab454aac9762bc6c1917373d",
      "9a7218b02f924029abb0b5e22afde9ed",
      "32943b81e0b64b3fa2af1f4f66f781ec",
      "421a046bb06544bfb3641e1747922e7d",
      "339ad729a3e4445e8524fa66f7aabb45",
      "e936d01f314648e098bee0a9e133ec7f",
      "c9b48d3f09d04d219176844fe1ee9046",
      "1a6b38aca0ce4adb9898d9cac042cf11",
      "d5d8385b58774cbea455b378a9186443",
      "353f3db573e64d238eaacb91bae1c5fa",
      "7693ad9de082400ea858866c77f46f2b",
      "019f56acf34344aebdfa7b3ee15f3261",
      "dbe116372cfa439083b6d44863e50453",
      "7946a35895da4b3ea78361d84eb91cda",
      "848b664d20a4476da239f1e6b5fbf8a7",
      "0be4520c604f4c938ae07d026db67fb1",
      "d576293895f24082a00b76570baa567c",
      "65f2832bce7748b986129a9361d4af4d",
      "c5ce27ea0339491da34eec3f9874ca03",
      "77159aaec226492fbf83cd80f4726dcf",
      "6a775153f0ea4c42b9c1b36476ba882b"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T07:13:20.104737Z",
     "iopub.status.busy": "2025-08-18T07:13:20.104475Z",
     "iopub.status.idle": "2025-08-18T07:13:40.336119Z",
     "shell.execute_reply": "2025-08-18T07:13:40.335277Z",
     "shell.execute_reply.started": "2025-08-18T07:13:20.104719Z"
    },
    "id": "dc0790e4",
    "outputId": "af248215-b7eb-4d8b-a955-37b4cd757546",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 560\n",
      "Validation samples: 140\n",
      "Preprocessing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acec549fd8f4bf4a699ed479a2bc6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/560 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187b524fd7ee469b8d243f40bea08377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n",
      "Train dataset: 560 samples\n",
      "Val dataset: 140 samples\n"
     ]
    }
   ],
   "source": [
    "# Create train/val splits\n",
    "train_dataset, val_dataset = data_loader.create_train_val_split(samples)\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "print(\"Preprocessing training data...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocessor.preprocess_batch,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "val_dataset = val_dataset.map(\n",
    "    preprocessor.preprocess_batch,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing complete!\")\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22094e74",
   "metadata": {
    "id": "22094e74"
   },
   "source": [
    "## Module 3: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c17d828d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148,
     "referenced_widgets": [
      "10d4f72e2f0e4bb3a7263a1ef307030b",
      "c8da8278a4124057bf6ff7bb351210d2",
      "45a9e6e85f544d6abaf81437e78c741f",
      "dcc2ec5b37fa4b80b0c72f24ab5bcf72",
      "8faef5c828b64c2fbf5ed3d30a73a8cd",
      "6610977290f54365997586d6567e668c",
      "7f840f99840c41aab032edaba86bdb40",
      "513aeb3eb72c455d88f296a3303fe51d",
      "a4eae7a0e44f4395871ef9c9ecdba3a3",
      "0365c5e1bbaa44acbb077c5bdf7461fd",
      "a802a4cbe23e4938b13e783f737670df",
      "61530ed0b77a4e5f9c26be8cb759d864",
      "981c5744d82145d39d3dabb88cdf4333",
      "3afa3942bd924152bad0c3dfd3e5dd3f",
      "4ae671de23f3441887cbc7a3953f9c90",
      "75c4e30b3a5643688941d2698f81ed29",
      "60a5b628ad514e1c97f9ca5728fa6dbf",
      "475528ff173f47858b79c97a5a1b15e8",
      "13d1c6d2eeef46f79ca57f6fc4acb0be",
      "8074c6f81d0f403d8dd06b6fc4a7bf83",
      "0cfcea8a22eb4a0d986f1da6e0257ce0",
      "eb0ba123434940768511f97b24d9a65d",
      "a95bc212127f44a2a4462e053b2dd42e",
      "f462f7d962d5405d864068efc8ba384a",
      "334b8fe0a13247578e17e4b56a852122",
      "a36d9401c10b4cd5920cca6979a6b4e8",
      "4a9a9f646b544aa09da77866cf344d2b",
      "66c1cb15102b4d278c01b71f189152be",
      "58fdbfad90444f7e9575d22d25f0b9a1",
      "df43c75037a046f9a64187ba1c94d926",
      "7966c6cded3742a885c79e635f99fb75",
      "9c054219d6ff439e850de50206933433",
      "6e2fdc7055a24ec68ba5d68b60530fa4"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T07:13:40.337648Z",
     "iopub.status.busy": "2025-08-18T07:13:40.337136Z",
     "iopub.status.idle": "2025-08-18T07:13:41.332099Z",
     "shell.execute_reply": "2025-08-18T07:13:41.331321Z",
     "shell.execute_reply.started": "2025-08-18T07:13:40.337621Z"
    },
    "id": "c17d828d",
    "outputId": "3d8873a9-e8fa-45a5-aa31-cb7edb86dea0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model: openai/whisper-tiny...\n",
      "Model loaded. Parameters: 37,760,640\n"
     ]
    }
   ],
   "source": [
    "class WhisperTrainer:\n",
    "    \"\"\"Handles Whisper model training and fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, output_dir: str):\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        # Load model and processor\n",
    "        print(f\"Loading Whisper model: {model_name}...\")\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "        # Force decoder to use correct language tokens\n",
    "        self.model.config.forced_decoder_ids = None\n",
    "        self.model.config.suppress_tokens = []\n",
    "\n",
    "        print(f\"Model loaded. Parameters: {self.model.num_parameters():,}\")\n",
    "\n",
    "    def create_data_collator(self):\n",
    "        \"\"\"Create data collator for training.\"\"\"\n",
    "        # Use DefaultDataCollator since we're handling padding in preprocessing\n",
    "        from transformers import DefaultDataCollator\n",
    "        return DefaultDataCollator()\n",
    "\n",
    "    def setup_training_args(self, config: Dict) -> TrainingArguments:\n",
    "        \"\"\"Setup training arguments.\"\"\"\n",
    "        return TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            per_device_train_batch_size=config['batch_size'],\n",
    "            per_device_eval_batch_size=config['batch_size'],\n",
    "            gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
    "            learning_rate=config['learning_rate'],\n",
    "            warmup_steps=config['warmup_steps'],\n",
    "            num_train_epochs=config['num_epochs'],\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=config['eval_steps'],\n",
    "            save_steps=config['save_steps'],\n",
    "            logging_steps=50,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            fp16=True if torch.cuda.is_available() else False,\n",
    "            dataloader_pin_memory=False,\n",
    "            dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "            report_to=[\"tensorboard\"],\n",
    "            push_to_hub=False,\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "\n",
    "    def create_trainer(self, train_dataset: Dataset, val_dataset: Dataset, config: Dict) -> Trainer:\n",
    "        \"\"\"Create and configure the trainer.\"\"\"\n",
    "        training_args = self.setup_training_args(config)\n",
    "        data_collator = self.create_data_collator()\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.processor.tokenizer\n",
    "        )\n",
    "\n",
    "        return trainer\n",
    "\n",
    "    def train(self, train_dataset: Dataset, val_dataset: Dataset, config: Dict):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        print(\"Setting up trainer...\")\n",
    "        trainer = self.create_trainer(train_dataset, val_dataset, config)\n",
    "\n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "\n",
    "        print(\"Saving final model...\")\n",
    "        trainer.save_model()\n",
    "        self.processor.save_pretrained(self.output_dir)\n",
    "\n",
    "        return trainer\n",
    "\n",
    "# Initialize trainer\n",
    "whisper_trainer = WhisperTrainer(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    output_dir=CONFIG['output_dir']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "AD4BqjSoIzOQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T07:13:41.333274Z",
     "iopub.status.busy": "2025-08-18T07:13:41.333053Z",
     "iopub.status.idle": "2025-08-18T07:13:42.029509Z",
     "shell.execute_reply": "2025-08-18T07:13:42.028900Z",
     "shell.execute_reply.started": "2025-08-18T07:13:41.333257Z"
    },
    "id": "AD4BqjSoIzOQ",
    "outputId": "ee1e8467-54d9-4e19-917c-bccc030f7788",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessed data structure...\n",
      "\n",
      "Sample 0 structure:\n",
      "  input_features: <class 'list'>\n",
      "  labels: <class 'list'>\n",
      "\n",
      "Sample 1 structure:\n",
      "  input_features: <class 'list'>\n",
      "  labels: <class 'list'>\n",
      "\n",
      "Sample 2 structure:\n",
      "  input_features: <class 'list'>\n",
      "  labels: <class 'list'>\n",
      "\n",
      "Testing data collator...\n",
      "\n",
      "Collated batch structure:\n",
      "  input_features: tensor with shape torch.Size([3, 80, 3000]), dtype torch.float32\n",
      "  labels: tensor with shape torch.Size([3, 448]), dtype torch.int64\n",
      " Data collation successful!\n",
      "\n",
      " Data preprocessing and collation working correctly!\n",
      "Ready to start training...\n"
     ]
    }
   ],
   "source": [
    "# Test preprocessed data structure\n",
    "def test_preprocessed_data():\n",
    "    \"\"\"Test that the preprocessed data has the correct structure for training.\"\"\"\n",
    "    print(\"Testing preprocessed data structure...\")\n",
    "\n",
    "    # Get a few samples from the training dataset\n",
    "    sample_indices = [0, 1, 2] if len(train_dataset) >= 3 else [0]\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        sample = train_dataset[idx]\n",
    "        print(f\"\\nSample {idx} structure:\")\n",
    "        for key, value in sample.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  {key}: tensor with shape {value.shape}, dtype {value.dtype}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {type(value)}\")\n",
    "\n",
    "    # Test data collator\n",
    "    print(\"\\nTesting data collator...\")\n",
    "    data_collator = whisper_trainer.create_data_collator()\n",
    "\n",
    "    # Try to collate a small batch\n",
    "    batch_samples = [train_dataset[i] for i in range(min(3, len(train_dataset)))]\n",
    "    try:\n",
    "        collated_batch = data_collator(batch_samples)\n",
    "        print(\"\\nCollated batch structure:\")\n",
    "        for key, value in collated_batch.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  {key}: tensor with shape {value.shape}, dtype {value.dtype}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {type(value)}\")\n",
    "        print(\" Data collation successful!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\" Data collation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "test_success = test_preprocessed_data()\n",
    "\n",
    "if test_success:\n",
    "    print(\"\\n Data preprocessing and collation working correctly!\")\n",
    "    print(\"Ready to start training...\")\n",
    "else:\n",
    "    print(\"\\n Issues found with data preprocessing. Please check the errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ceeec91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T07:13:42.031722Z",
     "iopub.status.busy": "2025-08-18T07:13:42.031468Z",
     "iopub.status.idle": "2025-08-18T07:18:59.749493Z",
     "shell.execute_reply": "2025-08-18T07:18:59.748676Z",
     "shell.execute_reply.started": "2025-08-18T07:13:42.031706Z"
    },
    "id": "1ceeec91",
    "outputId": "fb41224d-20af-46e4-d526-37783ba128be",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Setting up trainer...\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 05:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final model...\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting model training...\")\n",
    "trainer = whisper_trainer.train(train_dataset, val_dataset, CONFIG)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f0b46",
   "metadata": {
    "id": "114f0b46"
   },
   "source": [
    "## Module 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6110f476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T07:21:31.905368Z",
     "iopub.status.busy": "2025-08-18T07:21:31.904663Z",
     "iopub.status.idle": "2025-08-18T07:21:32.701750Z",
     "shell.execute_reply": "2025-08-18T07:21:32.701066Z",
     "shell.execute_reply.started": "2025-08-18T07:21:31.905343Z"
    },
    "id": "6110f476",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from ./whisper-medical-asr...\n",
      "Model and metrics loaded for evaluation.\n"
     ]
    }
   ],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Handles model evaluation and metrics computation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        # Load trained model\n",
    "        print(f\"Loading trained model from {model_path}...\")\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "        self.processor = WhisperProcessor.from_pretrained(model_path)\n",
    "        \n",
    "        # Fix generation config issues\n",
    "        self.model.generation_config.forced_decoder_ids = None\n",
    "        self.model.generation_config.suppress_tokens = []\n",
    "        \n",
    "        # Load WER metric\n",
    "        self.wer_metric = evaluate.load(\"wer\")\n",
    "        \n",
    "        print(\"Model and metrics loaded for evaluation.\")\n",
    "    \n",
    "    def transcribe_audio(self, audio_features: torch.Tensor) -> str:\n",
    "        \"\"\"Transcribe audio features to text.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Generate transcription with explicit parameters to avoid conflicts\n",
    "            predicted_ids = self.model.generate(\n",
    "                audio_features,\n",
    "                max_length=225,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                language=\"en\",  # Force English to avoid language detection\n",
    "                task=\"transcribe\",  # Explicit task\n",
    "                forced_decoder_ids=None,  # Explicitly set to None\n",
    "                suppress_tokens=[],  # Empty suppress tokens\n",
    "            )\n",
    "            \n",
    "            # Decode to text\n",
    "            transcription = self.processor.tokenizer.batch_decode(\n",
    "                predicted_ids, \n",
    "                skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            return transcription.strip()\n",
    "    \n",
    "    def evaluate_dataset(self, dataset: Dataset, max_samples: Optional[int] = None) -> Dict:\n",
    "        \"\"\"Evaluate model on a dataset and compute metrics.\"\"\"\n",
    "        print(\"Running evaluation...\")\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        eval_samples = min(len(dataset), max_samples) if max_samples else len(dataset)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(eval_samples):\n",
    "            try:\n",
    "                sample = dataset[i]\n",
    "                \n",
    "                # Handle input_features - convert to tensor if needed\n",
    "                input_features = sample['input_features']\n",
    "                if isinstance(input_features, list):\n",
    "                    input_features = torch.tensor(input_features)\n",
    "                elif not isinstance(input_features, torch.Tensor):\n",
    "                    input_features = torch.tensor(input_features)\n",
    "                \n",
    "                # Ensure correct shape - add batch dimension if needed\n",
    "                if len(input_features.shape) == 2:\n",
    "                    input_features = input_features.unsqueeze(0)\n",
    "                \n",
    "                # Handle labels\n",
    "                labels = sample['labels']\n",
    "                if isinstance(labels, list):\n",
    "                    labels = torch.tensor(labels)\n",
    "                elif not isinstance(labels, torch.Tensor):\n",
    "                    labels = torch.tensor(labels)\n",
    "                \n",
    "                # Get reference text (decode labels)\n",
    "                reference = self.processor.tokenizer.decode(\n",
    "                    labels, \n",
    "                    skip_special_tokens=True\n",
    "                ).strip()\n",
    "                \n",
    "                # Generate prediction\n",
    "                prediction = self.transcribe_audio(input_features)\n",
    "                \n",
    "                predictions.append(prediction)\n",
    "                references.append(reference)\n",
    "                \n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"Evaluated {i + 1}/{eval_samples} samples...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i}: {e}\")\n",
    "                # Add empty results to maintain alignment\n",
    "                predictions.append(\"\")\n",
    "                references.append(\"\")\n",
    "                continue\n",
    "        \n",
    "        # Filter out empty predictions/references\n",
    "        valid_pairs = [(p, r) for p, r in zip(predictions, references) if p.strip() and r.strip()]\n",
    "        if valid_pairs:\n",
    "            valid_predictions, valid_references = zip(*valid_pairs)\n",
    "        else:\n",
    "            valid_predictions, valid_references = [], []\n",
    "        \n",
    "        # Compute WER\n",
    "        if len(valid_predictions) > 0:\n",
    "            wer_score = self.wer_metric.compute(predictions=list(valid_predictions), references=list(valid_references))\n",
    "        else:\n",
    "            wer_score = 1.0  # 100% error if no valid predictions\n",
    "        \n",
    "        results = {\n",
    "            'wer': wer_score,\n",
    "            'num_samples': len(valid_predictions),\n",
    "            'predictions': list(valid_predictions[:5]),  # First 5 for inspection\n",
    "            'references': list(valid_references[:5])\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_evaluation_results(self, results: Dict):\n",
    "        \"\"\"Print evaluation results in a readable format.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Word Error Rate (WER): {results['wer']:.4f}\")\n",
    "        print(f\"Samples evaluated: {results['num_samples']}\")\n",
    "        \n",
    "        print(\"\\nSample Predictions vs References:\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        for i, (pred, ref) in enumerate(zip(results['predictions'], results['references'])):\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(f\"  Reference: {ref}\")\n",
    "            print(f\"  Prediction: {pred}\")\n",
    "            print()\n",
    "\n",
    "# Initialize evaluator (will load the trained model)\n",
    "evaluator = ModelEvaluator(CONFIG['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab01752d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T07:21:37.771296Z",
     "iopub.status.busy": "2025-08-18T07:21:37.771004Z",
     "iopub.status.idle": "2025-08-18T07:23:18.127963Z",
     "shell.execute_reply": "2025-08-18T07:23:18.127168Z",
     "shell.execute_reply.started": "2025-08-18T07:21:37.771274Z"
    },
    "id": "ab01752d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on validation set...\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 50/100 samples...\n",
      "Evaluated 100/100 samples...\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Word Error Rate (WER): 0.1766\n",
      "Samples evaluated: 100\n",
      "\n",
      "Sample Predictions vs References:\n",
      "--------------------------------------------------\n",
      "Sample 1:\n",
      "  Reference: Remember to follow your healthcare provider's instructions carefully when taking umeclidinium.\n",
      "  Prediction: Remember to follow your healthcare providers instructions carefully when taking U-Micladinium.\n",
      "\n",
      "Sample 2:\n",
      "  Reference: DUORANDIL is a commonly prescribed medication for individuals with heart conditions.\n",
      "  Prediction: Durandil is a commonly prescribed medication for individuals with heart conditions.\n",
      "\n",
      "Sample 3:\n",
      "  Reference: It is important to follow the dosage instructions when taking JILAZO to ensure its effectiveness.\n",
      "  Prediction: It is important to follow the dosage instructions when taking gelazzo to ensure its effectiveness.\n",
      "\n",
      "Sample 4:\n",
      "  Reference: Have you tried Bevon softules for an easy-to-take multivitamin solution?\n",
      "  Prediction: Have you tried Bavansoftchels for an easy? To take multivitum, in solution.\n",
      "\n",
      "Sample 5:\n",
      "  Reference: Remember to take VISPREDA exactly as directed by your healthcare provider for best results.\n",
      "  Prediction: Remember to take Vesprata, exactly as directed by your healthcare provider for best results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating model on validation set...\")\n",
    "eval_results = evaluator.evaluate_dataset(val_dataset, max_samples=100)  # Limit for speed\n",
    "\n",
    "# Print results\n",
    "evaluator.print_evaluation_results(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c084d",
   "metadata": {
    "id": "776c084d"
   },
   "source": [
    "## Module 5: Inference and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32a0e1d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T07:25:01.298524Z",
     "iopub.status.busy": "2025-08-18T07:25:01.298225Z",
     "iopub.status.idle": "2025-08-18T07:25:01.714837Z",
     "shell.execute_reply": "2025-08-18T07:25:01.714219Z",
     "shell.execute_reply.started": "2025-08-18T07:25:01.298506Z"
    },
    "id": "32a0e1d5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for inference from ./whisper-medical-asr...\n",
      "Inference engine ready.\n"
     ]
    }
   ],
   "source": [
    "class InferenceEngine:\n",
    "    \"\"\"Handles inference on new audio samples.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        # Load model and processor\n",
    "        print(f\"Loading model for inference from {model_path}...\")\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "        self.processor = WhisperProcessor.from_pretrained(model_path)\n",
    "        \n",
    "        # Fix generation config issues\n",
    "        self.model.generation_config.forced_decoder_ids = None\n",
    "        self.model.generation_config.suppress_tokens = []\n",
    "        \n",
    "        # Set to eval mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(\"Inference engine ready.\")\n",
    "    \n",
    "    def transcribe_from_features(self, input_features: torch.Tensor) -> str:\n",
    "        \"\"\"Transcribe audio from preprocessed features.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Generate transcription with explicit parameters\n",
    "            predicted_ids = self.model.generate(\n",
    "                input_features,\n",
    "                max_length=225,\n",
    "                num_beams=2,  # Slightly better quality\n",
    "                do_sample=False,\n",
    "                temperature=1.0,\n",
    "                language=\"en\",  # Force English\n",
    "                task=\"transcribe\",  # Explicit task\n",
    "                forced_decoder_ids=None,  # Explicitly set to None\n",
    "                suppress_tokens=[],  # Empty suppress tokens\n",
    "            )\n",
    "            \n",
    "            # Decode to text\n",
    "            transcription = self.processor.tokenizer.batch_decode(\n",
    "                predicted_ids, \n",
    "                skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            return transcription.strip()\n",
    "    \n",
    "    def transcribe_raw_audio(self, audio_array: np.ndarray, sampling_rate: int) -> str:\n",
    "        \"\"\"Transcribe from raw audio array.\"\"\"\n",
    "        # Preprocess audio\n",
    "        if sampling_rate != 16000:\n",
    "            # Resample to 16kHz\n",
    "            audio_tensor = torch.from_numpy(audio_array).float()\n",
    "            if len(audio_tensor.shape) == 1:\n",
    "                audio_tensor = audio_tensor.unsqueeze(0)\n",
    "            \n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                orig_freq=sampling_rate, \n",
    "                new_freq=16000\n",
    "            )\n",
    "            audio_array = resampler(audio_tensor).squeeze().numpy()\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.processor.feature_extractor(\n",
    "            audio_array, \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Transcribe\n",
    "        return self.transcribe_from_features(features[\"input_features\"])\n",
    "    \n",
    "    def demo_inference(self, dataset: Dataset, num_samples: int = 3):\n",
    "        \"\"\"Run demo inference on dataset samples.\"\"\"\n",
    "        print(f\"\\nRunning demo inference on {num_samples} samples...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for i in range(min(num_samples, len(dataset))):\n",
    "            try:\n",
    "                sample = dataset[i]\n",
    "                \n",
    "                # Handle input_features - convert to tensor if needed\n",
    "                input_features = sample['input_features']\n",
    "                if isinstance(input_features, list):\n",
    "                    input_features = torch.tensor(input_features)\n",
    "                elif not isinstance(input_features, torch.Tensor):\n",
    "                    input_features = torch.tensor(input_features)\n",
    "                \n",
    "                # Ensure correct shape - add batch dimension if needed\n",
    "                if len(input_features.shape) == 2:\n",
    "                    input_features = input_features.unsqueeze(0)\n",
    "                \n",
    "                # Handle labels\n",
    "                labels = sample['labels']\n",
    "                if isinstance(labels, list):\n",
    "                    labels = torch.tensor(labels)\n",
    "                elif not isinstance(labels, torch.Tensor):\n",
    "                    labels = torch.tensor(labels)\n",
    "                \n",
    "                # Get reference\n",
    "                reference = self.processor.tokenizer.decode(\n",
    "                    labels, \n",
    "                    skip_special_tokens=True\n",
    "                ).strip()\n",
    "                \n",
    "                # Get prediction\n",
    "                prediction = self.transcribe_from_features(input_features)\n",
    "                \n",
    "                # Display results\n",
    "                print(f\"\\nSample {i+1}:\")\n",
    "                print(f\"Reference:  {reference}\")\n",
    "                print(f\"Prediction: {prediction}\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing sample {i+1}: {e}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# Initialize inference engine\n",
    "inference_engine = InferenceEngine(CONFIG['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a073980e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T07:25:16.865706Z",
     "iopub.status.busy": "2025-08-18T07:25:16.865113Z",
     "iopub.status.idle": "2025-08-18T07:25:21.424538Z",
     "shell.execute_reply": "2025-08-18T07:25:21.423820Z",
     "shell.execute_reply.started": "2025-08-18T07:25:16.865683Z"
    },
    "id": "a073980e",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running demo inference on 5 samples...\n",
      "============================================================\n",
      "\n",
      "Sample 1:\n",
      "Reference:  Remember to follow your healthcare provider's instructions carefully when taking umeclidinium.\n",
      "Prediction: Remember to follow your healthcare provider's instruction carefully when taking you Mickladinium.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "Reference:  DUORANDIL is a commonly prescribed medication for individuals with heart conditions.\n",
      "Prediction: Durandel is a commonly prescribed medication for individuals with heart conditions.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample 3:\n",
      "Reference:  It is important to follow the dosage instructions when taking JILAZO to ensure its effectiveness.\n",
      "Prediction: It is important to follow the dosage instructions when taking jillazo to ensure its effectiveness.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample 4:\n",
      "Reference:  Have you tried Bevon softules for an easy-to-take multivitamin solution?\n",
      "Prediction: \"Have you tried Bavan soft-shells for an easy, to take multivitum, in solution?\" (Ju)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample 5:\n",
      "Reference:  Remember to take VISPREDA exactly as directed by your healthcare provider for best results.\n",
      "Prediction: Remember to take this spread-a, exactly as directed by your health care provider for best results.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run demo inference\n",
    "inference_engine.demo_inference(val_dataset, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6d968",
   "metadata": {
    "id": "bcc6d968"
   },
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71cd1e2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T07:25:25.495567Z",
     "iopub.status.busy": "2025-08-18T07:25:25.495324Z",
     "iopub.status.idle": "2025-08-18T07:25:25.502709Z",
     "shell.execute_reply": "2025-08-18T07:25:25.501908Z",
     "shell.execute_reply.started": "2025-08-18T07:25:25.495552Z"
    },
    "id": "71cd1e2f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BASELINE ASR MODEL TRAINING COMPLETE\n",
      "============================================================\n",
      "Model: openai/whisper-tiny\n",
      "Dataset: Shamus/United-Syn-Med\n",
      "Samples used: 700\n",
      "Training epochs: 3\n",
      "Model saved to: ./whisper-medical-asr\n",
      "Final WER: 0.1766\n",
      "\n",
      "Next Steps:\n",
      "1. Fine-tune hyperparameters for better WER\n",
      "2. Increase dataset size for more robust training\n",
      "3. Implement EHR structuring pipeline\n",
      "4. Add domain-specific medical vocabulary\n",
      "5. Evaluate on held-out test set\n",
      "\n",
      "Model Files:\n",
      "  - runs\n",
      "  - model.safetensors\n",
      "  - normalizer.json\n",
      "  - vocab.json\n",
      "  - generation_config.json\n",
      "  - config.json\n",
      "  - checkpoint-54\n",
      "  - preprocessor_config.json\n",
      "  - tokenizer_config.json\n",
      "  - training_args.bin\n",
      "  - merges.txt\n",
      "  - special_tokens_map.json\n",
      "  - added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "# Summary of the training run\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE ASR MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Dataset: {CONFIG['dataset_name']}\")\n",
    "print(f\"Samples used: {CONFIG['num_samples']}\")\n",
    "print(f\"Training epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Model saved to: {CONFIG['output_dir']}\")\n",
    "print(f\"Final WER: {eval_results['wer']:.4f}\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Fine-tune hyperparameters for better WER\")\n",
    "print(\"2. Increase dataset size for more robust training\")\n",
    "print(\"3. Implement EHR structuring pipeline\")\n",
    "print(\"4. Add domain-specific medical vocabulary\")\n",
    "print(\"5. Evaluate on held-out test set\")\n",
    "\n",
    "print(\"\\nModel Files:\")\n",
    "import os\n",
    "if os.path.exists(CONFIG['output_dir']):\n",
    "    files = os.listdir(CONFIG['output_dir'])\n",
    "    for file in files:\n",
    "        print(f\"  - {file}\")\n",
    "else:\n",
    "    print(\"  Model directory not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde0f00-c55f-4765-9df3-1bb5d5ad98fd",
   "metadata": {},
   "source": [
    "## Model downloading \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9981da2-a135-4593-8af2-3d440fe9a5e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T07:47:44.289449Z",
     "iopub.status.busy": "2025-08-18T07:47:44.289128Z",
     "iopub.status.idle": "2025-08-18T07:48:15.370054Z",
     "shell.execute_reply": "2025-08-18T07:48:15.369483Z",
     "shell.execute_reply.started": "2025-08-18T07:47:44.289427Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model packaged as whisper_medical_asr_model.zip\n",
      " Size: 529.4 MB\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_9f661718-feed-4ae2-ace0-8dbbaa119873\", \"whisper_medical_asr_model.zip\", 555078991)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Download started!\n"
     ]
    }
   ],
   "source": [
    "# Download trained model as ZIP file\n",
    "def download_trained_model():\n",
    "    \"\"\"Download the complete trained model as a ZIP file.\"\"\"\n",
    "    import zipfile\n",
    "    import os\n",
    "    \n",
    "    # Your model directory\n",
    "    model_dir = CONFIG['output_dir']  # './whisper-medical-asr'\n",
    "    zip_filename = 'whisper_medical_asr_model.zip'\n",
    "    \n",
    "    # Create ZIP file with all model files\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(model_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Add file to zip with relative path\n",
    "                zipf.write(file_path, os.path.relpath(file_path, model_dir))\n",
    "    \n",
    "    print(f\" Model packaged as {zip_filename}\")\n",
    "    print(f\" Size: {os.path.getsize(zip_filename) / (1024*1024):.1f} MB\")\n",
    "    \n",
    "    # Download the file (works in Colab/Kaggle)\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(zip_filename)\n",
    "        print(\" Download started!\")\n",
    "    except:\n",
    "        print(\" On Kaggle: Find the file in the output section\")\n",
    "        print(\" On local: File saved in current directory\")\n",
    "\n",
    "# Run this after training\n",
    "download_trained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c69129-a439-437d-914c-91e1763aa13d",
   "metadata": {},
   "source": [
    "##  Upload to Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46f4ad86-3f3a-44e3-a591-d21c142711d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T07:59:05.646741Z",
     "iopub.status.busy": "2025-08-18T07:59:05.646138Z",
     "iopub.status.idle": "2025-08-18T07:59:16.393909Z",
     "shell.execute_reply": "2025-08-18T07:59:16.393194Z",
     "shell.execute_reply.started": "2025-08-18T07:59:05.646717Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb05bd9abbb4334978c3893113fb8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f735968ba6f4de99904b5e8ae00564f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 10 LFS files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ff8fb975674c41b447f96623a41c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/298M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2696f5644d44f98bdec18911d565741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29496cbe896445288eddb9b0d05380da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09022c22b8e404190a9f909a5a4b5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856cd16808a14a5c9e4972cd667f798e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29a1d335ea2475ab0c05d7be85eca56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e3dea37155459b81bde7e281ac2b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1755499978.27923e26b12c.36.0:   0%|          | 0.00/6.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673fa2d5dd304292bfd2b18eb8f613c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1755501222.27923e26b12c.36.1:   0%|          | 0.00/6.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8057dfafc9d34a7296a5bfe55a578801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model uploaded to: https://huggingface.co/Abhijeet17o/whisper-tiny-1000-medical-asr\n",
      " Now you can load it from anywhere!\n"
     ]
    }
   ],
   "source": [
    "# Upload to Hugging Face Hub\n",
    "def upload_to_huggingface():\n",
    "    \"\"\"Upload your trained model to Hugging Face Hub.\"\"\"\n",
    "    from huggingface_hub import HfApi, create_repo\n",
    "    \n",
    "    # Your Hugging Face username and desired repo name\n",
    "    username = \"Abhijeet17o\"  # Replace with your HF username\n",
    "    repo_name = \"whisper-tiny-1000-medical-asr\"\n",
    "    \n",
    "    # Create repository\n",
    "    api = HfApi()\n",
    "    repo_url = create_repo(f\"{username}/{repo_name}\", exist_ok=True)\n",
    "    \n",
    "    # Upload model files\n",
    "    api.upload_folder(\n",
    "        folder_path=CONFIG['output_dir'],\n",
    "        repo_id=f\"{username}/{repo_name}\",\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    \n",
    "    print(f\" Model uploaded to: https://huggingface.co/{username}/{repo_name}\")\n",
    "    print(\" Now you can load it from anywhere!\")\n",
    "\n",
    "upload_to_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9456aa03-6d58-4c06-a801-2d44f1d9872a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T07:59:03.622419Z",
     "iopub.status.busy": "2025-08-18T07:59:03.622153Z",
     "iopub.status.idle": "2025-08-18T07:59:03.675409Z",
     "shell.execute_reply": "2025-08-18T07:59:03.674814Z",
     "shell.execute_reply.started": "2025-08-18T07:59:03.622400Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "# Authenticate with Hugging Face\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Method 1: Direct token input (less secure but quick)\n",
    "token = \"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\"  # Replace with your actual token\n",
    "login(token=token)\n",
    "\n",
    "print(\" Logged in to Hugging Face!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a6df2-fb2d-4ab9-9062-9d0b0a22c8d5",
   "metadata": {},
   "source": [
    "# Base Whisper model to check the WER score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2088033c-ac2f-46ca-aaca-2800debb5b2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T08:07:57.206893Z",
     "iopub.status.busy": "2025-08-18T08:07:57.206343Z",
     "iopub.status.idle": "2025-08-18T08:07:59.326955Z",
     "shell.execute_reply": "2025-08-18T08:07:59.326245Z",
     "shell.execute_reply.started": "2025-08-18T08:07:57.206867Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline Whisper model: openai/whisper-tiny...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f1c536d2ff4e97a7b4f7d5c0fe3755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Whisper model loaded for comparison.\n"
     ]
    }
   ],
   "source": [
    "# Compare with Normal Whisper Model\n",
    "class BaselineWhisperEvaluator:\n",
    "    \"\"\"Evaluate normal Whisper model (not fine-tuned) for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'openai/whisper-tiny'):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Load normal Whisper model (not fine-tuned)\n",
    "        print(f\"Loading baseline Whisper model: {model_name}...\")\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.processor = WhisperProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        # Fix generation config\n",
    "        self.model.generation_config.forced_decoder_ids = None\n",
    "        self.model.generation_config.suppress_tokens = []\n",
    "        \n",
    "        # Load WER metric\n",
    "        self.wer_metric = evaluate.load(\"wer\")\n",
    "        \n",
    "        print(\"Baseline Whisper model loaded for comparison.\")\n",
    "    \n",
    "    def transcribe_audio(self, audio_features: torch.Tensor) -> str:\n",
    "        \"\"\"Transcribe audio features to text using baseline Whisper.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Generate transcription\n",
    "            predicted_ids = self.model.generate(\n",
    "                audio_features,\n",
    "                max_length=225,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                language=\"en\",\n",
    "                task=\"transcribe\",\n",
    "                forced_decoder_ids=None,\n",
    "                suppress_tokens=[],\n",
    "            )\n",
    "            \n",
    "            # Decode to text\n",
    "            transcription = self.processor.tokenizer.batch_decode(\n",
    "                predicted_ids, \n",
    "                skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            return transcription.strip()\n",
    "    \n",
    "    def evaluate_dataset(self, dataset: Dataset, max_samples: Optional[int] = None) -> Dict:\n",
    "        \"\"\"Evaluate baseline model on the dataset.\"\"\"\n",
    "        print(\"Running baseline Whisper evaluation...\")\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        eval_samples = min(len(dataset), max_samples) if max_samples else len(dataset)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(eval_samples):\n",
    "            try:\n",
    "                sample = dataset[i]\n",
    "                \n",
    "                # Handle input_features\n",
    "                input_features = sample['input_features']\n",
    "                if isinstance(input_features, list):\n",
    "                    input_features = torch.tensor(input_features)\n",
    "                elif not isinstance(input_features, torch.Tensor):\n",
    "                    input_features = torch.tensor(input_features)\n",
    "                \n",
    "                if len(input_features.shape) == 2:\n",
    "                    input_features = input_features.unsqueeze(0)\n",
    "                \n",
    "                # Handle labels\n",
    "                labels = sample['labels']\n",
    "                if isinstance(labels, list):\n",
    "                    labels = torch.tensor(labels)\n",
    "                elif not isinstance(labels, torch.Tensor):\n",
    "                    labels = torch.tensor(labels)\n",
    "                \n",
    "                # Get reference text\n",
    "                reference = self.processor.tokenizer.decode(\n",
    "                    labels, \n",
    "                    skip_special_tokens=True\n",
    "                ).strip()\n",
    "                \n",
    "                # Generate prediction with baseline model\n",
    "                prediction = self.transcribe_audio(input_features)\n",
    "                \n",
    "                predictions.append(prediction)\n",
    "                references.append(reference)\n",
    "                \n",
    "                if (i + 1) % 25 == 0:\n",
    "                    print(f\"Evaluated {i + 1}/{eval_samples} samples...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i}: {e}\")\n",
    "                predictions.append(\"\")\n",
    "                references.append(\"\")\n",
    "                continue\n",
    "        \n",
    "        # Filter out empty predictions/references\n",
    "        valid_pairs = [(p, r) for p, r in zip(predictions, references) if p.strip() and r.strip()]\n",
    "        if valid_pairs:\n",
    "            valid_predictions, valid_references = zip(*valid_pairs)\n",
    "        else:\n",
    "            valid_predictions, valid_references = [], []\n",
    "        \n",
    "        # Compute WER\n",
    "        if len(valid_predictions) > 0:\n",
    "            wer_score = self.wer_metric.compute(predictions=list(valid_predictions), references=list(valid_references))\n",
    "        else:\n",
    "            wer_score = 1.0\n",
    "        \n",
    "        results = {\n",
    "            'wer': wer_score,\n",
    "            'num_samples': len(valid_predictions),\n",
    "            'predictions': list(valid_predictions[:5]),\n",
    "            'references': list(valid_references[:5])\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize baseline evaluator\n",
    "baseline_evaluator = BaselineWhisperEvaluator('openai/whisper-tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "078960ce-c3fa-4605-a864-0947503d0241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T08:08:07.385581Z",
     "iopub.status.busy": "2025-08-18T08:08:07.384838Z",
     "iopub.status.idle": "2025-08-18T08:09:34.212602Z",
     "shell.execute_reply": "2025-08-18T08:09:34.211812Z",
     "shell.execute_reply.started": "2025-08-18T08:08:07.385555Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluating BASELINE Whisper-tiny model...\n",
      "Running baseline Whisper evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 25/100 samples...\n",
      "Evaluated 50/100 samples...\n",
      "Evaluated 75/100 samples...\n",
      "Evaluated 100/100 samples...\n",
      "\n",
      "============================================================\n",
      " MODEL COMPARISON RESULTS\n",
      "============================================================\n",
      "\n",
      " BASELINE Whisper-tiny (no fine-tuning):\n",
      "   Word Error Rate: 0.1892 (18.92%)\n",
      "   Samples: 100\n",
      "\n",
      " YOUR FINE-TUNED Model:\n",
      "   Word Error Rate: 0.1766 (17.66%)\n",
      "   Samples: 100\n",
      "\n",
      " IMPROVEMENT:\n",
      "   Absolute WER reduction: 0.0126\n",
      "   Relative improvement: 6.7%\n",
      "    Your model is 6.7% better!\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run comparison evaluation\n",
    "print(\" Evaluating BASELINE Whisper-tiny model...\")\n",
    "baseline_results = baseline_evaluator.evaluate_dataset(val_dataset, max_samples=100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n BASELINE Whisper-tiny (no fine-tuning):\")\n",
    "print(f\"   Word Error Rate: {baseline_results['wer']:.4f} ({baseline_results['wer']*100:.2f}%)\")\n",
    "print(f\"   Samples: {baseline_results['num_samples']}\")\n",
    "\n",
    "print(f\"\\n YOUR FINE-TUNED Model:\")\n",
    "print(f\"   Word Error Rate: {eval_results['wer']:.4f} ({eval_results['wer']*100:.2f}%)\")\n",
    "print(f\"   Samples: {eval_results['num_samples']}\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = baseline_results['wer'] - eval_results['wer']\n",
    "improvement_percent = (improvement / baseline_results['wer']) * 100\n",
    "\n",
    "print(f\"\\n IMPROVEMENT:\")\n",
    "print(f\"   Absolute WER reduction: {improvement:.4f}\")\n",
    "print(f\"   Relative improvement: {improvement_percent:.1f}%\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"    Your model is {improvement_percent:.1f}% better!\")\n",
    "else:\n",
    "    print(f\"    Baseline performed {abs(improvement_percent):.1f}% better\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1048f87-2a71-4d36-a37c-b4f1c97cdb0a",
   "metadata": {},
   "source": [
    "# Comparing the baseline and the finetuned model side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ae5a1b3-0e6b-4fb9-a64f-f5139babbcdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T08:09:34.213848Z",
     "iopub.status.busy": "2025-08-18T08:09:34.213600Z",
     "iopub.status.idle": "2025-08-18T08:09:34.219817Z",
     "shell.execute_reply": "2025-08-18T08:09:34.219046Z",
     "shell.execute_reply.started": "2025-08-18T08:09:34.213830Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " PREDICTION COMPARISON (Baseline vs Fine-tuned)\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      " Reference:   Remember to follow your healthcare provider's instructions carefully when taking umeclidinium.\n",
      " Baseline:    Remember to follow your health care providers instructions carefully when taking you McLidinium.\n",
      " Fine-tuned:  Remember to follow your healthcare providers instructions carefully when taking U-Micladinium.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      " Reference:   DUORANDIL is a commonly prescribed medication for individuals with heart conditions.\n",
      " Baseline:    Durandil is a commonly prescribed medication for individuals with heart conditions.\n",
      " Fine-tuned:  Durandil is a commonly prescribed medication for individuals with heart conditions.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 3:\n",
      " Reference:   It is important to follow the dosage instructions when taking JILAZO to ensure its effectiveness.\n",
      " Baseline:    It is important to follow the dosage instructions when taking gelazzo to ensure its effectiveness.\n",
      " Fine-tuned:  It is important to follow the dosage instructions when taking gelazzo to ensure its effectiveness.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 4:\n",
      " Reference:   Have you tried Bevon softules for an easy-to-take multivitamin solution?\n",
      " Baseline:    Have you tried Bevan Softchills for an easy? To take multivitum in solution.\n",
      " Fine-tuned:  Have you tried Bavansoftchels for an easy? To take multivitum, in solution.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 5:\n",
      " Reference:   Remember to take VISPREDA exactly as directed by your healthcare provider for best results.\n",
      " Baseline:    Remember to take Vesprata exactly as directed by your health care provider for best results.\n",
      " Fine-tuned:  Remember to take Vesprata, exactly as directed by your healthcare provider for best results.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side prediction comparison\n",
    "def compare_predictions(baseline_results, finetuned_results, num_samples=5):\n",
    "    \"\"\"Compare predictions side by side.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" PREDICTION COMPARISON (Baseline vs Fine-tuned)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i in range(min(num_samples, len(baseline_results['predictions']))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\" Reference:   {baseline_results['references'][i]}\")\n",
    "        print(f\" Baseline:    {baseline_results['predictions'][i]}\")\n",
    "        print(f\" Fine-tuned:  {finetuned_results['predictions'][i]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Run the comparison\n",
    "compare_predictions(baseline_results, eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e86764-e464-4804-8878-15b38eb67f07",
   "metadata": {},
   "source": [
    "# Techniques for Major WER Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "826929c4-a372-4eab-bdb7-f2ea433f38f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T08:17:20.654931Z",
     "iopub.status.busy": "2025-08-18T08:17:20.654227Z",
     "iopub.status.idle": "2025-08-18T08:17:20.666262Z",
     "shell.execute_reply": "2025-08-18T08:17:20.665381Z",
     "shell.execute_reply.started": "2025-08-18T08:17:20.654895Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Advanced Audio Data Augmentation\n",
    "import torchaudio.transforms as T\n",
    "import random\n",
    "\n",
    "class MedicalAudioAugmentor:\n",
    "    \"\"\"Advanced audio augmentation for medical conversations.\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # Define augmentation transforms\n",
    "        self.transforms = {\n",
    "            'noise': self.add_background_noise,\n",
    "            'speed': self.change_speed,\n",
    "            'pitch': self.change_pitch,\n",
    "            'volume': self.change_volume,\n",
    "            'reverb': self.add_reverb,\n",
    "            'bandpass': self.bandpass_filter\n",
    "        }\n",
    "    \n",
    "    def add_background_noise(self, audio, noise_factor=0.005):\n",
    "        \"\"\"Add subtle background noise (hospital environment).\"\"\"\n",
    "        noise = torch.randn_like(audio) * noise_factor\n",
    "        return audio + noise\n",
    "    \n",
    "    def change_speed(self, audio, speed_factor=None):\n",
    "        \"\"\"Change speaking speed (0.9-1.1x).\"\"\"\n",
    "        if speed_factor is None:\n",
    "            speed_factor = random.uniform(0.9, 1.1)\n",
    "        \n",
    "        # Use time stretching\n",
    "        return torch.nn.functional.interpolate(\n",
    "            audio.unsqueeze(0).unsqueeze(0),\n",
    "            scale_factor=speed_factor,\n",
    "            mode='linear',\n",
    "            align_corners=False\n",
    "        ).squeeze()\n",
    "    \n",
    "    def change_pitch(self, audio, n_steps=None):\n",
    "        \"\"\"Shift pitch slightly (2 semitones).\"\"\"\n",
    "        if n_steps is None:\n",
    "            n_steps = random.uniform(-2, 2)\n",
    "        \n",
    "        # Pitch shift using resampling approximation\n",
    "        shift_factor = 2 ** (n_steps / 12)\n",
    "        return self.change_speed(audio, shift_factor)\n",
    "    \n",
    "    def change_volume(self, audio, volume_factor=None):\n",
    "        \"\"\"Adjust volume (0.7-1.3x).\"\"\"\n",
    "        if volume_factor is None:\n",
    "            volume_factor = random.uniform(0.7, 1.3)\n",
    "        return audio * volume_factor\n",
    "    \n",
    "    def add_reverb(self, audio, room_size=0.1):\n",
    "        \"\"\"Add subtle room reverb.\"\"\"\n",
    "        # Simple reverb using delay and decay\n",
    "        delay_samples = int(0.05 * self.sample_rate)  # 50ms delay\n",
    "        decay = 0.3\n",
    "        \n",
    "        if len(audio) > delay_samples:\n",
    "            reverb = torch.zeros_like(audio)\n",
    "            reverb[delay_samples:] = audio[:-delay_samples] * decay\n",
    "            return audio + reverb\n",
    "        return audio\n",
    "    \n",
    "    def bandpass_filter(self, audio, low_freq=300, high_freq=8000):\n",
    "        \"\"\"Apply bandpass filter (telephone quality).\"\"\"\n",
    "        # Simulate different recording devices\n",
    "        nyquist = self.sample_rate // 2\n",
    "        low = low_freq / nyquist\n",
    "        high = high_freq / nyquist\n",
    "        \n",
    "        # Simple frequency domain filtering\n",
    "        fft = torch.fft.rfft(audio)\n",
    "        freqs = torch.fft.rfftfreq(len(audio), 1/self.sample_rate)\n",
    "        \n",
    "        # Create bandpass mask\n",
    "        mask = (freqs >= low_freq) & (freqs <= high_freq)\n",
    "        fft = fft * mask.float()\n",
    "        \n",
    "        return torch.fft.irfft(fft, n=len(audio))\n",
    "    \n",
    "    def augment_batch(self, audio_batch, augment_prob=0.8, num_augs=2):\n",
    "        \"\"\"Apply random augmentations to a batch.\"\"\"\n",
    "        augmented_batch = []\n",
    "        \n",
    "        for audio in audio_batch:\n",
    "            if random.random() < augment_prob:\n",
    "                # Apply random augmentations\n",
    "                aug_names = random.sample(list(self.transforms.keys()), num_augs)\n",
    "                \n",
    "                augmented_audio = audio.clone()\n",
    "                for aug_name in aug_names:\n",
    "                    augmented_audio = self.transforms[aug_name](augmented_audio)\n",
    "                \n",
    "                augmented_batch.append(augmented_audio)\n",
    "            else:\n",
    "                augmented_batch.append(audio)\n",
    "        \n",
    "        return augmented_batch\n",
    "\n",
    "# Initialize augmentor\n",
    "augmentor = MedicalAudioAugmentor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05e3f744-9306-4ab7-b5a3-e73345540aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T08:17:20.667427Z",
     "iopub.status.busy": "2025-08-18T08:17:20.667240Z",
     "iopub.status.idle": "2025-08-18T08:17:20.695953Z",
     "shell.execute_reply": "2025-08-18T08:17:20.695368Z",
     "shell.execute_reply.started": "2025-08-18T08:17:20.667413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Medical-Specific Preprocessing\n",
    "class MedicalTextProcessor:\n",
    "    \"\"\"Enhanced text processing for medical terminology.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Common medical abbreviations and their expansions\n",
    "        self.medical_abbreviations = {\n",
    "            'mg': 'milligrams',\n",
    "            'ml': 'milliliters',\n",
    "            'bp': 'blood pressure',\n",
    "            'hr': 'heart rate',\n",
    "            'temp': 'temperature',\n",
    "            'wbc': 'white blood cell',\n",
    "            'rbc': 'red blood cell',\n",
    "            'ecg': 'electrocardiogram',\n",
    "            'mri': 'magnetic resonance imaging',\n",
    "            'ct': 'computed tomography',\n",
    "            'iv': 'intravenous',\n",
    "            'po': 'by mouth',\n",
    "            'bid': 'twice daily',\n",
    "            'tid': 'three times daily',\n",
    "            'qid': 'four times daily'\n",
    "        }\n",
    "        \n",
    "        # Common medication name patterns\n",
    "        self.medication_patterns = [\n",
    "            r'(\\w+)mycin',  # antibiotics\n",
    "            r'(\\w+)cillin', # penicillins\n",
    "            r'(\\w+)pril',   # ACE inhibitors\n",
    "            r'(\\w+)sartan', # ARBs\n",
    "            r'(\\w+)olol',   # beta blockers\n",
    "            r'(\\w+)statin', # statins\n",
    "        ]\n",
    "    \n",
    "    def normalize_medical_text(self, text):\n",
    "        \"\"\"Normalize medical text for better training.\"\"\"\n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # Expand abbreviations\n",
    "        for abbrev, expansion in self.medical_abbreviations.items():\n",
    "            text = text.replace(f' {abbrev} ', f' {expansion} ')\n",
    "            text = text.replace(f' {abbrev}.', f' {expansion}')\n",
    "        \n",
    "        # Normalize medication names\n",
    "        import re\n",
    "        for pattern in self.medication_patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                # Ensure consistent casing for medication names\n",
    "                original = f\"{match}{pattern.split('(')[1].split(')')[1]}\"\n",
    "                normalized = original.lower()\n",
    "                text = text.replace(original, normalized)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def add_medical_context(self, text):\n",
    "        \"\"\"Add context markers for medical conversations.\"\"\"\n",
    "        # Add role markers\n",
    "        if any(word in text.lower() for word in ['doctor', 'physician', 'dr']):\n",
    "            text = f\"[DOCTOR] {text}\"\n",
    "        elif any(word in text.lower() for word in ['patient', 'feel', 'pain', 'hurt']):\n",
    "            text = f\"[PATIENT] {text}\"\n",
    "        \n",
    "        return text\n",
    "\n",
    "medical_processor = MedicalTextProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "487d0121-39b9-491b-867a-74211fef2316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T08:17:20.804486Z",
     "iopub.status.busy": "2025-08-18T08:17:20.804260Z",
     "iopub.status.idle": "2025-08-18T08:17:20.815718Z",
     "shell.execute_reply": "2025-08-18T08:17:20.814930Z",
     "shell.execute_reply.started": "2025-08-18T08:17:20.804469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Enhanced Audio Preprocessor with Augmentation\n",
    "class EnhancedAudioPreprocessor(AudioPreprocessor):\n",
    "    \"\"\"Enhanced preprocessing with augmentation and medical-specific handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, target_sample_rate: int = 16000, max_audio_length: float = 30.0):\n",
    "        super().__init__(model_name, target_sample_rate, max_audio_length)\n",
    "        self.augmentor = MedicalAudioAugmentor(target_sample_rate)\n",
    "        self.text_processor = MedicalTextProcessor()\n",
    "        \n",
    "    def enhanced_audio_processing(self, audio_array: np.ndarray, augment: bool = True) -> np.ndarray:\n",
    "        \"\"\"Enhanced audio processing with noise reduction and augmentation.\"\"\"\n",
    "        # Convert to tensor\n",
    "        audio_tensor = torch.from_numpy(audio_array).float()\n",
    "        \n",
    "        # Apply spectral noise reduction (simple highpass filter)\n",
    "        audio_tensor = self.apply_noise_reduction(audio_tensor)\n",
    "        \n",
    "        # Apply augmentation during training\n",
    "        if augment:\n",
    "            # 50% chance to augment\n",
    "            if random.random() < 0.5:\n",
    "                audio_tensor = self.augmentor.add_background_noise(audio_tensor)\n",
    "            if random.random() < 0.3:\n",
    "                audio_tensor = self.augmentor.change_volume(audio_tensor)\n",
    "            if random.random() < 0.2:\n",
    "                audio_tensor = self.augmentor.change_speed(audio_tensor)\n",
    "        \n",
    "        return audio_tensor.numpy()\n",
    "    \n",
    "    def apply_noise_reduction(self, audio_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply simple noise reduction.\"\"\"\n",
    "        # High-pass filter to remove low-frequency noise\n",
    "        highpass = T.HighpassBiquad(self.target_sample_rate, cutoff_freq=80)\n",
    "        audio_tensor = highpass(audio_tensor)\n",
    "        \n",
    "        # Normalize amplitude\n",
    "        audio_tensor = audio_tensor / (torch.max(torch.abs(audio_tensor)) + 1e-8)\n",
    "        \n",
    "        return audio_tensor\n",
    "    \n",
    "    def preprocess_batch(self, batch: Dict, augment: bool = True) -> Dict:\n",
    "        \"\"\"Enhanced batch preprocessing with medical text processing.\"\"\"\n",
    "        audio_data = []\n",
    "        texts = []\n",
    "        \n",
    "        for i in range(len(batch['audio'])):\n",
    "            try:\n",
    "                # Handle audio\n",
    "                audio_info = batch['audio'][i]\n",
    "                audio_array, sampling_rate = self.decode_audio_bytes(audio_info)\n",
    "                \n",
    "                # Enhanced audio processing\n",
    "                audio_array = self.resample_audio(audio_array, sampling_rate)\n",
    "                audio_array = self.enhanced_audio_processing(audio_array, augment=augment)\n",
    "                audio_array = self.trim_or_pad_audio(audio_array)\n",
    "                audio_data.append(audio_array)\n",
    "                \n",
    "                # Enhanced text processing\n",
    "                text_field = \"\"\n",
    "                for field in ['text', 'transcription', 'sentence', 'transcript']:\n",
    "                    if field in batch and i < len(batch[field]):\n",
    "                        text_field = batch[field][i]\n",
    "                        break\n",
    "                \n",
    "                if text_field:\n",
    "                    # Apply medical text processing\n",
    "                    text_field = self.text_processor.normalize_medical_text(str(text_field))\n",
    "                    text_field = self.text_processor.add_medical_context(text_field)\n",
    "                \n",
    "                texts.append(text_field if text_field else \"\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i}: {e}\")\n",
    "                audio_data.append(np.zeros(int(self.max_audio_length * self.target_sample_rate)))\n",
    "                texts.append(\"\")\n",
    "        \n",
    "        # Process with Whisper feature extractor\n",
    "        features = self.feature_extractor(\n",
    "            audio_data, \n",
    "            sampling_rate=self.target_sample_rate, \n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Enhanced tokenization with medical vocabulary\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                texts,\n",
    "                max_length=448,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids\n",
    "        \n",
    "        # Replace padding tokens\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_features\": features[\"input_features\"],\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed0d13c1-fdc5-494e-994e-010fce41c647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T08:17:20.817380Z",
     "iopub.status.busy": "2025-08-18T08:17:20.817150Z",
     "iopub.status.idle": "2025-08-18T08:17:20.840604Z",
     "shell.execute_reply": "2025-08-18T08:17:20.840000Z",
     "shell.execute_reply.started": "2025-08-18T08:17:20.817365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Enhanced training configuration loaded!\n",
      "Expected improvements:\n",
      "- Data Augmentation: 15-25% WER reduction\n",
      "- Medical Text Processing: 10-20% WER reduction\n",
      "- Larger Model (Small): 8-12% WER reduction\n",
      "- Advanced Training: 5-10% WER reduction\n",
      "- Total Expected WER: 8-12% (vs current 17.66%)\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Training Configuration\n",
    "ENHANCED_CONFIG = {\n",
    "    'dataset_name': 'Shamus/United-Syn-Med',\n",
    "    'model_name': 'openai/whisper-tiny',  # Upgrade to small for better performance\n",
    "    'num_samples': 1000,  # Increase dataset size\n",
    "    'target_sample_rate': 16000,\n",
    "    'train_split_ratio': 0.85,  # More training data\n",
    "    'output_dir': './whisper-enhanced-medical-asr',\n",
    "    'max_audio_length': 25.0,  # Slightly shorter for efficiency\n",
    "    \n",
    "    # Enhanced training parameters\n",
    "    'batch_size': 12,  # Larger batch size\n",
    "    'num_epochs': 5,   # More epochs\n",
    "    'learning_rate': 3e-5,  # Lower learning rate for stability\n",
    "    'warmup_steps': 200,\n",
    "    'eval_steps': 300,\n",
    "    'save_steps': 600,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    \n",
    "    # Advanced training techniques\n",
    "    'weight_decay': 0.01,\n",
    "    'lr_scheduler_type': 'cosine',\n",
    "    'gradient_checkpointing': True,\n",
    "    'fp16': True,\n",
    "    'dataloader_num_workers': 2,\n",
    "    \n",
    "    # Regularization\n",
    "    'dropout': 0.1,\n",
    "    'attention_dropout': 0.1,\n",
    "}\n",
    "\n",
    "print(\" Enhanced training configuration loaded!\")\n",
    "print(\"Expected improvements:\")\n",
    "print(\"- Data Augmentation: 15-25% WER reduction\")\n",
    "print(\"- Medical Text Processing: 10-20% WER reduction\") \n",
    "print(\"- Larger Model (Small): 8-12% WER reduction\")\n",
    "print(\"- Advanced Training: 5-10% WER reduction\")\n",
    "print(\"- Total Expected WER: 8-12% (vs current 17.66%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1afb9e62-1baa-4505-b605-84e5178e8887",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-08-18T08:17:35.862233Z",
     "iopub.status.busy": "2025-08-18T08:17:35.861422Z",
     "iopub.status.idle": "2025-08-18T08:19:27.707679Z",
     "shell.execute_reply": "2025-08-18T08:19:27.706482Z",
     "shell.execute_reply.started": "2025-08-18T08:17:35.862206Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper processor for openai/whisper-tiny...\n",
      "Processor loaded. Target sample rate: 16000Hz\n",
      "Loading enhanced dataset...\n",
      "Loading 1000 samples from Shamus/United-Syn-Med...\n",
      "Attempting direct parquet loading...\n",
      "Found 33 parquet files\n",
      "Loading from data/train-00000-of-00033.parquet...\n",
      "Loaded 31 samples so far...\n",
      "Loading from data/train-00001-of-00033.parquet...\n",
      "Loaded 62 samples so far...\n",
      "Loading from data/train-00002-of-00033.parquet...\n",
      "Loaded 93 samples so far...\n",
      "Loading from data/train-00003-of-00033.parquet...\n",
      "Loaded 124 samples so far...\n",
      "Loading from data/train-00004-of-00033.parquet...\n",
      "Loaded 155 samples so far...\n",
      "Loading from data/train-00005-of-00033.parquet...\n",
      "Loaded 186 samples so far...\n",
      "Loading from data/train-00006-of-00033.parquet...\n",
      "Loaded 217 samples so far...\n",
      "Loading from data/train-00007-of-00033.parquet...\n",
      "Loaded 248 samples so far...\n",
      "Loading from data/train-00008-of-00033.parquet...\n",
      "Loaded 279 samples so far...\n",
      "Loading from data/train-00009-of-00033.parquet...\n",
      "Loaded 310 samples so far...\n",
      "Loading from data/train-00010-of-00033.parquet...\n",
      "Loaded 341 samples so far...\n",
      "Loading from data/train-00011-of-00033.parquet...\n",
      "Loaded 372 samples so far...\n",
      "Loading from data/train-00012-of-00033.parquet...\n",
      "Loaded 403 samples so far...\n",
      "Loading from data/train-00013-of-00033.parquet...\n",
      "Loaded 434 samples so far...\n",
      "Loading from data/train-00014-of-00033.parquet...\n",
      "Loaded 465 samples so far...\n",
      "Loading from data/train-00015-of-00033.parquet...\n",
      "Loaded 496 samples so far...\n",
      "Loading from data/train-00016-of-00033.parquet...\n",
      "Loaded 527 samples so far...\n",
      "Loading from data/train-00017-of-00033.parquet...\n",
      "Loaded 558 samples so far...\n",
      "Loading from data/train-00018-of-00033.parquet...\n",
      "Loaded 589 samples so far...\n",
      "Loading from data/train-00019-of-00033.parquet...\n",
      "Loaded 620 samples so far...\n",
      "Loading from data/train-00020-of-00033.parquet...\n",
      "Loaded 651 samples so far...\n",
      "Loading from data/train-00021-of-00033.parquet...\n",
      "Loaded 682 samples so far...\n",
      "Loading from data/train-00022-of-00033.parquet...\n",
      "Loaded 713 samples so far...\n",
      "Loading from data/train-00023-of-00033.parquet...\n",
      "Loaded 744 samples so far...\n",
      "Loading from data/train-00024-of-00033.parquet...\n",
      "Loaded 775 samples so far...\n",
      "Loading from data/train-00025-of-00033.parquet...\n",
      "Loaded 806 samples so far...\n",
      "Loading from data/train-00026-of-00033.parquet...\n",
      "Loaded 837 samples so far...\n",
      "Loading from data/train-00027-of-00033.parquet...\n",
      "Loaded 868 samples so far...\n",
      "Loading from data/train-00028-of-00033.parquet...\n",
      "Loaded 899 samples so far...\n",
      "Loading from data/train-00029-of-00033.parquet...\n",
      "Loaded 930 samples so far...\n",
      "Loading from data/train-00030-of-00033.parquet...\n",
      "Loaded 961 samples so far...\n",
      "Loading from data/train-00031-of-00033.parquet...\n",
      "Loaded 992 samples so far...\n",
      "Loading from data/train-00032-of-00033.parquet...\n",
      "Loaded 1000 samples so far...\n",
      "Train samples: 850\n",
      "Validation samples: 150\n",
      "Applying enhanced preprocessing with augmentation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d48997d8d04245aee04e567c1a9de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ed12b1e46344f19e6c91f46dfe087f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Error processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n",
      "Loading Whisper model: openai/whisper-tiny...\n",
      "Model loaded. Parameters: 37,760,640\n",
      "Starting enhanced training...\n",
      "Setting up trainer...\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1694, in forward\n    outputs = self.model(\n              ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1513, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 882, in forward\n    raise ValueError(\nValueError: Whisper expects the mel input features to be of length 3000, but found 2500. Make sure to pad the input mel features to 3000.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/336461442.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Run enhanced training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0menhanced_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_enhanced_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_36/336461442.py\u001b[0m in \u001b[0;36mtrain_enhanced_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Train enhanced model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting enhanced training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhanced_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENHANCED_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/99557485.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, config)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving final model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3744\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3745\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3747\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3810\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3811\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1694, in forward\n    outputs = self.model(\n              ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1513, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 882, in forward\n    raise ValueError(\nValueError: Whisper expects the mel input features to be of length 3000, but found 2500. Make sure to pad the input mel features to 3000.\n"
     ]
    }
   ],
   "source": [
    "# Complete Enhanced Training Pipeline\n",
    "def train_enhanced_model():\n",
    "    \"\"\"Train the enhanced model with all improvements.\"\"\"\n",
    "    \n",
    "    # Initialize enhanced preprocessor\n",
    "    enhanced_preprocessor = EnhancedAudioPreprocessor(\n",
    "        model_name=ENHANCED_CONFIG['model_name'],\n",
    "        target_sample_rate=ENHANCED_CONFIG['target_sample_rate'],\n",
    "        max_audio_length=ENHANCED_CONFIG['max_audio_length']\n",
    "    )\n",
    "    \n",
    "    # Load larger dataset\n",
    "    enhanced_data_loader = DataLoader(\n",
    "        dataset_name=ENHANCED_CONFIG['dataset_name'],\n",
    "        num_samples=ENHANCED_CONFIG['num_samples'],\n",
    "        train_split_ratio=ENHANCED_CONFIG['train_split_ratio']\n",
    "    )\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    print(\"Loading enhanced dataset...\")\n",
    "    samples = enhanced_data_loader.load_dataset_subset()\n",
    "    train_dataset, val_dataset = enhanced_data_loader.create_train_val_split(samples)\n",
    "    \n",
    "    # Apply enhanced preprocessing with augmentation\n",
    "    print(\"Applying enhanced preprocessing with augmentation...\")\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda batch: enhanced_preprocessor.preprocess_batch(batch, augment=True),\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    val_dataset = val_dataset.map(\n",
    "        lambda batch: enhanced_preprocessor.preprocess_batch(batch, augment=False),\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "        remove_columns=val_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    # Initialize enhanced trainer\n",
    "    enhanced_trainer = WhisperTrainer(\n",
    "        model_name=ENHANCED_CONFIG['model_name'],\n",
    "        output_dir=ENHANCED_CONFIG['output_dir']\n",
    "    )\n",
    "    \n",
    "    # Train enhanced model\n",
    "    print(\"Starting enhanced training...\")\n",
    "    trainer = enhanced_trainer.train(train_dataset, val_dataset, ENHANCED_CONFIG)\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Run enhanced training\n",
    "enhanced_trainer = train_enhanced_model()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "019f56acf34344aebdfa7b3ee15f3261": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0be4520c604f4c938ae07d026db67fb1",
      "placeholder": "",
      "style": "IPY_MODEL_d576293895f24082a00b76570baa567c",
      "value": "Map:100%"
     }
    },
    "01b6552018cc425ead85eae0e17fda06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "02317272e64c4fce899c59e381ee0690": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2170942122d74128ba878bbc098a20cd",
      "placeholder": "",
      "style": "IPY_MODEL_78f8f798727b4556ab94a40afe3702f7",
      "value": "vocab.json:"
     }
    },
    "02935574cb384d6b84fad32f3d93f4c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e59c3e69e09f420cb6e1b80ffe223136",
      "placeholder": "",
      "style": "IPY_MODEL_01b6552018cc425ead85eae0e17fda06",
      "value": "merges.txt:"
     }
    },
    "0365c5e1bbaa44acbb077c5bdf7461fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05f78daa68a04dd4a2b4a84e5ccfebd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56750607047846f593672599988e32dc",
       "IPY_MODEL_98f6bd36ee97497bbd9b7e006b10b538",
       "IPY_MODEL_60fa01c6a764403aa74defec1f0c2bd0"
      ],
      "layout": "IPY_MODEL_3c2dbfa45e0b4d9b9303621f433f6165"
     }
    },
    "0be4520c604f4c938ae07d026db67fb1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cfcea8a22eb4a0d986f1da6e0257ce0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dddc4553e0549db97f215be1b53c51f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec4dcc28a67546ee8265ff2dcf24b2b6",
       "IPY_MODEL_0f4671829e9149318d030838ca788766",
       "IPY_MODEL_8c8e25c295f34225933d590ecf002f3a"
      ],
      "layout": "IPY_MODEL_b713465adfce4da58ab44cdbbfe11e85"
     }
    },
    "0f4671829e9149318d030838ca788766": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af32c32d09104beea9d774e82b6b1fdb",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_28cf4d3041b44138b05fde3a6c0c71db",
      "value": 1
     }
    },
    "10d4f72e2f0e4bb3a7263a1ef307030b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c8da8278a4124057bf6ff7bb351210d2",
       "IPY_MODEL_45a9e6e85f544d6abaf81437e78c741f",
       "IPY_MODEL_dcc2ec5b37fa4b80b0c72f24ab5bcf72"
      ],
      "layout": "IPY_MODEL_8faef5c828b64c2fbf5ed3d30a73a8cd"
     }
    },
    "10f7ee7868da4ec0ab1e57342d8abf1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "11d2d86b57c84318827ea42aa4b59b8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13d1c6d2eeef46f79ca57f6fc4acb0be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "150f986d62bb41c9a56fd38e4c954020": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84c50442ccbe41868faad529f5bb2f3e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7696c8d022c8457fba0fdaa1cdfd411d",
      "value": 1
     }
    },
    "1867ff7ca04e4a54987eefcf6cf7ec9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a6b38aca0ce4adb9898d9cac042cf11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1b198dd6dd524741abe5df656b99a309": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2170942122d74128ba878bbc098a20cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2234a1a88efc4d5fae411866a3adf66e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "252a6ceac04d4dadacb05eb742067031": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_79018e6e9f2745749ad812a6f6713bc3",
       "IPY_MODEL_e31be64fc0104355a83f09cc6a0031f1",
       "IPY_MODEL_f8e29ea23cd64bb4a5c6d03ca2576fde"
      ],
      "layout": "IPY_MODEL_d5300ea0814440b79fbf23f489a758ed"
     }
    },
    "275da4af86f6427d844bbace06cb7bf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "28cf4d3041b44138b05fde3a6c0c71db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "294e60cd05a04286a238ce7282503b9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "32943b81e0b64b3fa2af1f4f66f781ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5d8385b58774cbea455b378a9186443",
      "placeholder": "",
      "style": "IPY_MODEL_353f3db573e64d238eaacb91bae1c5fa",
      "value": "560/560[02:04&lt;00:00,4.12examples/s]"
     }
    },
    "334b8fe0a13247578e17e4b56a852122": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df43c75037a046f9a64187ba1c94d926",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7966c6cded3742a885c79e635f99fb75",
      "value": 1
     }
    },
    "334ccf4dec2b47ffa3297075d90f9190": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a04d448a56e749b2a3fb7f193a22bcd2",
       "IPY_MODEL_775960d2e6934bd899e40fad5e93a148",
       "IPY_MODEL_7bae267dec7a4c9687fb19d6cd903e33"
      ],
      "layout": "IPY_MODEL_8157314a37e54b3facfcc0a44142806f"
     }
    },
    "339ad729a3e4445e8524fa66f7aabb45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "353f3db573e64d238eaacb91bae1c5fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "358261fd07324927b6a7455df91e736b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3afa3942bd924152bad0c3dfd3e5dd3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13d1c6d2eeef46f79ca57f6fc4acb0be",
      "max": 151061672,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8074c6f81d0f403d8dd06b6fc4a7bf83",
      "value": 151061672
     }
    },
    "3c2dbfa45e0b4d9b9303621f433f6165": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cb416067c7443e8b745e61d8d2e76dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d7cec6d6c30410aaa06cd380062c9a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e8dd3c7670340ad944cea975f7bc3bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "421a046bb06544bfb3641e1747922e7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4467cacde89c470ba36ceb991ccf804b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45a9e6e85f544d6abaf81437e78c741f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_513aeb3eb72c455d88f296a3303fe51d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a4eae7a0e44f4395871ef9c9ecdba3a3",
      "value": 1
     }
    },
    "475528ff173f47858b79c97a5a1b15e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a9a9f646b544aa09da77866cf344d2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ae671de23f3441887cbc7a3953f9c90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cfcea8a22eb4a0d986f1da6e0257ce0",
      "placeholder": "",
      "style": "IPY_MODEL_eb0ba123434940768511f97b24d9a65d",
      "value": "151M/151M[00:01&lt;00:00,67.2MB/s]"
     }
    },
    "4d63aff65c524fbfbc2d35081a29f5d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc33cb13dd604d74a9b4023b133cb4da",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a8102832b23944f58faed6032e863aea",
      "value": 1
     }
    },
    "513aeb3eb72c455d88f296a3303fe51d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "52587a0ecb9d4f16b96a5e284c5c1c7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "56750607047846f593672599988e32dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1cf0f30095741ff910247f5a84be592",
      "placeholder": "",
      "style": "IPY_MODEL_358261fd07324927b6a7455df91e736b",
      "value": "normalizer.json:"
     }
    },
    "58fdbfad90444f7e9575d22d25f0b9a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e9ec97932474f0882e7e9300115695f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60a5b628ad514e1c97f9ca5728fa6dbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60fa01c6a764403aa74defec1f0c2bd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8148517597314edb9ca2ab6179877954",
      "placeholder": "",
      "style": "IPY_MODEL_9dbc5ba6a7e64e2f84bb3613c0a38c63",
      "value": "52.7k/?[00:00&lt;00:00,6.38MB/s]"
     }
    },
    "61530ed0b77a4e5f9c26be8cb759d864": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_981c5744d82145d39d3dabb88cdf4333",
       "IPY_MODEL_3afa3942bd924152bad0c3dfd3e5dd3f",
       "IPY_MODEL_4ae671de23f3441887cbc7a3953f9c90"
      ],
      "layout": "IPY_MODEL_75c4e30b3a5643688941d2698f81ed29"
     }
    },
    "61e3e744b3b44bd9bea0de1d8bd3bdcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62a8e5ea484341a289feab09fe608859": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "64b72284dc7c4fab9eff625817401828": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6abb6151ab454aac9762bc6c1917373d",
       "IPY_MODEL_9a7218b02f924029abb0b5e22afde9ed",
       "IPY_MODEL_32943b81e0b64b3fa2af1f4f66f781ec"
      ],
      "layout": "IPY_MODEL_421a046bb06544bfb3641e1747922e7d"
     }
    },
    "65bfd81001294311a03d464b29987164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b198dd6dd524741abe5df656b99a309",
      "placeholder": "",
      "style": "IPY_MODEL_4467cacde89c470ba36ceb991ccf804b",
      "value": "special_tokens_map.json:"
     }
    },
    "65f2832bce7748b986129a9361d4af4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6610977290f54365997586d6567e668c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66c1cb15102b4d278c01b71f189152be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a775153f0ea4c42b9c1b36476ba882b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6abb6151ab454aac9762bc6c1917373d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_339ad729a3e4445e8524fa66f7aabb45",
      "placeholder": "",
      "style": "IPY_MODEL_e936d01f314648e098bee0a9e133ec7f",
      "value": "Map:100%"
     }
    },
    "6ba64c270f0f4f7ab02d0471153ff58a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6df02e70d3b04da8b134e4a26b657152": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "6e2fdc7055a24ec68ba5d68b60530fa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "709f36990648403f82bb2d8012fa005d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75c4e30b3a5643688941d2698f81ed29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7690e3a451944633aeb89a3bc25a809d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "7693ad9de082400ea858866c77f46f2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_019f56acf34344aebdfa7b3ee15f3261",
       "IPY_MODEL_dbe116372cfa439083b6d44863e50453",
       "IPY_MODEL_7946a35895da4b3ea78361d84eb91cda"
      ],
      "layout": "IPY_MODEL_848b664d20a4476da239f1e6b5fbf8a7"
     }
    },
    "7696c8d022c8457fba0fdaa1cdfd411d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "77159aaec226492fbf83cd80f4726dcf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "775960d2e6934bd899e40fad5e93a148": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7690e3a451944633aeb89a3bc25a809d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_52587a0ecb9d4f16b96a5e284c5c1c7c",
      "value": 1
     }
    },
    "78bf40d6a36e4ae4ac35990aa44f5d9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "78f8f798727b4556ab94a40afe3702f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79018e6e9f2745749ad812a6f6713bc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61e3e744b3b44bd9bea0de1d8bd3bdcb",
      "placeholder": "",
      "style": "IPY_MODEL_275da4af86f6427d844bbace06cb7bf8",
      "value": "tokenizer.json:"
     }
    },
    "7923ada824f343f8a26d2fe5cd3999dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7946a35895da4b3ea78361d84eb91cda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77159aaec226492fbf83cd80f4726dcf",
      "placeholder": "",
      "style": "IPY_MODEL_6a775153f0ea4c42b9c1b36476ba882b",
      "value": "140/140[00:30&lt;00:00,4.52examples/s]"
     }
    },
    "7966c6cded3742a885c79e635f99fb75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7bae267dec7a4c9687fb19d6cd903e33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b69023098a164d1a8d1e5628b01a09ec",
      "placeholder": "",
      "style": "IPY_MODEL_7923ada824f343f8a26d2fe5cd3999dd",
      "value": "34.6k/?[00:00&lt;00:00,4.67MB/s]"
     }
    },
    "7f840f99840c41aab032edaba86bdb40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8074c6f81d0f403d8dd06b6fc4a7bf83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8128a80de9714b66bba0f79c0c940071": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_02935574cb384d6b84fad32f3d93f4c8",
       "IPY_MODEL_d3414bec85964ba8a9a5270fd09dced7",
       "IPY_MODEL_a40c1fab1563407b904db049f9450c0e"
      ],
      "layout": "IPY_MODEL_94cd766edca34b319e87de8dfc7044b2"
     }
    },
    "8148517597314edb9ca2ab6179877954": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8157314a37e54b3facfcc0a44142806f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8179b13c47ba46feb3b9cb009740587c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83ace04551a745a68f33c9c1105e3b01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e8dd3c7670340ad944cea975f7bc3bb",
      "placeholder": "",
      "style": "IPY_MODEL_3cb416067c7443e8b745e61d8d2e76dd",
      "value": "836k/?[00:00&lt;00:00,18.0MB/s]"
     }
    },
    "848b664d20a4476da239f1e6b5fbf8a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84c50442ccbe41868faad529f5bb2f3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "84cd0cb9060744f1892d1efd4bd7adbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c8e25c295f34225933d590ecf002f3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d7cec6d6c30410aaa06cd380062c9a7",
      "placeholder": "",
      "style": "IPY_MODEL_10f7ee7868da4ec0ab1e57342d8abf1f",
      "value": "185k/?[00:00&lt;00:00,18.8MB/s]"
     }
    },
    "8faef5c828b64c2fbf5ed3d30a73a8cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "927848144429471abfaa5671841c520e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2fd054a4e0945f8a714d69cb61bb674",
      "placeholder": "",
      "style": "IPY_MODEL_adbcba351b8349bc9293ba950bfc6d59",
      "value": "tokenizer_config.json:"
     }
    },
    "93d4de194b0146e0bfbf1f9203a8efbc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "948d4af818ce4da0a4499a758aeccc7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bfde110d84074490891834a0bda7f1d3",
      "placeholder": "",
      "style": "IPY_MODEL_c23e74984a7549199308b25d6721d27d",
      "value": "2.19k/?[00:00&lt;00:00,298kB/s]"
     }
    },
    "94cd766edca34b319e87de8dfc7044b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9685febcf74c401db8e24574b84af2b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93d4de194b0146e0bfbf1f9203a8efbc",
      "placeholder": "",
      "style": "IPY_MODEL_11d2d86b57c84318827ea42aa4b59b8f",
      "value": "283k/?[00:00&lt;00:00,34.0MB/s]"
     }
    },
    "981c5744d82145d39d3dabb88cdf4333": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60a5b628ad514e1c97f9ca5728fa6dbf",
      "placeholder": "",
      "style": "IPY_MODEL_475528ff173f47858b79c97a5a1b15e8",
      "value": "model.safetensors:100%"
     }
    },
    "98f6bd36ee97497bbd9b7e006b10b538": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78bf40d6a36e4ae4ac35990aa44f5d9a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_294e60cd05a04286a238ce7282503b9a",
      "value": 1
     }
    },
    "9a7218b02f924029abb0b5e22afde9ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9b48d3f09d04d219176844fe1ee9046",
      "max": 560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1a6b38aca0ce4adb9898d9cac042cf11",
      "value": 560
     }
    },
    "9c054219d6ff439e850de50206933433": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9dbc5ba6a7e64e2f84bb3613c0a38c63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a04d448a56e749b2a3fb7f193a22bcd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2234a1a88efc4d5fae411866a3adf66e",
      "placeholder": "",
      "style": "IPY_MODEL_6ba64c270f0f4f7ab02d0471153ff58a",
      "value": "added_tokens.json:"
     }
    },
    "a2fd054a4e0945f8a714d69cb61bb674": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a36d9401c10b4cd5920cca6979a6b4e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c054219d6ff439e850de50206933433",
      "placeholder": "",
      "style": "IPY_MODEL_6e2fdc7055a24ec68ba5d68b60530fa4",
      "value": "3.75k/?[00:00&lt;00:00,481kB/s]"
     }
    },
    "a3ee6162d2134f53b95f156fc645963a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a40c1fab1563407b904db049f9450c0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84cd0cb9060744f1892d1efd4bd7adbd",
      "placeholder": "",
      "style": "IPY_MODEL_eb472304e4e942c091f4230917bd4e9c",
      "value": "494k/?[00:00&lt;00:00,43.2MB/s]"
     }
    },
    "a4eae7a0e44f4395871ef9c9ecdba3a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a802a4cbe23e4938b13e783f737670df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8102832b23944f58faed6032e863aea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a95bc212127f44a2a4462e053b2dd42e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f462f7d962d5405d864068efc8ba384a",
       "IPY_MODEL_334b8fe0a13247578e17e4b56a852122",
       "IPY_MODEL_a36d9401c10b4cd5920cca6979a6b4e8"
      ],
      "layout": "IPY_MODEL_4a9a9f646b544aa09da77866cf344d2b"
     }
    },
    "aab1012473c840ccb99b5508c1626d88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adbcba351b8349bc9293ba950bfc6d59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af32c32d09104beea9d774e82b6b1fdb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "b69023098a164d1a8d1e5628b01a09ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b713465adfce4da58ab44cdbbfe11e85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfde110d84074490891834a0bda7f1d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c23e74984a7549199308b25d6721d27d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c5ce27ea0339491da34eec3f9874ca03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c8da8278a4124057bf6ff7bb351210d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6610977290f54365997586d6567e668c",
      "placeholder": "",
      "style": "IPY_MODEL_7f840f99840c41aab032edaba86bdb40",
      "value": "config.json:"
     }
    },
    "c9b27b25160b40a589b20dcfc2d38de6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_927848144429471abfaa5671841c520e",
       "IPY_MODEL_4d63aff65c524fbfbc2d35081a29f5d9",
       "IPY_MODEL_9685febcf74c401db8e24574b84af2b3"
      ],
      "layout": "IPY_MODEL_aab1012473c840ccb99b5508c1626d88"
     }
    },
    "c9b48d3f09d04d219176844fe1ee9046": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3414bec85964ba8a9a5270fd09dced7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc2c4395392b474f93e941b352e38ca4",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d4adf3b026b44ec789233e36a0f0c64c",
      "value": 1
     }
    },
    "d37a192fc6274871b0c2230dab76db86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62a8e5ea484341a289feab09fe608859",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8179b13c47ba46feb3b9cb009740587c",
      "value": 1
     }
    },
    "d4adf3b026b44ec789233e36a0f0c64c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5300ea0814440b79fbf23f489a758ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d576293895f24082a00b76570baa567c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5d8385b58774cbea455b378a9186443": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbe116372cfa439083b6d44863e50453": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65f2832bce7748b986129a9361d4af4d",
      "max": 140,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5ce27ea0339491da34eec3f9874ca03",
      "value": 140
     }
    },
    "dc2c4395392b474f93e941b352e38ca4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "dcc2ec5b37fa4b80b0c72f24ab5bcf72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0365c5e1bbaa44acbb077c5bdf7461fd",
      "placeholder": "",
      "style": "IPY_MODEL_a802a4cbe23e4938b13e783f737670df",
      "value": "1.98k/?[00:00&lt;00:00,245kB/s]"
     }
    },
    "df43c75037a046f9a64187ba1c94d926": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "e31be64fc0104355a83f09cc6a0031f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6df02e70d3b04da8b134e4a26b657152",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1867ff7ca04e4a54987eefcf6cf7ec9b",
      "value": 1
     }
    },
    "e59c3e69e09f420cb6e1b80ffe223136": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e81b51ef0132489a8d8c73fbe25d4692": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e936d01f314648e098bee0a9e133ec7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea9152547d904d11ab2aeec6db3fdc89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb0ba123434940768511f97b24d9a65d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb472304e4e942c091f4230917bd4e9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec4dcc28a67546ee8265ff2dcf24b2b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3ee6162d2134f53b95f156fc645963a",
      "placeholder": "",
      "style": "IPY_MODEL_709f36990648403f82bb2d8012fa005d",
      "value": "preprocessor_config.json:"
     }
    },
    "efabae23841d4f08a97feeed631c6ed8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efb34333e20c4b719b6d30668c6952f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_02317272e64c4fce899c59e381ee0690",
       "IPY_MODEL_150f986d62bb41c9a56fd38e4c954020",
       "IPY_MODEL_83ace04551a745a68f33c9c1105e3b01"
      ],
      "layout": "IPY_MODEL_e81b51ef0132489a8d8c73fbe25d4692"
     }
    },
    "f1cf0f30095741ff910247f5a84be592": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f462f7d962d5405d864068efc8ba384a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66c1cb15102b4d278c01b71f189152be",
      "placeholder": "",
      "style": "IPY_MODEL_58fdbfad90444f7e9575d22d25f0b9a1",
      "value": "generation_config.json:"
     }
    },
    "f77ba27784944863819ba100c650f793": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_65bfd81001294311a03d464b29987164",
       "IPY_MODEL_d37a192fc6274871b0c2230dab76db86",
       "IPY_MODEL_948d4af818ce4da0a4499a758aeccc7f"
      ],
      "layout": "IPY_MODEL_efabae23841d4f08a97feeed631c6ed8"
     }
    },
    "f8e29ea23cd64bb4a5c6d03ca2576fde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea9152547d904d11ab2aeec6db3fdc89",
      "placeholder": "",
      "style": "IPY_MODEL_5e9ec97932474f0882e7e9300115695f",
      "value": "2.48M/?[00:00&lt;00:00,114MB/s]"
     }
    },
    "fc33cb13dd604d74a9b4023b133cb4da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

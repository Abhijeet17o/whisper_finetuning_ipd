{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0dddc4553e0549db97f215be1b53c51f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec4dcc28a67546ee8265ff2dcf24b2b6","IPY_MODEL_0f4671829e9149318d030838ca788766","IPY_MODEL_8c8e25c295f34225933d590ecf002f3a"],"layout":"IPY_MODEL_b713465adfce4da58ab44cdbbfe11e85"}},"ec4dcc28a67546ee8265ff2dcf24b2b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3ee6162d2134f53b95f156fc645963a","placeholder":"​","style":"IPY_MODEL_709f36990648403f82bb2d8012fa005d","value":"preprocessor_config.json: "}},"0f4671829e9149318d030838ca788766":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_af32c32d09104beea9d774e82b6b1fdb","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_28cf4d3041b44138b05fde3a6c0c71db","value":1}},"8c8e25c295f34225933d590ecf002f3a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d7cec6d6c30410aaa06cd380062c9a7","placeholder":"​","style":"IPY_MODEL_10f7ee7868da4ec0ab1e57342d8abf1f","value":" 185k/? [00:00&lt;00:00, 18.8MB/s]"}},"b713465adfce4da58ab44cdbbfe11e85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3ee6162d2134f53b95f156fc645963a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"709f36990648403f82bb2d8012fa005d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af32c32d09104beea9d774e82b6b1fdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"28cf4d3041b44138b05fde3a6c0c71db":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d7cec6d6c30410aaa06cd380062c9a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10f7ee7868da4ec0ab1e57342d8abf1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9b27b25160b40a589b20dcfc2d38de6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_927848144429471abfaa5671841c520e","IPY_MODEL_4d63aff65c524fbfbc2d35081a29f5d9","IPY_MODEL_9685febcf74c401db8e24574b84af2b3"],"layout":"IPY_MODEL_aab1012473c840ccb99b5508c1626d88"}},"927848144429471abfaa5671841c520e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2fd054a4e0945f8a714d69cb61bb674","placeholder":"​","style":"IPY_MODEL_adbcba351b8349bc9293ba950bfc6d59","value":"tokenizer_config.json: "}},"4d63aff65c524fbfbc2d35081a29f5d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc33cb13dd604d74a9b4023b133cb4da","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8102832b23944f58faed6032e863aea","value":1}},"9685febcf74c401db8e24574b84af2b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93d4de194b0146e0bfbf1f9203a8efbc","placeholder":"​","style":"IPY_MODEL_11d2d86b57c84318827ea42aa4b59b8f","value":" 283k/? [00:00&lt;00:00, 34.0MB/s]"}},"aab1012473c840ccb99b5508c1626d88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2fd054a4e0945f8a714d69cb61bb674":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adbcba351b8349bc9293ba950bfc6d59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc33cb13dd604d74a9b4023b133cb4da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a8102832b23944f58faed6032e863aea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93d4de194b0146e0bfbf1f9203a8efbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11d2d86b57c84318827ea42aa4b59b8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efb34333e20c4b719b6d30668c6952f1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02317272e64c4fce899c59e381ee0690","IPY_MODEL_150f986d62bb41c9a56fd38e4c954020","IPY_MODEL_83ace04551a745a68f33c9c1105e3b01"],"layout":"IPY_MODEL_e81b51ef0132489a8d8c73fbe25d4692"}},"02317272e64c4fce899c59e381ee0690":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2170942122d74128ba878bbc098a20cd","placeholder":"​","style":"IPY_MODEL_78f8f798727b4556ab94a40afe3702f7","value":"vocab.json: "}},"150f986d62bb41c9a56fd38e4c954020":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_84c50442ccbe41868faad529f5bb2f3e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7696c8d022c8457fba0fdaa1cdfd411d","value":1}},"83ace04551a745a68f33c9c1105e3b01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e8dd3c7670340ad944cea975f7bc3bb","placeholder":"​","style":"IPY_MODEL_3cb416067c7443e8b745e61d8d2e76dd","value":" 836k/? [00:00&lt;00:00, 18.0MB/s]"}},"e81b51ef0132489a8d8c73fbe25d4692":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2170942122d74128ba878bbc098a20cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78f8f798727b4556ab94a40afe3702f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84c50442ccbe41868faad529f5bb2f3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7696c8d022c8457fba0fdaa1cdfd411d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e8dd3c7670340ad944cea975f7bc3bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cb416067c7443e8b745e61d8d2e76dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"252a6ceac04d4dadacb05eb742067031":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_79018e6e9f2745749ad812a6f6713bc3","IPY_MODEL_e31be64fc0104355a83f09cc6a0031f1","IPY_MODEL_f8e29ea23cd64bb4a5c6d03ca2576fde"],"layout":"IPY_MODEL_d5300ea0814440b79fbf23f489a758ed"}},"79018e6e9f2745749ad812a6f6713bc3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61e3e744b3b44bd9bea0de1d8bd3bdcb","placeholder":"​","style":"IPY_MODEL_275da4af86f6427d844bbace06cb7bf8","value":"tokenizer.json: "}},"e31be64fc0104355a83f09cc6a0031f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6df02e70d3b04da8b134e4a26b657152","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1867ff7ca04e4a54987eefcf6cf7ec9b","value":1}},"f8e29ea23cd64bb4a5c6d03ca2576fde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea9152547d904d11ab2aeec6db3fdc89","placeholder":"​","style":"IPY_MODEL_5e9ec97932474f0882e7e9300115695f","value":" 2.48M/? [00:00&lt;00:00, 114MB/s]"}},"d5300ea0814440b79fbf23f489a758ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61e3e744b3b44bd9bea0de1d8bd3bdcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"275da4af86f6427d844bbace06cb7bf8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6df02e70d3b04da8b134e4a26b657152":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"1867ff7ca04e4a54987eefcf6cf7ec9b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea9152547d904d11ab2aeec6db3fdc89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e9ec97932474f0882e7e9300115695f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8128a80de9714b66bba0f79c0c940071":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02935574cb384d6b84fad32f3d93f4c8","IPY_MODEL_d3414bec85964ba8a9a5270fd09dced7","IPY_MODEL_a40c1fab1563407b904db049f9450c0e"],"layout":"IPY_MODEL_94cd766edca34b319e87de8dfc7044b2"}},"02935574cb384d6b84fad32f3d93f4c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e59c3e69e09f420cb6e1b80ffe223136","placeholder":"​","style":"IPY_MODEL_01b6552018cc425ead85eae0e17fda06","value":"merges.txt: "}},"d3414bec85964ba8a9a5270fd09dced7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc2c4395392b474f93e941b352e38ca4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d4adf3b026b44ec789233e36a0f0c64c","value":1}},"a40c1fab1563407b904db049f9450c0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84cd0cb9060744f1892d1efd4bd7adbd","placeholder":"​","style":"IPY_MODEL_eb472304e4e942c091f4230917bd4e9c","value":" 494k/? [00:00&lt;00:00, 43.2MB/s]"}},"94cd766edca34b319e87de8dfc7044b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e59c3e69e09f420cb6e1b80ffe223136":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01b6552018cc425ead85eae0e17fda06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc2c4395392b474f93e941b352e38ca4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d4adf3b026b44ec789233e36a0f0c64c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84cd0cb9060744f1892d1efd4bd7adbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb472304e4e942c091f4230917bd4e9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05f78daa68a04dd4a2b4a84e5ccfebd3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_56750607047846f593672599988e32dc","IPY_MODEL_98f6bd36ee97497bbd9b7e006b10b538","IPY_MODEL_60fa01c6a764403aa74defec1f0c2bd0"],"layout":"IPY_MODEL_3c2dbfa45e0b4d9b9303621f433f6165"}},"56750607047846f593672599988e32dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1cf0f30095741ff910247f5a84be592","placeholder":"​","style":"IPY_MODEL_358261fd07324927b6a7455df91e736b","value":"normalizer.json: "}},"98f6bd36ee97497bbd9b7e006b10b538":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78bf40d6a36e4ae4ac35990aa44f5d9a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_294e60cd05a04286a238ce7282503b9a","value":1}},"60fa01c6a764403aa74defec1f0c2bd0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8148517597314edb9ca2ab6179877954","placeholder":"​","style":"IPY_MODEL_9dbc5ba6a7e64e2f84bb3613c0a38c63","value":" 52.7k/? [00:00&lt;00:00, 6.38MB/s]"}},"3c2dbfa45e0b4d9b9303621f433f6165":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1cf0f30095741ff910247f5a84be592":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"358261fd07324927b6a7455df91e736b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78bf40d6a36e4ae4ac35990aa44f5d9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"294e60cd05a04286a238ce7282503b9a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8148517597314edb9ca2ab6179877954":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dbc5ba6a7e64e2f84bb3613c0a38c63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"334ccf4dec2b47ffa3297075d90f9190":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a04d448a56e749b2a3fb7f193a22bcd2","IPY_MODEL_775960d2e6934bd899e40fad5e93a148","IPY_MODEL_7bae267dec7a4c9687fb19d6cd903e33"],"layout":"IPY_MODEL_8157314a37e54b3facfcc0a44142806f"}},"a04d448a56e749b2a3fb7f193a22bcd2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2234a1a88efc4d5fae411866a3adf66e","placeholder":"​","style":"IPY_MODEL_6ba64c270f0f4f7ab02d0471153ff58a","value":"added_tokens.json: "}},"775960d2e6934bd899e40fad5e93a148":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7690e3a451944633aeb89a3bc25a809d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_52587a0ecb9d4f16b96a5e284c5c1c7c","value":1}},"7bae267dec7a4c9687fb19d6cd903e33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b69023098a164d1a8d1e5628b01a09ec","placeholder":"​","style":"IPY_MODEL_7923ada824f343f8a26d2fe5cd3999dd","value":" 34.6k/? [00:00&lt;00:00, 4.67MB/s]"}},"8157314a37e54b3facfcc0a44142806f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2234a1a88efc4d5fae411866a3adf66e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ba64c270f0f4f7ab02d0471153ff58a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7690e3a451944633aeb89a3bc25a809d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"52587a0ecb9d4f16b96a5e284c5c1c7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b69023098a164d1a8d1e5628b01a09ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7923ada824f343f8a26d2fe5cd3999dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f77ba27784944863819ba100c650f793":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_65bfd81001294311a03d464b29987164","IPY_MODEL_d37a192fc6274871b0c2230dab76db86","IPY_MODEL_948d4af818ce4da0a4499a758aeccc7f"],"layout":"IPY_MODEL_efabae23841d4f08a97feeed631c6ed8"}},"65bfd81001294311a03d464b29987164":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b198dd6dd524741abe5df656b99a309","placeholder":"​","style":"IPY_MODEL_4467cacde89c470ba36ceb991ccf804b","value":"special_tokens_map.json: "}},"d37a192fc6274871b0c2230dab76db86":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_62a8e5ea484341a289feab09fe608859","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8179b13c47ba46feb3b9cb009740587c","value":1}},"948d4af818ce4da0a4499a758aeccc7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfde110d84074490891834a0bda7f1d3","placeholder":"​","style":"IPY_MODEL_c23e74984a7549199308b25d6721d27d","value":" 2.19k/? [00:00&lt;00:00, 298kB/s]"}},"efabae23841d4f08a97feeed631c6ed8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b198dd6dd524741abe5df656b99a309":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4467cacde89c470ba36ceb991ccf804b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62a8e5ea484341a289feab09fe608859":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"8179b13c47ba46feb3b9cb009740587c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bfde110d84074490891834a0bda7f1d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c23e74984a7549199308b25d6721d27d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64b72284dc7c4fab9eff625817401828":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6abb6151ab454aac9762bc6c1917373d","IPY_MODEL_9a7218b02f924029abb0b5e22afde9ed","IPY_MODEL_32943b81e0b64b3fa2af1f4f66f781ec"],"layout":"IPY_MODEL_421a046bb06544bfb3641e1747922e7d"}},"6abb6151ab454aac9762bc6c1917373d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_339ad729a3e4445e8524fa66f7aabb45","placeholder":"​","style":"IPY_MODEL_e936d01f314648e098bee0a9e133ec7f","value":"Map: 100%"}},"9a7218b02f924029abb0b5e22afde9ed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9b48d3f09d04d219176844fe1ee9046","max":560,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a6b38aca0ce4adb9898d9cac042cf11","value":560}},"32943b81e0b64b3fa2af1f4f66f781ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5d8385b58774cbea455b378a9186443","placeholder":"​","style":"IPY_MODEL_353f3db573e64d238eaacb91bae1c5fa","value":" 560/560 [02:04&lt;00:00,  4.12 examples/s]"}},"421a046bb06544bfb3641e1747922e7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"339ad729a3e4445e8524fa66f7aabb45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e936d01f314648e098bee0a9e133ec7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9b48d3f09d04d219176844fe1ee9046":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a6b38aca0ce4adb9898d9cac042cf11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5d8385b58774cbea455b378a9186443":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"353f3db573e64d238eaacb91bae1c5fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7693ad9de082400ea858866c77f46f2b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_019f56acf34344aebdfa7b3ee15f3261","IPY_MODEL_dbe116372cfa439083b6d44863e50453","IPY_MODEL_7946a35895da4b3ea78361d84eb91cda"],"layout":"IPY_MODEL_848b664d20a4476da239f1e6b5fbf8a7"}},"019f56acf34344aebdfa7b3ee15f3261":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0be4520c604f4c938ae07d026db67fb1","placeholder":"​","style":"IPY_MODEL_d576293895f24082a00b76570baa567c","value":"Map: 100%"}},"dbe116372cfa439083b6d44863e50453":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65f2832bce7748b986129a9361d4af4d","max":140,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5ce27ea0339491da34eec3f9874ca03","value":140}},"7946a35895da4b3ea78361d84eb91cda":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77159aaec226492fbf83cd80f4726dcf","placeholder":"​","style":"IPY_MODEL_6a775153f0ea4c42b9c1b36476ba882b","value":" 140/140 [00:30&lt;00:00,  4.52 examples/s]"}},"848b664d20a4476da239f1e6b5fbf8a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0be4520c604f4c938ae07d026db67fb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d576293895f24082a00b76570baa567c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65f2832bce7748b986129a9361d4af4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5ce27ea0339491da34eec3f9874ca03":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77159aaec226492fbf83cd80f4726dcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a775153f0ea4c42b9c1b36476ba882b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10d4f72e2f0e4bb3a7263a1ef307030b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c8da8278a4124057bf6ff7bb351210d2","IPY_MODEL_45a9e6e85f544d6abaf81437e78c741f","IPY_MODEL_dcc2ec5b37fa4b80b0c72f24ab5bcf72"],"layout":"IPY_MODEL_8faef5c828b64c2fbf5ed3d30a73a8cd"}},"c8da8278a4124057bf6ff7bb351210d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6610977290f54365997586d6567e668c","placeholder":"​","style":"IPY_MODEL_7f840f99840c41aab032edaba86bdb40","value":"config.json: "}},"45a9e6e85f544d6abaf81437e78c741f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_513aeb3eb72c455d88f296a3303fe51d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4eae7a0e44f4395871ef9c9ecdba3a3","value":1}},"dcc2ec5b37fa4b80b0c72f24ab5bcf72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0365c5e1bbaa44acbb077c5bdf7461fd","placeholder":"​","style":"IPY_MODEL_a802a4cbe23e4938b13e783f737670df","value":" 1.98k/? [00:00&lt;00:00, 245kB/s]"}},"8faef5c828b64c2fbf5ed3d30a73a8cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6610977290f54365997586d6567e668c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f840f99840c41aab032edaba86bdb40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"513aeb3eb72c455d88f296a3303fe51d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a4eae7a0e44f4395871ef9c9ecdba3a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0365c5e1bbaa44acbb077c5bdf7461fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a802a4cbe23e4938b13e783f737670df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61530ed0b77a4e5f9c26be8cb759d864":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_981c5744d82145d39d3dabb88cdf4333","IPY_MODEL_3afa3942bd924152bad0c3dfd3e5dd3f","IPY_MODEL_4ae671de23f3441887cbc7a3953f9c90"],"layout":"IPY_MODEL_75c4e30b3a5643688941d2698f81ed29"}},"981c5744d82145d39d3dabb88cdf4333":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60a5b628ad514e1c97f9ca5728fa6dbf","placeholder":"​","style":"IPY_MODEL_475528ff173f47858b79c97a5a1b15e8","value":"model.safetensors: 100%"}},"3afa3942bd924152bad0c3dfd3e5dd3f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_13d1c6d2eeef46f79ca57f6fc4acb0be","max":151061672,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8074c6f81d0f403d8dd06b6fc4a7bf83","value":151061672}},"4ae671de23f3441887cbc7a3953f9c90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cfcea8a22eb4a0d986f1da6e0257ce0","placeholder":"​","style":"IPY_MODEL_eb0ba123434940768511f97b24d9a65d","value":" 151M/151M [00:01&lt;00:00, 67.2MB/s]"}},"75c4e30b3a5643688941d2698f81ed29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60a5b628ad514e1c97f9ca5728fa6dbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"475528ff173f47858b79c97a5a1b15e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13d1c6d2eeef46f79ca57f6fc4acb0be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8074c6f81d0f403d8dd06b6fc4a7bf83":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0cfcea8a22eb4a0d986f1da6e0257ce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb0ba123434940768511f97b24d9a65d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a95bc212127f44a2a4462e053b2dd42e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f462f7d962d5405d864068efc8ba384a","IPY_MODEL_334b8fe0a13247578e17e4b56a852122","IPY_MODEL_a36d9401c10b4cd5920cca6979a6b4e8"],"layout":"IPY_MODEL_4a9a9f646b544aa09da77866cf344d2b"}},"f462f7d962d5405d864068efc8ba384a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66c1cb15102b4d278c01b71f189152be","placeholder":"​","style":"IPY_MODEL_58fdbfad90444f7e9575d22d25f0b9a1","value":"generation_config.json: "}},"334b8fe0a13247578e17e4b56a852122":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df43c75037a046f9a64187ba1c94d926","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7966c6cded3742a885c79e635f99fb75","value":1}},"a36d9401c10b4cd5920cca6979a6b4e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c054219d6ff439e850de50206933433","placeholder":"​","style":"IPY_MODEL_6e2fdc7055a24ec68ba5d68b60530fa4","value":" 3.75k/? [00:00&lt;00:00, 481kB/s]"}},"4a9a9f646b544aa09da77866cf344d2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66c1cb15102b4d278c01b71f189152be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58fdbfad90444f7e9575d22d25f0b9a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df43c75037a046f9a64187ba1c94d926":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7966c6cded3742a885c79e635f99fb75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c054219d6ff439e850de50206933433":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e2fdc7055a24ec68ba5d68b60530fa4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"8f4e7d62","cell_type":"markdown","source":"# Doctor-Patient ASR Baseline Model\n\nThis notebook implements a baseline ASR model for doctor-patient conversations using the Hugging Face dataset `Shamus/United-Syn-Med`.\n\n## Pipeline Overview:\n1. **Data Loading**: Stream subset from HF dataset\n2. **Preprocessing**: Audio resampling + feature extraction\n3. **Training**: Fine-tune Whisper Small/Tiny\n4. **Evaluation**: Compute WER metrics\n5. **Inference**: Test on validation samples\n\nTarget: ~2000 samples (~500MB) for baseline prototype","metadata":{"id":"8f4e7d62"}},{"id":"0ece06a5","cell_type":"markdown","source":"## Setup and Dependencies","metadata":{"id":"0ece06a5"}},{"id":"4761114c","cell_type":"code","source":"# Install required packages\n!pip install datasets transformers torch torchaudio accelerate evaluate jiwer tensorboard\n!pip install --upgrade huggingface_hub\n# Install additional audio dependencies if needed\n!pip install soundfile librosa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4761114c","outputId":"d5921c83-01ce-409d-d7bb-b97a184c755c","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:11:48.644069Z","iopub.execute_input":"2025-08-18T07:11:48.645002Z","iopub.status.idle":"2025-08-18T07:11:59.741490Z","shell.execute_reply.started":"2025-08-18T07:11:48.644953Z","shell.execute_reply":"2025-08-18T07:11:59.740689Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\nRequirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (4.0.0)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\nRequirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.13.0)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.73.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8.2)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\nRequirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\nRequirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.26.4)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\nRequirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.2.2)\nRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\nRequirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.0)\nRequirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2.4.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->soundfile) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->soundfile) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->soundfile) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->soundfile) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->soundfile) (2024.2.0)\n","output_type":"stream"}],"execution_count":14},{"id":"0c6f6bce","cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom transformers import (\n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    WhisperTokenizer,\n    WhisperFeatureExtractor,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\nimport evaluate\nfrom typing import Dict, List, Optional, Tuple\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration\nCONFIG = {\n    'dataset_name': 'Shamus/United-Syn-Med',\n    'model_name': 'openai/whisper-tiny',  # Change to 'openai/whisper-tiny' for faster testing\n    'num_samples': 700,\n    'target_sample_rate': 16000,\n    'train_split_ratio': 0.8,\n    'output_dir': './whisper-medical-asr',\n    'max_audio_length': 30.0,  # seconds\n    'batch_size': 8,\n    'num_epochs': 3,\n    'learning_rate': 1e-5,\n    'warmup_steps': 500,\n    'eval_steps': 500,\n    'save_steps': 1000,\n    'gradient_accumulation_steps': 2\n}\n\nprint(f\"Using model: {CONFIG['model_name']}\")\nprint(f\"Target samples: {CONFIG['num_samples']}\")\nprint(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0c6f6bce","outputId":"fa45d114-cf21-4282-f37d-c0b04e6a4814","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:11:59.743234Z","iopub.execute_input":"2025-08-18T07:11:59.743541Z","iopub.status.idle":"2025-08-18T07:11:59.751034Z","shell.execute_reply.started":"2025-08-18T07:11:59.743518Z","shell.execute_reply":"2025-08-18T07:11:59.750364Z"}},"outputs":[{"name":"stdout","text":"Using model: openai/whisper-tiny\nTarget samples: 700\nDevice: cuda\n","output_type":"stream"}],"execution_count":15},{"id":"21dded15","cell_type":"markdown","source":"## Module 1: Data Loading","metadata":{"id":"21dded15"}},{"id":"10fc4022","cell_type":"code","source":"class DataLoader:\n    \"\"\"Handles loading and streaming of the medical conversation dataset.\"\"\"\n\n    def __init__(self, dataset_name: str, num_samples: int, train_split_ratio: float = 0.8):\n        self.dataset_name = dataset_name\n        self.num_samples = num_samples\n        self.train_split_ratio = train_split_ratio\n        self.raw_data = None\n\n    def load_dataset_subset(self) -> List[Dict]:\n        \"\"\"Load a subset of the dataset using direct parquet access to avoid audio decoding issues.\"\"\"\n        print(f\"Loading {self.num_samples} samples from {self.dataset_name}...\")\n\n        try:\n            # Method 1: Direct parquet loading to avoid audio decoding\n            print(\"Attempting direct parquet loading...\")\n            samples = self._load_from_parquet()\n            if samples:\n                self.raw_data = samples\n                return samples\n        except Exception as e:\n            print(f\"Parquet loading failed: {e}\")\n\n        try:\n            # Method 2: Use datasets library but process samples carefully\n            print(\"Attempting careful streaming with manual audio handling...\")\n            samples = self._load_with_manual_processing()\n            if samples:\n                self.raw_data = samples\n                return samples\n        except Exception as e:\n            print(f\"Manual processing failed: {e}\")\n\n        raise Exception(\"All loading methods failed\")\n\n    def _load_from_parquet(self) -> List[Dict]:\n        \"\"\"Load dataset directly from parquet files.\"\"\"\n        import pandas as pd\n        from huggingface_hub import list_repo_files\n\n        # Get parquet files\n        files = list_repo_files(self.dataset_name, repo_type=\"dataset\")\n        parquet_files = [f for f in files if f.endswith('.parquet') and 'train' in f]\n\n        if not parquet_files:\n            raise Exception(\"No parquet files found\")\n\n        print(f\"Found {len(parquet_files)} parquet files\")\n\n        samples = []\n        samples_per_file = self.num_samples // len(parquet_files) + 1\n\n        for file_idx, file_path in enumerate(parquet_files):\n            if len(samples) >= self.num_samples:\n                break\n\n            try:\n                print(f\"Loading from {file_path}...\")\n                file_url = f\"https://huggingface.co/datasets/{self.dataset_name}/resolve/main/{file_path}\"\n                df = pd.read_parquet(file_url, engine='pyarrow')\n\n                # Take only the samples we need from this file\n                remaining_samples = self.num_samples - len(samples)\n                df_subset = df.head(min(samples_per_file, remaining_samples))\n\n                for _, row in df_subset.iterrows():\n                    if len(samples) >= self.num_samples:\n                        break\n\n                    # Convert row to dict and handle audio separately\n                    sample = {}\n                    for col, value in row.items():\n                        if col == 'audio':\n                            # Keep audio as raw data structure\n                            if isinstance(value, dict):\n                                sample[col] = value\n                            else:\n                                # If it's a different format, create a dict structure\n                                sample[col] = {'bytes': value, 'path': None, 'sampling_rate': 16000}\n                        elif col == 'transcription':\n                            # Map transcription to text field\n                            sample['text'] = str(value) if value is not None else \"\"\n                        else:\n                            sample[col] = value\n\n                    # Ensure we have a text field\n                    if 'text' not in sample:\n                        sample['text'] = \"\"\n\n                    samples.append(sample)\n\n                print(f\"Loaded {len(samples)} samples so far...\")\n\n            except Exception as e:\n                print(f\"Error loading {file_path}: {e}\")\n                continue\n\n        return samples\n\n    def _load_with_manual_processing(self) -> List[Dict]:\n        \"\"\"Fallback method using datasets library with careful processing.\"\"\"\n        # Try loading without any schema constraints\n        dataset_stream = load_dataset(\n            self.dataset_name,\n            split='train',\n            streaming=True\n        )\n\n        samples = []\n        error_count = 0\n        max_errors = 10  # Allow some errors before giving up\n\n        iterator = iter(dataset_stream)\n\n        while len(samples) < self.num_samples and error_count < max_errors:\n            try:\n                sample = next(iterator)\n\n                # Process sample carefully\n                processed_sample = {}\n\n                for key, value in sample.items():\n                    if key == 'audio':\n                        # Try to keep audio without triggering decoding\n                        if hasattr(value, 'keys') and callable(getattr(value, 'keys')):\n                            # It's dict-like, extract raw data\n                            processed_sample[key] = {\n                                'bytes': getattr(value, 'bytes', None),\n                                'path': getattr(value, 'path', None),\n                                'sampling_rate': getattr(value, 'sampling_rate', 16000)\n                            }\n                        else:\n                            # Store as-is and hope for the best\n                            processed_sample[key] = value\n                    elif key == 'transcription':\n                        processed_sample['text'] = str(value) if value is not None else \"\"\n                    else:\n                        processed_sample[key] = value\n\n                # Ensure text field exists\n                if 'text' not in processed_sample:\n                    processed_sample['text'] = \"\"\n\n                samples.append(processed_sample)\n\n                if (len(samples) + 1) % 100 == 0:\n                    print(f\"Loaded {len(samples)} samples...\")\n\n            except StopIteration:\n                print(\"Reached end of dataset\")\n                break\n            except Exception as e:\n                error_count += 1\n                print(f\"Error processing sample {len(samples) + error_count}: {e}\")\n                if error_count >= max_errors:\n                    print(f\"Too many errors ({error_count}), stopping...\")\n                    break\n\n        return samples\n\n    def create_train_val_split(self, samples: List[Dict]) -> Tuple[Dataset, Dataset]:\n        \"\"\"Split the data into train and validation sets.\"\"\"\n        split_idx = int(len(samples) * self.train_split_ratio)\n\n        train_samples = samples[:split_idx]\n        val_samples = samples[split_idx:]\n\n        # Convert to HF datasets\n        train_dataset = Dataset.from_list(train_samples)\n        val_dataset = Dataset.from_list(val_samples)\n\n        print(f\"Train samples: {len(train_dataset)}\")\n        print(f\"Validation samples: {len(val_dataset)}\")\n\n        return train_dataset, val_dataset\n\n    def inspect_sample(self, idx: int = 0) -> None:\n        \"\"\"Inspect a sample to understand the data structure.\"\"\"\n        if self.raw_data is None:\n            print(\"No data loaded. Run load_dataset_subset() first.\")\n            return\n\n        sample = self.raw_data[idx]\n        print(f\"Sample {idx} structure:\")\n        for key, value in sample.items():\n            if key == 'audio':\n                if isinstance(value, dict):\n                    print(f\"  {key}: dict with keys {list(value.keys())}\")\n                    for sub_key, sub_value in value.items():\n                        if sub_key == 'bytes' and sub_value is not None:\n                            print(f\"    {sub_key}: {len(sub_value)} bytes\")\n                        else:\n                            print(f\"    {sub_key}: {sub_value}\")\n                else:\n                    print(f\"  {key}: {type(value)}\")\n            else:\n                print(f\"  {key}: {str(value)[:100]}...\" if len(str(value)) > 100 else f\"  {key}: {value}\")\n\n# Initialize data loader\ndata_loader = DataLoader(\n    dataset_name=CONFIG['dataset_name'],\n    num_samples=CONFIG['num_samples'],\n    train_split_ratio=CONFIG['train_split_ratio']\n)","metadata":{"id":"10fc4022","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:11:59.752039Z","iopub.execute_input":"2025-08-18T07:11:59.752291Z","iopub.status.idle":"2025-08-18T07:11:59.779183Z","shell.execute_reply.started":"2025-08-18T07:11:59.752269Z","shell.execute_reply":"2025-08-18T07:11:59.778537Z"}},"outputs":[],"execution_count":16},{"id":"OSgb7prEDy_O","cell_type":"code","source":"# Test cell to examine the original dataset structure without triggering audio decoding\ndef check_dataset_structure_safe():\n    \"\"\"Check the actual structure of the dataset without triggering audio decoding.\"\"\"\n    print(\"Examining dataset structure safely...\")\n\n    try:\n        # Load dataset info without streaming to avoid audio decoding\n        from datasets import get_dataset_config_names, get_dataset_split_names\n        from datasets.utils.info_utils import get_dataset_infos\n\n        print(\"Dataset configs:\", get_dataset_config_names(CONFIG['dataset_name']))\n        print(\"Dataset splits:\", get_dataset_split_names(CONFIG['dataset_name']))\n\n        # Try to get dataset info\n        dataset_infos = get_dataset_infos(CONFIG['dataset_name'])\n        print(\"Dataset info keys:\", list(dataset_infos.keys()))\n\n        # If we have default config, print its features\n        if 'default' in dataset_infos:\n            features = dataset_infos['default'].features\n            print(\"Dataset features:\")\n            for name, feature in features.items():\n                print(f\"  {name}: {feature}\")\n\n        return True\n\n    except Exception as e:\n        print(f\"Error getting dataset info: {e}\")\n\n        # Alternative: try loading parquet files directly\n        try:\n            print(\"Trying direct parquet approach...\")\n            import pandas as pd\n            from huggingface_hub import list_repo_files\n\n            # List files in the repository\n            files = list_repo_files(CONFIG['dataset_name'], repo_type=\"dataset\")\n            parquet_files = [f for f in files if f.endswith('.parquet') and 'train' in f]\n            print(f\"Found {len(parquet_files)} training parquet files\")\n\n            if parquet_files:\n                # Load just the first few rows of the first parquet file to check structure\n                first_file = parquet_files[0]\n                print(f\"Examining first file: {first_file}\")\n\n                # Load parquet file directly\n                file_url = f\"https://huggingface.co/datasets/{CONFIG['dataset_name']}/resolve/main/{first_file}\"\n                df_sample = pd.read_parquet(file_url, engine='pyarrow').head(3)\n\n                print(\"Parquet file structure:\")\n                print(f\"Columns: {list(df_sample.columns)}\")\n                print(f\"Shape: {df_sample.shape}\")\n\n                for col in df_sample.columns:\n                    print(f\"  {col}: {df_sample[col].dtype}\")\n                    if col != 'audio':  # Skip audio column to avoid issues\n                        print(f\"    Sample value: {str(df_sample[col].iloc[0])[:100]}...\")\n\n                return df_sample\n\n        except Exception as e2:\n            print(f\"Parquet approach also failed: {e2}\")\n            return None\n\n# Run the safe structure check\nprint(\"Checking dataset structure safely...\")\nsample_structure = check_dataset_structure_safe()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OSgb7prEDy_O","outputId":"fbd15a9e-877a-4b8f-afe7-4c27badc571a","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:11:59.780776Z","iopub.execute_input":"2025-08-18T07:11:59.781015Z","iopub.status.idle":"2025-08-18T07:12:02.849742Z","shell.execute_reply.started":"2025-08-18T07:11:59.780996Z","shell.execute_reply":"2025-08-18T07:12:02.849036Z"}},"outputs":[{"name":"stdout","text":"Checking dataset structure safely...\nExamining dataset structure safely...\nError getting dataset info: cannot import name 'get_dataset_infos' from 'datasets.utils.info_utils' (/usr/local/lib/python3.11/dist-packages/datasets/utils/info_utils.py)\nTrying direct parquet approach...\nFound 33 training parquet files\nExamining first file: data/train-00000-of-00033.parquet\nParquet file structure:\nColumns: ['audio', 'transcription']\nShape: (3, 2)\n  audio: object\n  transcription: object\n    Sample value: Durysta is a medication used to reduce eye pressure in patients with open-angle glaucoma or ocular h...\n","output_type":"stream"}],"execution_count":17},{"id":"caa97aeb","cell_type":"code","source":"# Load the dataset subset\nsamples = data_loader.load_dataset_subset()\n\n# Inspect a sample to understand the structure\ndata_loader.inspect_sample(0)\n\n# Test audio decoding on the first sample\nprint(\"\\n\" + \"=\"*50)\nprint(\"Testing audio decoding...\")\ntry:\n    first_sample = samples[0]\n    audio_info = first_sample['audio']\n\n    # Try to decode the audio manually\n    import io\n    if 'bytes' in audio_info and audio_info['bytes'] is not None:\n        print(\"Audio stored as bytes - decoding...\")\n        audio_bytes = audio_info['bytes']\n        audio_file = io.BytesIO(audio_bytes)\n        waveform, sample_rate = torchaudio.load(audio_file)\n        print(f\"Successfully decoded audio: shape={waveform.shape}, sample_rate={sample_rate}\")\n    elif 'path' in audio_info:\n        print(f\"Audio path: {audio_info['path']}\")\n        waveform, sample_rate = torchaudio.load(audio_info['path'])\n        print(f\"Successfully loaded audio: shape={waveform.shape}, sample_rate={sample_rate}\")\n    else:\n        print(f\"Unexpected audio format: {audio_info}\")\n\nexcept Exception as e:\n    print(f\"Error decoding audio: {e}\")\n    print(\"This might indicate an issue with the audio format.\")\n\nprint(\"=\"*50)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"caa97aeb","outputId":"5866c26e-9821-460b-e55d-32ea4304fb7e","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:12:02.850737Z","iopub.execute_input":"2025-08-18T07:12:02.850944Z","iopub.status.idle":"2025-08-18T07:13:18.960488Z","shell.execute_reply.started":"2025-08-18T07:12:02.850928Z","shell.execute_reply":"2025-08-18T07:13:18.959670Z"}},"outputs":[{"name":"stdout","text":"Loading 700 samples from Shamus/United-Syn-Med...\nAttempting direct parquet loading...\nFound 33 parquet files\nLoading from data/train-00000-of-00033.parquet...\nLoaded 22 samples so far...\nLoading from data/train-00001-of-00033.parquet...\nLoaded 44 samples so far...\nLoading from data/train-00002-of-00033.parquet...\nLoaded 66 samples so far...\nLoading from data/train-00003-of-00033.parquet...\nLoaded 88 samples so far...\nLoading from data/train-00004-of-00033.parquet...\nLoaded 110 samples so far...\nLoading from data/train-00005-of-00033.parquet...\nLoaded 132 samples so far...\nLoading from data/train-00006-of-00033.parquet...\nLoaded 154 samples so far...\nLoading from data/train-00007-of-00033.parquet...\nLoaded 176 samples so far...\nLoading from data/train-00008-of-00033.parquet...\nLoaded 198 samples so far...\nLoading from data/train-00009-of-00033.parquet...\nLoaded 220 samples so far...\nLoading from data/train-00010-of-00033.parquet...\nLoaded 242 samples so far...\nLoading from data/train-00011-of-00033.parquet...\nLoaded 264 samples so far...\nLoading from data/train-00012-of-00033.parquet...\nLoaded 286 samples so far...\nLoading from data/train-00013-of-00033.parquet...\nLoaded 308 samples so far...\nLoading from data/train-00014-of-00033.parquet...\nLoaded 330 samples so far...\nLoading from data/train-00015-of-00033.parquet...\nLoaded 352 samples so far...\nLoading from data/train-00016-of-00033.parquet...\nLoaded 374 samples so far...\nLoading from data/train-00017-of-00033.parquet...\nLoaded 396 samples so far...\nLoading from data/train-00018-of-00033.parquet...\nLoaded 418 samples so far...\nLoading from data/train-00019-of-00033.parquet...\nLoaded 440 samples so far...\nLoading from data/train-00020-of-00033.parquet...\nLoaded 462 samples so far...\nLoading from data/train-00021-of-00033.parquet...\nLoaded 484 samples so far...\nLoading from data/train-00022-of-00033.parquet...\nLoaded 506 samples so far...\nLoading from data/train-00023-of-00033.parquet...\nLoaded 528 samples so far...\nLoading from data/train-00024-of-00033.parquet...\nLoaded 550 samples so far...\nLoading from data/train-00025-of-00033.parquet...\nLoaded 572 samples so far...\nLoading from data/train-00026-of-00033.parquet...\nLoaded 594 samples so far...\nLoading from data/train-00027-of-00033.parquet...\nLoaded 616 samples so far...\nLoading from data/train-00028-of-00033.parquet...\nLoaded 638 samples so far...\nLoading from data/train-00029-of-00033.parquet...\nLoaded 660 samples so far...\nLoading from data/train-00030-of-00033.parquet...\nLoaded 682 samples so far...\nLoading from data/train-00031-of-00033.parquet...\nLoaded 700 samples so far...\nSample 0 structure:\n  audio: dict with keys ['bytes', 'path']\n    bytes: 65613 bytes\n    path: drug-female-defa7fcb-89d7-4b25-8834-90888b201d25.mp3\n  text: Durysta is a medication used to reduce eye pressure in patients with open-angle glaucoma or ocular h...\n\n==================================================\nTesting audio decoding...\nAudio stored as bytes - decoding...\nSuccessfully decoded audio: shape=torch.Size([1, 178150]), sample_rate=24000\n==================================================\n","output_type":"stream"}],"execution_count":18},{"id":"5f518969","cell_type":"markdown","source":"## Module 2: Audio Preprocessing","metadata":{"id":"5f518969"}},{"id":"3af9c017","cell_type":"code","source":"class AudioPreprocessor:\n    \"\"\"Handles audio preprocessing for Whisper model.\"\"\"\n\n    def __init__(self, model_name: str, target_sample_rate: int = 16000, max_audio_length: float = 30.0):\n        self.model_name = model_name\n        self.target_sample_rate = target_sample_rate\n        self.max_audio_length = max_audio_length\n\n        # Initialize Whisper components\n        print(f\"Loading Whisper processor for {model_name}...\")\n        self.processor = WhisperProcessor.from_pretrained(model_name)\n        self.feature_extractor = self.processor.feature_extractor\n        self.tokenizer = self.processor.tokenizer\n\n        print(f\"Processor loaded. Target sample rate: {self.target_sample_rate}Hz\")\n\n    def decode_audio_bytes(self, audio_info: Dict) -> Tuple[np.ndarray, int]:\n        \"\"\"Decode audio from bytes using torchaudio.\"\"\"\n        import io\n\n        if 'bytes' in audio_info and audio_info['bytes'] is not None:\n            # Load audio from bytes\n            audio_bytes = audio_info['bytes']\n            audio_file = io.BytesIO(audio_bytes)\n\n            # Use torchaudio to load the audio\n            waveform, sample_rate = torchaudio.load(audio_file)\n\n            # Convert to numpy and handle multi-channel audio\n            audio_array = waveform.numpy()\n            if len(audio_array.shape) > 1:\n                # Convert to mono by averaging channels\n                audio_array = np.mean(audio_array, axis=0)\n\n            return audio_array, sample_rate\n\n        elif 'path' in audio_info and audio_info['path'] is not None:\n            # Load audio from file path\n            waveform, sample_rate = torchaudio.load(audio_info['path'])\n\n            # Convert to numpy and handle multi-channel audio\n            audio_array = waveform.numpy()\n            if len(audio_array.shape) > 1:\n                # Convert to mono by averaging channels\n                audio_array = np.mean(audio_array, axis=0)\n\n            return audio_array, sample_rate\n\n        elif 'array' in audio_info:\n            # Already decoded audio\n            return np.array(audio_info['array']), audio_info.get('sampling_rate', 16000)\n\n        else:\n            raise ValueError(f\"Unsupported audio format: {audio_info.keys()}\")\n\n    def resample_audio(self, audio_array: np.ndarray, original_sr: int) -> np.ndarray:\n        \"\"\"Resample audio to target sample rate.\"\"\"\n        if original_sr == self.target_sample_rate:\n            return audio_array\n\n        # Convert to tensor for resampling\n        audio_tensor = torch.from_numpy(audio_array).float()\n        if len(audio_tensor.shape) == 1:\n            audio_tensor = audio_tensor.unsqueeze(0)  # Add channel dimension\n\n        # Resample\n        resampler = torchaudio.transforms.Resample(\n            orig_freq=original_sr,\n            new_freq=self.target_sample_rate\n        )\n        resampled = resampler(audio_tensor)\n\n        return resampled.squeeze().numpy()\n\n    def trim_or_pad_audio(self, audio_array: np.ndarray) -> np.ndarray:\n        \"\"\"Trim or pad audio to max length.\"\"\"\n        max_samples = int(self.max_audio_length * self.target_sample_rate)\n\n        if len(audio_array) > max_samples:\n            # Trim to max length\n            return audio_array[:max_samples]\n        elif len(audio_array) < max_samples:\n            # Pad with zeros\n            padding = max_samples - len(audio_array)\n            return np.pad(audio_array, (0, padding), mode='constant')\n        else:\n            return audio_array\n\n    def preprocess_batch(self, batch: Dict) -> Dict:\n        \"\"\"Preprocess a batch of audio samples.\"\"\"\n        # Extract audio data\n        audio_data = []\n        texts = []\n\n        for i in range(len(batch['audio'])):\n            try:\n                # Handle audio - decode from bytes/path\n                audio_info = batch['audio'][i]\n                audio_array, sampling_rate = self.decode_audio_bytes(audio_info)\n\n                # Preprocess audio\n                audio_array = self.resample_audio(audio_array, sampling_rate)\n                audio_array = self.trim_or_pad_audio(audio_array)\n                audio_data.append(audio_array)\n\n                # Handle text (check multiple possible field names)\n                text_field = \"\"\n                for field in ['text', 'transcription', 'sentence', 'transcript']:\n                    if field in batch and i < len(batch[field]):\n                        text_field = batch[field][i]\n                        break\n\n                if text_field is None:\n                    text_field = \"\"  # Empty fallback\n                texts.append(str(text_field))\n\n            except Exception as e:\n                print(f\"Error processing sample {i}: {e}\")\n                # Skip this sample or use empty data\n                audio_data.append(np.zeros(int(self.max_audio_length * self.target_sample_rate)))\n                texts.append(\"\")\n\n        # Process with Whisper feature extractor\n        features = self.feature_extractor(\n            audio_data,\n            sampling_rate=self.target_sample_rate,\n            return_tensors=\"pt\",\n            padding=True\n        )\n\n        # Tokenize texts with proper padding and truncation for Whisper\n        with self.tokenizer.as_target_tokenizer():\n            labels = self.tokenizer(\n                texts,\n                max_length=448,  # Whisper's max sequence length\n                padding=\"max_length\",  # Force consistent padding\n                truncation=True,\n                return_tensors=\"pt\"\n            ).input_ids\n\n        # Replace padding token id with -100 so it's ignored in loss computation\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"input_features\": features[\"input_features\"],\n            \"labels\": labels\n        }\n\n# Initialize preprocessor\npreprocessor = AudioPreprocessor(\n    model_name=CONFIG['model_name'],\n    target_sample_rate=CONFIG['target_sample_rate'],\n    max_audio_length=CONFIG['max_audio_length']\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":308,"referenced_widgets":["0dddc4553e0549db97f215be1b53c51f","ec4dcc28a67546ee8265ff2dcf24b2b6","0f4671829e9149318d030838ca788766","8c8e25c295f34225933d590ecf002f3a","b713465adfce4da58ab44cdbbfe11e85","a3ee6162d2134f53b95f156fc645963a","709f36990648403f82bb2d8012fa005d","af32c32d09104beea9d774e82b6b1fdb","28cf4d3041b44138b05fde3a6c0c71db","3d7cec6d6c30410aaa06cd380062c9a7","10f7ee7868da4ec0ab1e57342d8abf1f","c9b27b25160b40a589b20dcfc2d38de6","927848144429471abfaa5671841c520e","4d63aff65c524fbfbc2d35081a29f5d9","9685febcf74c401db8e24574b84af2b3","aab1012473c840ccb99b5508c1626d88","a2fd054a4e0945f8a714d69cb61bb674","adbcba351b8349bc9293ba950bfc6d59","fc33cb13dd604d74a9b4023b133cb4da","a8102832b23944f58faed6032e863aea","93d4de194b0146e0bfbf1f9203a8efbc","11d2d86b57c84318827ea42aa4b59b8f","efb34333e20c4b719b6d30668c6952f1","02317272e64c4fce899c59e381ee0690","150f986d62bb41c9a56fd38e4c954020","83ace04551a745a68f33c9c1105e3b01","e81b51ef0132489a8d8c73fbe25d4692","2170942122d74128ba878bbc098a20cd","78f8f798727b4556ab94a40afe3702f7","84c50442ccbe41868faad529f5bb2f3e","7696c8d022c8457fba0fdaa1cdfd411d","3e8dd3c7670340ad944cea975f7bc3bb","3cb416067c7443e8b745e61d8d2e76dd","252a6ceac04d4dadacb05eb742067031","79018e6e9f2745749ad812a6f6713bc3","e31be64fc0104355a83f09cc6a0031f1","f8e29ea23cd64bb4a5c6d03ca2576fde","d5300ea0814440b79fbf23f489a758ed","61e3e744b3b44bd9bea0de1d8bd3bdcb","275da4af86f6427d844bbace06cb7bf8","6df02e70d3b04da8b134e4a26b657152","1867ff7ca04e4a54987eefcf6cf7ec9b","ea9152547d904d11ab2aeec6db3fdc89","5e9ec97932474f0882e7e9300115695f","8128a80de9714b66bba0f79c0c940071","02935574cb384d6b84fad32f3d93f4c8","d3414bec85964ba8a9a5270fd09dced7","a40c1fab1563407b904db049f9450c0e","94cd766edca34b319e87de8dfc7044b2","e59c3e69e09f420cb6e1b80ffe223136","01b6552018cc425ead85eae0e17fda06","dc2c4395392b474f93e941b352e38ca4","d4adf3b026b44ec789233e36a0f0c64c","84cd0cb9060744f1892d1efd4bd7adbd","eb472304e4e942c091f4230917bd4e9c","05f78daa68a04dd4a2b4a84e5ccfebd3","56750607047846f593672599988e32dc","98f6bd36ee97497bbd9b7e006b10b538","60fa01c6a764403aa74defec1f0c2bd0","3c2dbfa45e0b4d9b9303621f433f6165","f1cf0f30095741ff910247f5a84be592","358261fd07324927b6a7455df91e736b","78bf40d6a36e4ae4ac35990aa44f5d9a","294e60cd05a04286a238ce7282503b9a","8148517597314edb9ca2ab6179877954","9dbc5ba6a7e64e2f84bb3613c0a38c63","334ccf4dec2b47ffa3297075d90f9190","a04d448a56e749b2a3fb7f193a22bcd2","775960d2e6934bd899e40fad5e93a148","7bae267dec7a4c9687fb19d6cd903e33","8157314a37e54b3facfcc0a44142806f","2234a1a88efc4d5fae411866a3adf66e","6ba64c270f0f4f7ab02d0471153ff58a","7690e3a451944633aeb89a3bc25a809d","52587a0ecb9d4f16b96a5e284c5c1c7c","b69023098a164d1a8d1e5628b01a09ec","7923ada824f343f8a26d2fe5cd3999dd","f77ba27784944863819ba100c650f793","65bfd81001294311a03d464b29987164","d37a192fc6274871b0c2230dab76db86","948d4af818ce4da0a4499a758aeccc7f","efabae23841d4f08a97feeed631c6ed8","1b198dd6dd524741abe5df656b99a309","4467cacde89c470ba36ceb991ccf804b","62a8e5ea484341a289feab09fe608859","8179b13c47ba46feb3b9cb009740587c","bfde110d84074490891834a0bda7f1d3","c23e74984a7549199308b25d6721d27d"]},"id":"3af9c017","outputId":"15c2ae14-8a13-4fd8-e79f-f778f31fd26b","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:13:18.961593Z","iopub.execute_input":"2025-08-18T07:13:18.961930Z","iopub.status.idle":"2025-08-18T07:13:20.103295Z","shell.execute_reply.started":"2025-08-18T07:13:18.961904Z","shell.execute_reply":"2025-08-18T07:13:20.102482Z"}},"outputs":[{"name":"stdout","text":"Loading Whisper processor for openai/whisper-tiny...\nProcessor loaded. Target sample rate: 16000Hz\n","output_type":"stream"}],"execution_count":19},{"id":"a5GQv7CKJHsJ","cell_type":"code","source":"","metadata":{"id":"a5GQv7CKJHsJ","trusted":true},"outputs":[],"execution_count":null},{"id":"dc0790e4","cell_type":"code","source":"# Create train/val splits\ntrain_dataset, val_dataset = data_loader.create_train_val_split(samples)\n\n# Apply preprocessing to datasets\nprint(\"Preprocessing training data...\")\ntrain_dataset = train_dataset.map(\n    preprocessor.preprocess_batch,\n    batched=True,\n    batch_size=8,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Preprocessing validation data...\")\nval_dataset = val_dataset.map(\n    preprocessor.preprocess_batch,\n    batched=True,\n    batch_size=8,\n    remove_columns=val_dataset.column_names\n)\n\nprint(\"Preprocessing complete!\")\nprint(f\"Train dataset: {len(train_dataset)} samples\")\nprint(f\"Val dataset: {len(val_dataset)} samples\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202,"referenced_widgets":["64b72284dc7c4fab9eff625817401828","6abb6151ab454aac9762bc6c1917373d","9a7218b02f924029abb0b5e22afde9ed","32943b81e0b64b3fa2af1f4f66f781ec","421a046bb06544bfb3641e1747922e7d","339ad729a3e4445e8524fa66f7aabb45","e936d01f314648e098bee0a9e133ec7f","c9b48d3f09d04d219176844fe1ee9046","1a6b38aca0ce4adb9898d9cac042cf11","d5d8385b58774cbea455b378a9186443","353f3db573e64d238eaacb91bae1c5fa","7693ad9de082400ea858866c77f46f2b","019f56acf34344aebdfa7b3ee15f3261","dbe116372cfa439083b6d44863e50453","7946a35895da4b3ea78361d84eb91cda","848b664d20a4476da239f1e6b5fbf8a7","0be4520c604f4c938ae07d026db67fb1","d576293895f24082a00b76570baa567c","65f2832bce7748b986129a9361d4af4d","c5ce27ea0339491da34eec3f9874ca03","77159aaec226492fbf83cd80f4726dcf","6a775153f0ea4c42b9c1b36476ba882b"]},"id":"dc0790e4","outputId":"af248215-b7eb-4d8b-a955-37b4cd757546","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:13:20.104475Z","iopub.execute_input":"2025-08-18T07:13:20.104737Z","iopub.status.idle":"2025-08-18T07:13:40.336119Z","shell.execute_reply.started":"2025-08-18T07:13:20.104719Z","shell.execute_reply":"2025-08-18T07:13:40.335277Z"}},"outputs":[{"name":"stdout","text":"Train samples: 560\nValidation samples: 140\nPreprocessing training data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/560 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1acec549fd8f4bf4a699ed479a2bc6fe"}},"metadata":{}},{"name":"stdout","text":"Preprocessing validation data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/140 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187b524fd7ee469b8d243f40bea08377"}},"metadata":{}},{"name":"stdout","text":"Preprocessing complete!\nTrain dataset: 560 samples\nVal dataset: 140 samples\n","output_type":"stream"}],"execution_count":20},{"id":"22094e74","cell_type":"markdown","source":"## Module 3: Model Training","metadata":{"id":"22094e74"}},{"id":"c17d828d","cell_type":"code","source":"class WhisperTrainer:\n    \"\"\"Handles Whisper model training and fine-tuning.\"\"\"\n\n    def __init__(self, model_name: str, output_dir: str):\n        self.model_name = model_name\n        self.output_dir = output_dir\n\n        # Load model and processor\n        print(f\"Loading Whisper model: {model_name}...\")\n        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n        self.processor = WhisperProcessor.from_pretrained(model_name)\n\n        # Force decoder to use correct language tokens\n        self.model.config.forced_decoder_ids = None\n        self.model.config.suppress_tokens = []\n\n        print(f\"Model loaded. Parameters: {self.model.num_parameters():,}\")\n\n    def create_data_collator(self):\n        \"\"\"Create data collator for training.\"\"\"\n        # Use DefaultDataCollator since we're handling padding in preprocessing\n        from transformers import DefaultDataCollator\n        return DefaultDataCollator()\n\n    def setup_training_args(self, config: Dict) -> TrainingArguments:\n        \"\"\"Setup training arguments.\"\"\"\n        return TrainingArguments(\n            output_dir=self.output_dir,\n            per_device_train_batch_size=config['batch_size'],\n            per_device_eval_batch_size=config['batch_size'],\n            gradient_accumulation_steps=config['gradient_accumulation_steps'],\n            learning_rate=config['learning_rate'],\n            warmup_steps=config['warmup_steps'],\n            num_train_epochs=config['num_epochs'],\n            eval_strategy=\"steps\",\n            eval_steps=config['eval_steps'],\n            save_steps=config['save_steps'],\n            logging_steps=50,\n            save_total_limit=2,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_loss\",\n            greater_is_better=False,\n            fp16=True if torch.cuda.is_available() else False,\n            dataloader_pin_memory=False,\n            dataloader_num_workers=0,  # Avoid multiprocessing issues\n            report_to=[\"tensorboard\"],\n            push_to_hub=False,\n            remove_unused_columns=False\n        )\n\n    def create_trainer(self, train_dataset: Dataset, val_dataset: Dataset, config: Dict) -> Trainer:\n        \"\"\"Create and configure the trainer.\"\"\"\n        training_args = self.setup_training_args(config)\n        data_collator = self.create_data_collator()\n\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=val_dataset,\n            data_collator=data_collator,\n            tokenizer=self.processor.tokenizer\n        )\n\n        return trainer\n\n    def train(self, train_dataset: Dataset, val_dataset: Dataset, config: Dict):\n        \"\"\"Train the model.\"\"\"\n        print(\"Setting up trainer...\")\n        trainer = self.create_trainer(train_dataset, val_dataset, config)\n\n        print(\"Starting training...\")\n        trainer.train()\n\n        print(\"Saving final model...\")\n        trainer.save_model()\n        self.processor.save_pretrained(self.output_dir)\n\n        return trainer\n\n# Initialize trainer\nwhisper_trainer = WhisperTrainer(\n    model_name=CONFIG['model_name'],\n    output_dir=CONFIG['output_dir']\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":148,"referenced_widgets":["10d4f72e2f0e4bb3a7263a1ef307030b","c8da8278a4124057bf6ff7bb351210d2","45a9e6e85f544d6abaf81437e78c741f","dcc2ec5b37fa4b80b0c72f24ab5bcf72","8faef5c828b64c2fbf5ed3d30a73a8cd","6610977290f54365997586d6567e668c","7f840f99840c41aab032edaba86bdb40","513aeb3eb72c455d88f296a3303fe51d","a4eae7a0e44f4395871ef9c9ecdba3a3","0365c5e1bbaa44acbb077c5bdf7461fd","a802a4cbe23e4938b13e783f737670df","61530ed0b77a4e5f9c26be8cb759d864","981c5744d82145d39d3dabb88cdf4333","3afa3942bd924152bad0c3dfd3e5dd3f","4ae671de23f3441887cbc7a3953f9c90","75c4e30b3a5643688941d2698f81ed29","60a5b628ad514e1c97f9ca5728fa6dbf","475528ff173f47858b79c97a5a1b15e8","13d1c6d2eeef46f79ca57f6fc4acb0be","8074c6f81d0f403d8dd06b6fc4a7bf83","0cfcea8a22eb4a0d986f1da6e0257ce0","eb0ba123434940768511f97b24d9a65d","a95bc212127f44a2a4462e053b2dd42e","f462f7d962d5405d864068efc8ba384a","334b8fe0a13247578e17e4b56a852122","a36d9401c10b4cd5920cca6979a6b4e8","4a9a9f646b544aa09da77866cf344d2b","66c1cb15102b4d278c01b71f189152be","58fdbfad90444f7e9575d22d25f0b9a1","df43c75037a046f9a64187ba1c94d926","7966c6cded3742a885c79e635f99fb75","9c054219d6ff439e850de50206933433","6e2fdc7055a24ec68ba5d68b60530fa4"]},"id":"c17d828d","outputId":"3d8873a9-e8fa-45a5-aa31-cb7edb86dea0","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:13:40.337136Z","iopub.execute_input":"2025-08-18T07:13:40.337648Z","iopub.status.idle":"2025-08-18T07:13:41.332099Z","shell.execute_reply.started":"2025-08-18T07:13:40.337621Z","shell.execute_reply":"2025-08-18T07:13:41.331321Z"}},"outputs":[{"name":"stdout","text":"Loading Whisper model: openai/whisper-tiny...\nModel loaded. Parameters: 37,760,640\n","output_type":"stream"}],"execution_count":21},{"id":"AD4BqjSoIzOQ","cell_type":"code","source":"# Test preprocessed data structure\ndef test_preprocessed_data():\n    \"\"\"Test that the preprocessed data has the correct structure for training.\"\"\"\n    print(\"Testing preprocessed data structure...\")\n\n    # Get a few samples from the training dataset\n    sample_indices = [0, 1, 2] if len(train_dataset) >= 3 else [0]\n\n    for idx in sample_indices:\n        sample = train_dataset[idx]\n        print(f\"\\nSample {idx} structure:\")\n        for key, value in sample.items():\n            if isinstance(value, torch.Tensor):\n                print(f\"  {key}: tensor with shape {value.shape}, dtype {value.dtype}\")\n            else:\n                print(f\"  {key}: {type(value)}\")\n\n    # Test data collator\n    print(\"\\nTesting data collator...\")\n    data_collator = whisper_trainer.create_data_collator()\n\n    # Try to collate a small batch\n    batch_samples = [train_dataset[i] for i in range(min(3, len(train_dataset)))]\n    try:\n        collated_batch = data_collator(batch_samples)\n        print(\"\\nCollated batch structure:\")\n        for key, value in collated_batch.items():\n            if isinstance(value, torch.Tensor):\n                print(f\"  {key}: tensor with shape {value.shape}, dtype {value.dtype}\")\n            else:\n                print(f\"  {key}: {type(value)}\")\n        print(\"✅ Data collation successful!\")\n        return True\n    except Exception as e:\n        print(f\"❌ Data collation failed: {e}\")\n        return False\n\n# Run the test\ntest_success = test_preprocessed_data()\n\nif test_success:\n    print(\"\\n🎉 Data preprocessing and collation working correctly!\")\n    print(\"Ready to start training...\")\nelse:\n    print(\"\\n⚠️ Issues found with data preprocessing. Please check the errors above.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AD4BqjSoIzOQ","outputId":"ee1e8467-54d9-4e19-917c-bccc030f7788","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:13:41.333053Z","iopub.execute_input":"2025-08-18T07:13:41.333274Z","iopub.status.idle":"2025-08-18T07:13:42.029509Z","shell.execute_reply.started":"2025-08-18T07:13:41.333257Z","shell.execute_reply":"2025-08-18T07:13:42.028900Z"}},"outputs":[{"name":"stdout","text":"Testing preprocessed data structure...\n\nSample 0 structure:\n  input_features: <class 'list'>\n  labels: <class 'list'>\n\nSample 1 structure:\n  input_features: <class 'list'>\n  labels: <class 'list'>\n\nSample 2 structure:\n  input_features: <class 'list'>\n  labels: <class 'list'>\n\nTesting data collator...\n\nCollated batch structure:\n  input_features: tensor with shape torch.Size([3, 80, 3000]), dtype torch.float32\n  labels: tensor with shape torch.Size([3, 448]), dtype torch.int64\n✅ Data collation successful!\n\n🎉 Data preprocessing and collation working correctly!\nReady to start training...\n","output_type":"stream"}],"execution_count":22},{"id":"1ceeec91","cell_type":"code","source":"# Start training\nprint(\"Starting model training...\")\ntrainer = whisper_trainer.train(train_dataset, val_dataset, CONFIG)\nprint(\"Training completed!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ceeec91","outputId":"fb41224d-20af-46e4-d526-37783ba128be","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:13:42.031468Z","iopub.execute_input":"2025-08-18T07:13:42.031722Z","iopub.status.idle":"2025-08-18T07:18:59.749493Z","shell.execute_reply.started":"2025-08-18T07:13:42.031706Z","shell.execute_reply":"2025-08-18T07:18:59.748676Z"}},"outputs":[{"name":"stdout","text":"Starting model training...\nSetting up trainer...\nStarting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='54' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [54/54 05:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saving final model...\nTraining completed!\n","output_type":"stream"}],"execution_count":23},{"id":"114f0b46","cell_type":"markdown","source":"## Module 4: Evaluation","metadata":{"id":"114f0b46"}},{"id":"6110f476","cell_type":"code","source":"class ModelEvaluator:\n    \"\"\"Handles model evaluation and metrics computation.\"\"\"\n    \n    def __init__(self, model_path: str):\n        self.model_path = model_path\n        \n        # Load trained model\n        print(f\"Loading trained model from {model_path}...\")\n        self.model = WhisperForConditionalGeneration.from_pretrained(model_path)\n        self.processor = WhisperProcessor.from_pretrained(model_path)\n        \n        # Fix generation config issues\n        self.model.generation_config.forced_decoder_ids = None\n        self.model.generation_config.suppress_tokens = []\n        \n        # Load WER metric\n        self.wer_metric = evaluate.load(\"wer\")\n        \n        print(\"Model and metrics loaded for evaluation.\")\n    \n    def transcribe_audio(self, audio_features: torch.Tensor) -> str:\n        \"\"\"Transcribe audio features to text.\"\"\"\n        with torch.no_grad():\n            # Generate transcription with explicit parameters to avoid conflicts\n            predicted_ids = self.model.generate(\n                audio_features,\n                max_length=225,\n                num_beams=1,\n                do_sample=False,\n                language=\"en\",  # Force English to avoid language detection\n                task=\"transcribe\",  # Explicit task\n                forced_decoder_ids=None,  # Explicitly set to None\n                suppress_tokens=[],  # Empty suppress tokens\n            )\n            \n            # Decode to text\n            transcription = self.processor.tokenizer.batch_decode(\n                predicted_ids, \n                skip_special_tokens=True\n            )[0]\n            \n            return transcription.strip()\n    \n    def evaluate_dataset(self, dataset: Dataset, max_samples: Optional[int] = None) -> Dict:\n        \"\"\"Evaluate model on a dataset and compute metrics.\"\"\"\n        print(\"Running evaluation...\")\n        \n        predictions = []\n        references = []\n        \n        eval_samples = min(len(dataset), max_samples) if max_samples else len(dataset)\n        \n        self.model.eval()\n        \n        for i in range(eval_samples):\n            try:\n                sample = dataset[i]\n                \n                # Handle input_features - convert to tensor if needed\n                input_features = sample['input_features']\n                if isinstance(input_features, list):\n                    input_features = torch.tensor(input_features)\n                elif not isinstance(input_features, torch.Tensor):\n                    input_features = torch.tensor(input_features)\n                \n                # Ensure correct shape - add batch dimension if needed\n                if len(input_features.shape) == 2:\n                    input_features = input_features.unsqueeze(0)\n                \n                # Handle labels\n                labels = sample['labels']\n                if isinstance(labels, list):\n                    labels = torch.tensor(labels)\n                elif not isinstance(labels, torch.Tensor):\n                    labels = torch.tensor(labels)\n                \n                # Get reference text (decode labels)\n                reference = self.processor.tokenizer.decode(\n                    labels, \n                    skip_special_tokens=True\n                ).strip()\n                \n                # Generate prediction\n                prediction = self.transcribe_audio(input_features)\n                \n                predictions.append(prediction)\n                references.append(reference)\n                \n                if (i + 1) % 50 == 0:\n                    print(f\"Evaluated {i + 1}/{eval_samples} samples...\")\n                    \n            except Exception as e:\n                print(f\"Error processing sample {i}: {e}\")\n                # Add empty results to maintain alignment\n                predictions.append(\"\")\n                references.append(\"\")\n                continue\n        \n        # Filter out empty predictions/references\n        valid_pairs = [(p, r) for p, r in zip(predictions, references) if p.strip() and r.strip()]\n        if valid_pairs:\n            valid_predictions, valid_references = zip(*valid_pairs)\n        else:\n            valid_predictions, valid_references = [], []\n        \n        # Compute WER\n        if len(valid_predictions) > 0:\n            wer_score = self.wer_metric.compute(predictions=list(valid_predictions), references=list(valid_references))\n        else:\n            wer_score = 1.0  # 100% error if no valid predictions\n        \n        results = {\n            'wer': wer_score,\n            'num_samples': len(valid_predictions),\n            'predictions': list(valid_predictions[:5]),  # First 5 for inspection\n            'references': list(valid_references[:5])\n        }\n        \n        return results\n    \n    def print_evaluation_results(self, results: Dict):\n        \"\"\"Print evaluation results in a readable format.\"\"\"\n        print(\"\\n\" + \"=\"*50)\n        print(\"EVALUATION RESULTS\")\n        print(\"=\"*50)\n        print(f\"Word Error Rate (WER): {results['wer']:.4f}\")\n        print(f\"Samples evaluated: {results['num_samples']}\")\n        \n        print(\"\\nSample Predictions vs References:\")\n        print(\"-\"*50)\n        \n        for i, (pred, ref) in enumerate(zip(results['predictions'], results['references'])):\n            print(f\"Sample {i+1}:\")\n            print(f\"  Reference: {ref}\")\n            print(f\"  Prediction: {pred}\")\n            print()\n\n# Initialize evaluator (will load the trained model)\nevaluator = ModelEvaluator(CONFIG['output_dir'])","metadata":{"id":"6110f476","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:21:31.904663Z","iopub.execute_input":"2025-08-18T07:21:31.905368Z","iopub.status.idle":"2025-08-18T07:21:32.701750Z","shell.execute_reply.started":"2025-08-18T07:21:31.905343Z","shell.execute_reply":"2025-08-18T07:21:32.701066Z"}},"outputs":[{"name":"stdout","text":"Loading trained model from ./whisper-medical-asr...\nModel and metrics loaded for evaluation.\n","output_type":"stream"}],"execution_count":26},{"id":"ab01752d","cell_type":"code","source":"# Evaluate on validation set\nprint(\"Evaluating model on validation set...\")\neval_results = evaluator.evaluate_dataset(val_dataset, max_samples=100)  # Limit for speed\n\n# Print results\nevaluator.print_evaluation_results(eval_results)","metadata":{"id":"ab01752d","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:21:37.771004Z","iopub.execute_input":"2025-08-18T07:21:37.771296Z","iopub.status.idle":"2025-08-18T07:23:18.127963Z","shell.execute_reply.started":"2025-08-18T07:21:37.771274Z","shell.execute_reply":"2025-08-18T07:23:18.127168Z"}},"outputs":[{"name":"stderr","text":"`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n","output_type":"stream"},{"name":"stdout","text":"Evaluating model on validation set...\nRunning evaluation...\n","output_type":"stream"},{"name":"stderr","text":"A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\nA custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n","output_type":"stream"},{"name":"stdout","text":"Evaluated 50/100 samples...\nEvaluated 100/100 samples...\n\n==================================================\nEVALUATION RESULTS\n==================================================\nWord Error Rate (WER): 0.1766\nSamples evaluated: 100\n\nSample Predictions vs References:\n--------------------------------------------------\nSample 1:\n  Reference: Remember to follow your healthcare provider's instructions carefully when taking umeclidinium.\n  Prediction: Remember to follow your healthcare providers instructions carefully when taking U-Micladinium.\n\nSample 2:\n  Reference: DUORANDIL is a commonly prescribed medication for individuals with heart conditions.\n  Prediction: Durandil is a commonly prescribed medication for individuals with heart conditions.\n\nSample 3:\n  Reference: It is important to follow the dosage instructions when taking JILAZO to ensure its effectiveness.\n  Prediction: It is important to follow the dosage instructions when taking gelazzo to ensure its effectiveness.\n\nSample 4:\n  Reference: Have you tried Bevon softules for an easy-to-take multivitamin solution?\n  Prediction: Have you tried Bavansoftchels for an easy? To take multivitum, in solution.\n\nSample 5:\n  Reference: Remember to take VISPREDA exactly as directed by your healthcare provider for best results.\n  Prediction: Remember to take Vesprata, exactly as directed by your healthcare provider for best results.\n\n","output_type":"stream"}],"execution_count":27},{"id":"776c084d","cell_type":"markdown","source":"## Module 5: Inference and Testing","metadata":{"id":"776c084d"}},{"id":"32a0e1d5","cell_type":"code","source":"class InferenceEngine:\n    \"\"\"Handles inference on new audio samples.\"\"\"\n    \n    def __init__(self, model_path: str):\n        self.model_path = model_path\n        \n        # Load model and processor\n        print(f\"Loading model for inference from {model_path}...\")\n        self.model = WhisperForConditionalGeneration.from_pretrained(model_path)\n        self.processor = WhisperProcessor.from_pretrained(model_path)\n        \n        # Fix generation config issues\n        self.model.generation_config.forced_decoder_ids = None\n        self.model.generation_config.suppress_tokens = []\n        \n        # Set to eval mode\n        self.model.eval()\n        \n        print(\"Inference engine ready.\")\n    \n    def transcribe_from_features(self, input_features: torch.Tensor) -> str:\n        \"\"\"Transcribe audio from preprocessed features.\"\"\"\n        with torch.no_grad():\n            # Generate transcription with explicit parameters\n            predicted_ids = self.model.generate(\n                input_features,\n                max_length=225,\n                num_beams=2,  # Slightly better quality\n                do_sample=False,\n                temperature=1.0,\n                language=\"en\",  # Force English\n                task=\"transcribe\",  # Explicit task\n                forced_decoder_ids=None,  # Explicitly set to None\n                suppress_tokens=[],  # Empty suppress tokens\n            )\n            \n            # Decode to text\n            transcription = self.processor.tokenizer.batch_decode(\n                predicted_ids, \n                skip_special_tokens=True\n            )[0]\n            \n            return transcription.strip()\n    \n    def transcribe_raw_audio(self, audio_array: np.ndarray, sampling_rate: int) -> str:\n        \"\"\"Transcribe from raw audio array.\"\"\"\n        # Preprocess audio\n        if sampling_rate != 16000:\n            # Resample to 16kHz\n            audio_tensor = torch.from_numpy(audio_array).float()\n            if len(audio_tensor.shape) == 1:\n                audio_tensor = audio_tensor.unsqueeze(0)\n            \n            resampler = torchaudio.transforms.Resample(\n                orig_freq=sampling_rate, \n                new_freq=16000\n            )\n            audio_array = resampler(audio_tensor).squeeze().numpy()\n        \n        # Extract features\n        features = self.processor.feature_extractor(\n            audio_array, \n            sampling_rate=16000, \n            return_tensors=\"pt\"\n        )\n        \n        # Transcribe\n        return self.transcribe_from_features(features[\"input_features\"])\n    \n    def demo_inference(self, dataset: Dataset, num_samples: int = 3):\n        \"\"\"Run demo inference on dataset samples.\"\"\"\n        print(f\"\\nRunning demo inference on {num_samples} samples...\")\n        print(\"=\"*60)\n        \n        for i in range(min(num_samples, len(dataset))):\n            try:\n                sample = dataset[i]\n                \n                # Handle input_features - convert to tensor if needed\n                input_features = sample['input_features']\n                if isinstance(input_features, list):\n                    input_features = torch.tensor(input_features)\n                elif not isinstance(input_features, torch.Tensor):\n                    input_features = torch.tensor(input_features)\n                \n                # Ensure correct shape - add batch dimension if needed\n                if len(input_features.shape) == 2:\n                    input_features = input_features.unsqueeze(0)\n                \n                # Handle labels\n                labels = sample['labels']\n                if isinstance(labels, list):\n                    labels = torch.tensor(labels)\n                elif not isinstance(labels, torch.Tensor):\n                    labels = torch.tensor(labels)\n                \n                # Get reference\n                reference = self.processor.tokenizer.decode(\n                    labels, \n                    skip_special_tokens=True\n                ).strip()\n                \n                # Get prediction\n                prediction = self.transcribe_from_features(input_features)\n                \n                # Display results\n                print(f\"\\nSample {i+1}:\")\n                print(f\"Reference:  {reference}\")\n                print(f\"Prediction: {prediction}\")\n                print(\"-\" * 60)\n                \n            except Exception as e:\n                print(f\"\\nError processing sample {i+1}: {e}\")\n                print(\"-\" * 60)\n\n# Initialize inference engine\ninference_engine = InferenceEngine(CONFIG['output_dir'])","metadata":{"id":"32a0e1d5","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:25:01.298225Z","iopub.execute_input":"2025-08-18T07:25:01.298524Z","iopub.status.idle":"2025-08-18T07:25:01.714837Z","shell.execute_reply.started":"2025-08-18T07:25:01.298506Z","shell.execute_reply":"2025-08-18T07:25:01.714219Z"}},"outputs":[{"name":"stdout","text":"Loading model for inference from ./whisper-medical-asr...\nInference engine ready.\n","output_type":"stream"}],"execution_count":28},{"id":"a073980e","cell_type":"code","source":"# Run demo inference\ninference_engine.demo_inference(val_dataset, num_samples=5)","metadata":{"id":"a073980e","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:25:16.865113Z","iopub.execute_input":"2025-08-18T07:25:16.865706Z","iopub.status.idle":"2025-08-18T07:25:21.424538Z","shell.execute_reply.started":"2025-08-18T07:25:16.865683Z","shell.execute_reply":"2025-08-18T07:25:21.423820Z"}},"outputs":[{"name":"stdout","text":"\nRunning demo inference on 5 samples...\n============================================================\n\nSample 1:\nReference:  Remember to follow your healthcare provider's instructions carefully when taking umeclidinium.\nPrediction: Remember to follow your healthcare provider's instruction carefully when taking you Mickladinium.\n------------------------------------------------------------\n\nSample 2:\nReference:  DUORANDIL is a commonly prescribed medication for individuals with heart conditions.\nPrediction: Durandel is a commonly prescribed medication for individuals with heart conditions.\n------------------------------------------------------------\n\nSample 3:\nReference:  It is important to follow the dosage instructions when taking JILAZO to ensure its effectiveness.\nPrediction: It is important to follow the dosage instructions when taking jillazo to ensure its effectiveness.\n------------------------------------------------------------\n\nSample 4:\nReference:  Have you tried Bevon softules for an easy-to-take multivitamin solution?\nPrediction: \"Have you tried Bavan soft-shells for an easy, to take multivitum, in solution?\" (Ju)\n------------------------------------------------------------\n\nSample 5:\nReference:  Remember to take VISPREDA exactly as directed by your healthcare provider for best results.\nPrediction: Remember to take this spread-a, exactly as directed by your health care provider for best results.\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":30},{"id":"bcc6d968","cell_type":"markdown","source":"## Summary and Next Steps","metadata":{"id":"bcc6d968"}},{"id":"71cd1e2f","cell_type":"code","source":"# Summary of the training run\nprint(\"\\n\" + \"=\"*60)\nprint(\"BASELINE ASR MODEL TRAINING COMPLETE\")\nprint(\"=\"*60)\nprint(f\"Model: {CONFIG['model_name']}\")\nprint(f\"Dataset: {CONFIG['dataset_name']}\")\nprint(f\"Samples used: {CONFIG['num_samples']}\")\nprint(f\"Training epochs: {CONFIG['num_epochs']}\")\nprint(f\"Model saved to: {CONFIG['output_dir']}\")\nprint(f\"Final WER: {eval_results['wer']:.4f}\")\n\nprint(\"\\nNext Steps:\")\nprint(\"1. Fine-tune hyperparameters for better WER\")\nprint(\"2. Increase dataset size for more robust training\")\nprint(\"3. Implement EHR structuring pipeline\")\nprint(\"4. Add domain-specific medical vocabulary\")\nprint(\"5. Evaluate on held-out test set\")\n\nprint(\"\\nModel Files:\")\nimport os\nif os.path.exists(CONFIG['output_dir']):\n    files = os.listdir(CONFIG['output_dir'])\n    for file in files:\n        print(f\"  - {file}\")\nelse:\n    print(\"  Model directory not found.\")","metadata":{"id":"71cd1e2f","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:25:25.495324Z","iopub.execute_input":"2025-08-18T07:25:25.495567Z","iopub.status.idle":"2025-08-18T07:25:25.502709Z","shell.execute_reply.started":"2025-08-18T07:25:25.495552Z","shell.execute_reply":"2025-08-18T07:25:25.501908Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nBASELINE ASR MODEL TRAINING COMPLETE\n============================================================\nModel: openai/whisper-tiny\nDataset: Shamus/United-Syn-Med\nSamples used: 700\nTraining epochs: 3\nModel saved to: ./whisper-medical-asr\nFinal WER: 0.1766\n\nNext Steps:\n1. Fine-tune hyperparameters for better WER\n2. Increase dataset size for more robust training\n3. Implement EHR structuring pipeline\n4. Add domain-specific medical vocabulary\n5. Evaluate on held-out test set\n\nModel Files:\n  - runs\n  - model.safetensors\n  - normalizer.json\n  - vocab.json\n  - generation_config.json\n  - config.json\n  - checkpoint-54\n  - preprocessor_config.json\n  - tokenizer_config.json\n  - training_args.bin\n  - merges.txt\n  - special_tokens_map.json\n  - added_tokens.json\n","output_type":"stream"}],"execution_count":31},{"id":"ebde0f00-c55f-4765-9df3-1bb5d5ad98fd","cell_type":"markdown","source":"## Model downloading \n","metadata":{}},{"id":"a9981da2-a135-4593-8af2-3d440fe9a5e0","cell_type":"code","source":"# Download trained model as ZIP file\ndef download_trained_model():\n    \"\"\"Download the complete trained model as a ZIP file.\"\"\"\n    import zipfile\n    import os\n    \n    # Your model directory\n    model_dir = CONFIG['output_dir']  # './whisper-medical-asr'\n    zip_filename = 'whisper_medical_asr_model.zip'\n    \n    # Create ZIP file with all model files\n    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(model_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                # Add file to zip with relative path\n                zipf.write(file_path, os.path.relpath(file_path, model_dir))\n    \n    print(f\"✅ Model packaged as {zip_filename}\")\n    print(f\"📁 Size: {os.path.getsize(zip_filename) / (1024*1024):.1f} MB\")\n    \n    # Download the file (works in Colab/Kaggle)\n    try:\n        from google.colab import files\n        files.download(zip_filename)\n        print(\"🚀 Download started!\")\n    except:\n        print(\"💡 On Kaggle: Find the file in the output section\")\n        print(\"💡 On local: File saved in current directory\")\n\n# Run this after training\ndownload_trained_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:47:44.289128Z","iopub.execute_input":"2025-08-18T07:47:44.289449Z","iopub.status.idle":"2025-08-18T07:48:15.370054Z","shell.execute_reply.started":"2025-08-18T07:47:44.289427Z","shell.execute_reply":"2025-08-18T07:48:15.369483Z"}},"outputs":[{"name":"stdout","text":"✅ Model packaged as whisper_medical_asr_model.zip\n📁 Size: 529.4 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"download(\"download_9f661718-feed-4ae2-ace0-8dbbaa119873\", \"whisper_medical_asr_model.zip\", 555078991)"},"metadata":{}},{"name":"stdout","text":"🚀 Download started!\n","output_type":"stream"}],"execution_count":32},{"id":"43c69129-a439-437d-914c-91e1763aa13d","cell_type":"markdown","source":"##  Upload to Hugging Face Hub\n","metadata":{}},{"id":"46f4ad86-3f3a-44e3-a591-d21c142711d9","cell_type":"code","source":"# Upload to Hugging Face Hub\ndef upload_to_huggingface():\n    \"\"\"Upload your trained model to Hugging Face Hub.\"\"\"\n    from huggingface_hub import HfApi, create_repo\n    \n    # Your Hugging Face username and desired repo name\n    username = \"Abhijeet17o\"  # Replace with your HF username\n    repo_name = \"whisper-tiny-1000-medical-asr\"\n    \n    # Create repository\n    api = HfApi()\n    repo_url = create_repo(f\"{username}/{repo_name}\", exist_ok=True)\n    \n    # Upload model files\n    api.upload_folder(\n        folder_path=CONFIG['output_dir'],\n        repo_id=f\"{username}/{repo_name}\",\n        repo_type=\"model\"\n    )\n    \n    print(f\"✅ Model uploaded to: https://huggingface.co/{username}/{repo_name}\")\n    print(\"🌐 Now you can load it from anywhere!\")\n\nupload_to_huggingface()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:59:05.646138Z","iopub.execute_input":"2025-08-18T07:59:05.646741Z","iopub.status.idle":"2025-08-18T07:59:16.393909Z","shell.execute_reply.started":"2025-08-18T07:59:05.646717Z","shell.execute_reply":"2025-08-18T07:59:16.393194Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb05bd9abbb4334978c3893113fb8dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 10 LFS files:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f735968ba6f4de99904b5e8ae00564f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"optimizer.pt:   0%|          | 0.00/298M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ff8fb975674c41b447f96623a41c16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2696f5644d44f98bdec18911d565741"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29496cbe896445288eddb9b0d05380da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c09022c22b8e404190a9f909a5a4b5e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"856cd16808a14a5c9e4972cd667f798e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c29a1d335ea2475ab0c05d7be85eca56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1755499978.27923e26b12c.36.0:   0%|          | 0.00/6.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2e3dea37155459b81bde7e281ac2b9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1755501222.27923e26b12c.36.1:   0%|          | 0.00/6.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"673fa2d5dd304292bfd2b18eb8f613c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8057dfafc9d34a7296a5bfe55a578801"}},"metadata":{}},{"name":"stdout","text":"✅ Model uploaded to: https://huggingface.co/Abhijeet17o/whisper-tiny-1000-medical-asr\n🌐 Now you can load it from anywhere!\n","output_type":"stream"}],"execution_count":39},{"id":"9456aa03-6d58-4c06-a801-2d44f1d9872a","cell_type":"code","source":"# Authenticate with Hugging Face\nfrom huggingface_hub import login\nimport os\n\n# Method 1: Direct token input (less secure but quick)\ntoken = \"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\"  # Replace with your actual token\nlogin(token=token)\n\nprint(\"✅ Logged in to Hugging Face!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T07:59:03.622153Z","iopub.execute_input":"2025-08-18T07:59:03.622419Z","iopub.status.idle":"2025-08-18T07:59:03.675409Z","shell.execute_reply.started":"2025-08-18T07:59:03.622400Z","shell.execute_reply":"2025-08-18T07:59:03.674814Z"}},"outputs":[{"name":"stdout","text":"✅ Logged in to Hugging Face!\n","output_type":"stream"}],"execution_count":38},{"id":"8a3a6df2-fb2d-4ab9-9062-9d0b0a22c8d5","cell_type":"markdown","source":"# Base Whisper model to check the WER score","metadata":{}},{"id":"2088033c-ac2f-46ca-aaca-2800debb5b2f","cell_type":"code","source":"# Compare with Normal Whisper Model\nclass BaselineWhisperEvaluator:\n    \"\"\"Evaluate normal Whisper model (not fine-tuned) for comparison.\"\"\"\n    \n    def __init__(self, model_name: str = 'openai/whisper-tiny'):\n        self.model_name = model_name\n        \n        # Load normal Whisper model (not fine-tuned)\n        print(f\"Loading baseline Whisper model: {model_name}...\")\n        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n        self.processor = WhisperProcessor.from_pretrained(model_name)\n        \n        # Fix generation config\n        self.model.generation_config.forced_decoder_ids = None\n        self.model.generation_config.suppress_tokens = []\n        \n        # Load WER metric\n        self.wer_metric = evaluate.load(\"wer\")\n        \n        print(\"Baseline Whisper model loaded for comparison.\")\n    \n    def transcribe_audio(self, audio_features: torch.Tensor) -> str:\n        \"\"\"Transcribe audio features to text using baseline Whisper.\"\"\"\n        with torch.no_grad():\n            # Generate transcription\n            predicted_ids = self.model.generate(\n                audio_features,\n                max_length=225,\n                num_beams=1,\n                do_sample=False,\n                language=\"en\",\n                task=\"transcribe\",\n                forced_decoder_ids=None,\n                suppress_tokens=[],\n            )\n            \n            # Decode to text\n            transcription = self.processor.tokenizer.batch_decode(\n                predicted_ids, \n                skip_special_tokens=True\n            )[0]\n            \n            return transcription.strip()\n    \n    def evaluate_dataset(self, dataset: Dataset, max_samples: Optional[int] = None) -> Dict:\n        \"\"\"Evaluate baseline model on the dataset.\"\"\"\n        print(\"Running baseline Whisper evaluation...\")\n        \n        predictions = []\n        references = []\n        \n        eval_samples = min(len(dataset), max_samples) if max_samples else len(dataset)\n        \n        self.model.eval()\n        \n        for i in range(eval_samples):\n            try:\n                sample = dataset[i]\n                \n                # Handle input_features\n                input_features = sample['input_features']\n                if isinstance(input_features, list):\n                    input_features = torch.tensor(input_features)\n                elif not isinstance(input_features, torch.Tensor):\n                    input_features = torch.tensor(input_features)\n                \n                if len(input_features.shape) == 2:\n                    input_features = input_features.unsqueeze(0)\n                \n                # Handle labels\n                labels = sample['labels']\n                if isinstance(labels, list):\n                    labels = torch.tensor(labels)\n                elif not isinstance(labels, torch.Tensor):\n                    labels = torch.tensor(labels)\n                \n                # Get reference text\n                reference = self.processor.tokenizer.decode(\n                    labels, \n                    skip_special_tokens=True\n                ).strip()\n                \n                # Generate prediction with baseline model\n                prediction = self.transcribe_audio(input_features)\n                \n                predictions.append(prediction)\n                references.append(reference)\n                \n                if (i + 1) % 25 == 0:\n                    print(f\"Evaluated {i + 1}/{eval_samples} samples...\")\n                    \n            except Exception as e:\n                print(f\"Error processing sample {i}: {e}\")\n                predictions.append(\"\")\n                references.append(\"\")\n                continue\n        \n        # Filter out empty predictions/references\n        valid_pairs = [(p, r) for p, r in zip(predictions, references) if p.strip() and r.strip()]\n        if valid_pairs:\n            valid_predictions, valid_references = zip(*valid_pairs)\n        else:\n            valid_predictions, valid_references = [], []\n        \n        # Compute WER\n        if len(valid_predictions) > 0:\n            wer_score = self.wer_metric.compute(predictions=list(valid_predictions), references=list(valid_references))\n        else:\n            wer_score = 1.0\n        \n        results = {\n            'wer': wer_score,\n            'num_samples': len(valid_predictions),\n            'predictions': list(valid_predictions[:5]),\n            'references': list(valid_references[:5])\n        }\n        \n        return results\n\n# Initialize baseline evaluator\nbaseline_evaluator = BaselineWhisperEvaluator('openai/whisper-tiny')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T08:07:57.206343Z","iopub.execute_input":"2025-08-18T08:07:57.206893Z","iopub.status.idle":"2025-08-18T08:07:59.326955Z","shell.execute_reply.started":"2025-08-18T08:07:57.206867Z","shell.execute_reply":"2025-08-18T08:07:59.326245Z"}},"outputs":[{"name":"stdout","text":"Loading baseline Whisper model: openai/whisper-tiny...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f1c536d2ff4e97a7b4f7d5c0fe3755"}},"metadata":{}},{"name":"stdout","text":"Baseline Whisper model loaded for comparison.\n","output_type":"stream"}],"execution_count":40},{"id":"078960ce-c3fa-4605-a864-0947503d0241","cell_type":"code","source":"# Run comparison evaluation\nprint(\"🔄 Evaluating BASELINE Whisper-tiny model...\")\nbaseline_results = baseline_evaluator.evaluate_dataset(val_dataset, max_samples=100)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 MODEL COMPARISON RESULTS\")\nprint(\"=\"*60)\n\nprint(f\"\\n🤖 BASELINE Whisper-tiny (no fine-tuning):\")\nprint(f\"   Word Error Rate: {baseline_results['wer']:.4f} ({baseline_results['wer']*100:.2f}%)\")\nprint(f\"   Samples: {baseline_results['num_samples']}\")\n\nprint(f\"\\n🎯 YOUR FINE-TUNED Model:\")\nprint(f\"   Word Error Rate: {eval_results['wer']:.4f} ({eval_results['wer']*100:.2f}%)\")\nprint(f\"   Samples: {eval_results['num_samples']}\")\n\n# Calculate improvement\nimprovement = baseline_results['wer'] - eval_results['wer']\nimprovement_percent = (improvement / baseline_results['wer']) * 100\n\nprint(f\"\\n📈 IMPROVEMENT:\")\nprint(f\"   Absolute WER reduction: {improvement:.4f}\")\nprint(f\"   Relative improvement: {improvement_percent:.1f}%\")\n\nif improvement > 0:\n    print(f\"   🎉 Your model is {improvement_percent:.1f}% better!\")\nelse:\n    print(f\"   ⚠️ Baseline performed {abs(improvement_percent):.1f}% better\")\n\nprint(\"\\n\" + \"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T08:08:07.384838Z","iopub.execute_input":"2025-08-18T08:08:07.385581Z","iopub.status.idle":"2025-08-18T08:09:34.212602Z","shell.execute_reply.started":"2025-08-18T08:08:07.385555Z","shell.execute_reply":"2025-08-18T08:09:34.211812Z"}},"outputs":[{"name":"stdout","text":"🔄 Evaluating BASELINE Whisper-tiny model...\nRunning baseline Whisper evaluation...\n","output_type":"stream"},{"name":"stderr","text":"You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n","output_type":"stream"},{"name":"stdout","text":"Evaluated 25/100 samples...\nEvaluated 50/100 samples...\nEvaluated 75/100 samples...\nEvaluated 100/100 samples...\n\n============================================================\n📊 MODEL COMPARISON RESULTS\n============================================================\n\n🤖 BASELINE Whisper-tiny (no fine-tuning):\n   Word Error Rate: 0.1892 (18.92%)\n   Samples: 100\n\n🎯 YOUR FINE-TUNED Model:\n   Word Error Rate: 0.1766 (17.66%)\n   Samples: 100\n\n📈 IMPROVEMENT:\n   Absolute WER reduction: 0.0126\n   Relative improvement: 6.7%\n   🎉 Your model is 6.7% better!\n\n============================================================\n","output_type":"stream"}],"execution_count":41},{"id":"c1048f87-2a71-4d36-a37c-b4f1c97cdb0a","cell_type":"markdown","source":"# Comparing the baseline and the finetuned model side by side","metadata":{}},{"id":"1ae5a1b3-0e6b-4fb9-a64f-f5139babbcdd","cell_type":"code","source":"# Side-by-side prediction comparison\ndef compare_predictions(baseline_results, finetuned_results, num_samples=5):\n    \"\"\"Compare predictions side by side.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"🔍 PREDICTION COMPARISON (Baseline vs Fine-tuned)\")\n    print(\"=\"*80)\n    \n    for i in range(min(num_samples, len(baseline_results['predictions']))):\n        print(f\"\\nSample {i+1}:\")\n        print(f\"📝 Reference:   {baseline_results['references'][i]}\")\n        print(f\"🤖 Baseline:    {baseline_results['predictions'][i]}\")\n        print(f\"🎯 Fine-tuned:  {finetuned_results['predictions'][i]}\")\n        print(\"-\" * 80)\n\n# Run the comparison\ncompare_predictions(baseline_results, eval_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T08:09:34.213600Z","iopub.execute_input":"2025-08-18T08:09:34.213848Z","iopub.status.idle":"2025-08-18T08:09:34.219817Z","shell.execute_reply.started":"2025-08-18T08:09:34.213830Z","shell.execute_reply":"2025-08-18T08:09:34.219046Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\n🔍 PREDICTION COMPARISON (Baseline vs Fine-tuned)\n================================================================================\n\nSample 1:\n📝 Reference:   Remember to follow your healthcare provider's instructions carefully when taking umeclidinium.\n🤖 Baseline:    Remember to follow your health care providers instructions carefully when taking you McLidinium.\n🎯 Fine-tuned:  Remember to follow your healthcare providers instructions carefully when taking U-Micladinium.\n--------------------------------------------------------------------------------\n\nSample 2:\n📝 Reference:   DUORANDIL is a commonly prescribed medication for individuals with heart conditions.\n🤖 Baseline:    Durandil is a commonly prescribed medication for individuals with heart conditions.\n🎯 Fine-tuned:  Durandil is a commonly prescribed medication for individuals with heart conditions.\n--------------------------------------------------------------------------------\n\nSample 3:\n📝 Reference:   It is important to follow the dosage instructions when taking JILAZO to ensure its effectiveness.\n🤖 Baseline:    It is important to follow the dosage instructions when taking gelazzo to ensure its effectiveness.\n🎯 Fine-tuned:  It is important to follow the dosage instructions when taking gelazzo to ensure its effectiveness.\n--------------------------------------------------------------------------------\n\nSample 4:\n📝 Reference:   Have you tried Bevon softules for an easy-to-take multivitamin solution?\n🤖 Baseline:    Have you tried Bevan Softchills for an easy? To take multivitum in solution.\n🎯 Fine-tuned:  Have you tried Bavansoftchels for an easy? To take multivitum, in solution.\n--------------------------------------------------------------------------------\n\nSample 5:\n📝 Reference:   Remember to take VISPREDA exactly as directed by your healthcare provider for best results.\n🤖 Baseline:    Remember to take Vesprata exactly as directed by your health care provider for best results.\n🎯 Fine-tuned:  Remember to take Vesprata, exactly as directed by your healthcare provider for best results.\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":42},{"id":"a3e86764-e464-4804-8878-15b38eb67f07","cell_type":"markdown","source":"# Techniques for Major WER Improvements","metadata":{}},{"id":"826929c4-a372-4eab-bdb7-f2ea433f38f6","cell_type":"code","source":"# Advanced Audio Data Augmentation\nimport torchaudio.transforms as T\nimport random\n\nclass MedicalAudioAugmentor:\n    \"\"\"Advanced audio augmentation for medical conversations.\"\"\"\n    \n    def __init__(self, sample_rate=16000):\n        self.sample_rate = sample_rate\n        \n        # Define augmentation transforms\n        self.transforms = {\n            'noise': self.add_background_noise,\n            'speed': self.change_speed,\n            'pitch': self.change_pitch,\n            'volume': self.change_volume,\n            'reverb': self.add_reverb,\n            'bandpass': self.bandpass_filter\n        }\n    \n    def add_background_noise(self, audio, noise_factor=0.005):\n        \"\"\"Add subtle background noise (hospital environment).\"\"\"\n        noise = torch.randn_like(audio) * noise_factor\n        return audio + noise\n    \n    def change_speed(self, audio, speed_factor=None):\n        \"\"\"Change speaking speed (0.9-1.1x).\"\"\"\n        if speed_factor is None:\n            speed_factor = random.uniform(0.9, 1.1)\n        \n        # Use time stretching\n        return torch.nn.functional.interpolate(\n            audio.unsqueeze(0).unsqueeze(0),\n            scale_factor=speed_factor,\n            mode='linear',\n            align_corners=False\n        ).squeeze()\n    \n    def change_pitch(self, audio, n_steps=None):\n        \"\"\"Shift pitch slightly (±2 semitones).\"\"\"\n        if n_steps is None:\n            n_steps = random.uniform(-2, 2)\n        \n        # Pitch shift using resampling approximation\n        shift_factor = 2 ** (n_steps / 12)\n        return self.change_speed(audio, shift_factor)\n    \n    def change_volume(self, audio, volume_factor=None):\n        \"\"\"Adjust volume (0.7-1.3x).\"\"\"\n        if volume_factor is None:\n            volume_factor = random.uniform(0.7, 1.3)\n        return audio * volume_factor\n    \n    def add_reverb(self, audio, room_size=0.1):\n        \"\"\"Add subtle room reverb.\"\"\"\n        # Simple reverb using delay and decay\n        delay_samples = int(0.05 * self.sample_rate)  # 50ms delay\n        decay = 0.3\n        \n        if len(audio) > delay_samples:\n            reverb = torch.zeros_like(audio)\n            reverb[delay_samples:] = audio[:-delay_samples] * decay\n            return audio + reverb\n        return audio\n    \n    def bandpass_filter(self, audio, low_freq=300, high_freq=8000):\n        \"\"\"Apply bandpass filter (telephone quality).\"\"\"\n        # Simulate different recording devices\n        nyquist = self.sample_rate // 2\n        low = low_freq / nyquist\n        high = high_freq / nyquist\n        \n        # Simple frequency domain filtering\n        fft = torch.fft.rfft(audio)\n        freqs = torch.fft.rfftfreq(len(audio), 1/self.sample_rate)\n        \n        # Create bandpass mask\n        mask = (freqs >= low_freq) & (freqs <= high_freq)\n        fft = fft * mask.float()\n        \n        return torch.fft.irfft(fft, n=len(audio))\n    \n    def augment_batch(self, audio_batch, augment_prob=0.8, num_augs=2):\n        \"\"\"Apply random augmentations to a batch.\"\"\"\n        augmented_batch = []\n        \n        for audio in audio_batch:\n            if random.random() < augment_prob:\n                # Apply random augmentations\n                aug_names = random.sample(list(self.transforms.keys()), num_augs)\n                \n                augmented_audio = audio.clone()\n                for aug_name in aug_names:\n                    augmented_audio = self.transforms[aug_name](augmented_audio)\n                \n                augmented_batch.append(augmented_audio)\n            else:\n                augmented_batch.append(audio)\n        \n        return augmented_batch\n\n# Initialize augmentor\naugmentor = MedicalAudioAugmentor()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T08:17:20.654227Z","iopub.execute_input":"2025-08-18T08:17:20.654931Z","iopub.status.idle":"2025-08-18T08:17:20.666262Z","shell.execute_reply.started":"2025-08-18T08:17:20.654895Z","shell.execute_reply":"2025-08-18T08:17:20.665381Z"}},"outputs":[],"execution_count":45},{"id":"05e3f744-9306-4ab7-b5a3-e73345540aef","cell_type":"code","source":"# Medical-Specific Preprocessing\nclass MedicalTextProcessor:\n    \"\"\"Enhanced text processing for medical terminology.\"\"\"\n    \n    def __init__(self):\n        # Common medical abbreviations and their expansions\n        self.medical_abbreviations = {\n            'mg': 'milligrams',\n            'ml': 'milliliters',\n            'bp': 'blood pressure',\n            'hr': 'heart rate',\n            'temp': 'temperature',\n            'wbc': 'white blood cell',\n            'rbc': 'red blood cell',\n            'ecg': 'electrocardiogram',\n            'mri': 'magnetic resonance imaging',\n            'ct': 'computed tomography',\n            'iv': 'intravenous',\n            'po': 'by mouth',\n            'bid': 'twice daily',\n            'tid': 'three times daily',\n            'qid': 'four times daily'\n        }\n        \n        # Common medication name patterns\n        self.medication_patterns = [\n            r'(\\w+)mycin',  # antibiotics\n            r'(\\w+)cillin', # penicillins\n            r'(\\w+)pril',   # ACE inhibitors\n            r'(\\w+)sartan', # ARBs\n            r'(\\w+)olol',   # beta blockers\n            r'(\\w+)statin', # statins\n        ]\n    \n    def normalize_medical_text(self, text):\n        \"\"\"Normalize medical text for better training.\"\"\"\n        text = text.lower().strip()\n        \n        # Expand abbreviations\n        for abbrev, expansion in self.medical_abbreviations.items():\n            text = text.replace(f' {abbrev} ', f' {expansion} ')\n            text = text.replace(f' {abbrev}.', f' {expansion}')\n        \n        # Normalize medication names\n        import re\n        for pattern in self.medication_patterns:\n            matches = re.findall(pattern, text, re.IGNORECASE)\n            for match in matches:\n                # Ensure consistent casing for medication names\n                original = f\"{match}{pattern.split('(')[1].split(')')[1]}\"\n                normalized = original.lower()\n                text = text.replace(original, normalized)\n        \n        return text\n    \n    def add_medical_context(self, text):\n        \"\"\"Add context markers for medical conversations.\"\"\"\n        # Add role markers\n        if any(word in text.lower() for word in ['doctor', 'physician', 'dr']):\n            text = f\"[DOCTOR] {text}\"\n        elif any(word in text.lower() for word in ['patient', 'feel', 'pain', 'hurt']):\n            text = f\"[PATIENT] {text}\"\n        \n        return text\n\nmedical_processor = MedicalTextProcessor()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T08:17:20.667240Z","iopub.execute_input":"2025-08-18T08:17:20.667427Z","iopub.status.idle":"2025-08-18T08:17:20.695953Z","shell.execute_reply.started":"2025-08-18T08:17:20.667413Z","shell.execute_reply":"2025-08-18T08:17:20.695368Z"}},"outputs":[],"execution_count":46},{"id":"487d0121-39b9-491b-867a-74211fef2316","cell_type":"code","source":"# Enhanced Audio Preprocessor with Augmentation\nclass EnhancedAudioPreprocessor(AudioPreprocessor):\n    \"\"\"Enhanced preprocessing with augmentation and medical-specific handling.\"\"\"\n    \n    def __init__(self, model_name: str, target_sample_rate: int = 16000, max_audio_length: float = 30.0):\n        super().__init__(model_name, target_sample_rate, max_audio_length)\n        self.augmentor = MedicalAudioAugmentor(target_sample_rate)\n        self.text_processor = MedicalTextProcessor()\n        \n    def enhanced_audio_processing(self, audio_array: np.ndarray, augment: bool = True) -> np.ndarray:\n        \"\"\"Enhanced audio processing with noise reduction and augmentation.\"\"\"\n        # Convert to tensor\n        audio_tensor = torch.from_numpy(audio_array).float()\n        \n        # Apply spectral noise reduction (simple highpass filter)\n        audio_tensor = self.apply_noise_reduction(audio_tensor)\n        \n        # Apply augmentation during training\n        if augment:\n            # 50% chance to augment\n            if random.random() < 0.5:\n                audio_tensor = self.augmentor.add_background_noise(audio_tensor)\n            if random.random() < 0.3:\n                audio_tensor = self.augmentor.change_volume(audio_tensor)\n            if random.random() < 0.2:\n                audio_tensor = self.augmentor.change_speed(audio_tensor)\n        \n        return audio_tensor.numpy()\n    \n    def apply_noise_reduction(self, audio_tensor: torch.Tensor) -> torch.Tensor:\n        \"\"\"Apply simple noise reduction.\"\"\"\n        # High-pass filter to remove low-frequency noise\n        highpass = T.HighpassBiquad(self.target_sample_rate, cutoff_freq=80)\n        audio_tensor = highpass(audio_tensor)\n        \n        # Normalize amplitude\n        audio_tensor = audio_tensor / (torch.max(torch.abs(audio_tensor)) + 1e-8)\n        \n        return audio_tensor\n    \n    def preprocess_batch(self, batch: Dict, augment: bool = True) -> Dict:\n        \"\"\"Enhanced batch preprocessing with medical text processing.\"\"\"\n        audio_data = []\n        texts = []\n        \n        for i in range(len(batch['audio'])):\n            try:\n                # Handle audio\n                audio_info = batch['audio'][i]\n                audio_array, sampling_rate = self.decode_audio_bytes(audio_info)\n                \n                # Enhanced audio processing\n                audio_array = self.resample_audio(audio_array, sampling_rate)\n                audio_array = self.enhanced_audio_processing(audio_array, augment=augment)\n                audio_array = self.trim_or_pad_audio(audio_array)\n                audio_data.append(audio_array)\n                \n                # Enhanced text processing\n                text_field = \"\"\n                for field in ['text', 'transcription', 'sentence', 'transcript']:\n                    if field in batch and i < len(batch[field]):\n                        text_field = batch[field][i]\n                        break\n                \n                if text_field:\n                    # Apply medical text processing\n                    text_field = self.text_processor.normalize_medical_text(str(text_field))\n                    text_field = self.text_processor.add_medical_context(text_field)\n                \n                texts.append(text_field if text_field else \"\")\n                \n            except Exception as e:\n                print(f\"Error processing sample {i}: {e}\")\n                audio_data.append(np.zeros(int(self.max_audio_length * self.target_sample_rate)))\n                texts.append(\"\")\n        \n        # Process with Whisper feature extractor\n        features = self.feature_extractor(\n            audio_data, \n            sampling_rate=self.target_sample_rate, \n            return_tensors=\"pt\",\n            padding=True\n        )\n        \n        # Enhanced tokenization with medical vocabulary\n        with self.tokenizer.as_target_tokenizer():\n            labels = self.tokenizer(\n                texts,\n                max_length=448,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\"\n            ).input_ids\n        \n        # Replace padding tokens\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        \n        return {\n            \"input_features\": features[\"input_features\"],\n            \"labels\": labels\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T08:17:20.804260Z","iopub.execute_input":"2025-08-18T08:17:20.804486Z","iopub.status.idle":"2025-08-18T08:17:20.815718Z","shell.execute_reply.started":"2025-08-18T08:17:20.804469Z","shell.execute_reply":"2025-08-18T08:17:20.814930Z"}},"outputs":[],"execution_count":47},{"id":"ed0d13c1-fdc5-494e-994e-010fce41c647","cell_type":"code","source":"# Enhanced Training Configuration\nENHANCED_CONFIG = {\n    'dataset_name': 'Shamus/United-Syn-Med',\n    'model_name': 'openai/whisper-tiny',  # Upgrade to small for better performance\n    'num_samples': 1000,  # Increase dataset size\n    'target_sample_rate': 16000,\n    'train_split_ratio': 0.85,  # More training data\n    'output_dir': './whisper-enhanced-medical-asr',\n    'max_audio_length': 25.0,  # Slightly shorter for efficiency\n    \n    # Enhanced training parameters\n    'batch_size': 12,  # Larger batch size\n    'num_epochs': 5,   # More epochs\n    'learning_rate': 3e-5,  # Lower learning rate for stability\n    'warmup_steps': 200,\n    'eval_steps': 300,\n    'save_steps': 600,\n    'gradient_accumulation_steps': 2,\n    \n    # Advanced training techniques\n    'weight_decay': 0.01,\n    'lr_scheduler_type': 'cosine',\n    'gradient_checkpointing': True,\n    'fp16': True,\n    'dataloader_num_workers': 2,\n    \n    # Regularization\n    'dropout': 0.1,\n    'attention_dropout': 0.1,\n}\n\nprint(\"🚀 Enhanced training configuration loaded!\")\nprint(\"Expected improvements:\")\nprint(\"- Data Augmentation: 15-25% WER reduction\")\nprint(\"- Medical Text Processing: 10-20% WER reduction\") \nprint(\"- Larger Model (Small): 8-12% WER reduction\")\nprint(\"- Advanced Training: 5-10% WER reduction\")\nprint(\"- Total Expected WER: 8-12% (vs current 17.66%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T08:17:20.817150Z","iopub.execute_input":"2025-08-18T08:17:20.817380Z","iopub.status.idle":"2025-08-18T08:17:20.840604Z","shell.execute_reply.started":"2025-08-18T08:17:20.817365Z","shell.execute_reply":"2025-08-18T08:17:20.840000Z"}},"outputs":[{"name":"stdout","text":"🚀 Enhanced training configuration loaded!\nExpected improvements:\n- Data Augmentation: 15-25% WER reduction\n- Medical Text Processing: 10-20% WER reduction\n- Larger Model (Small): 8-12% WER reduction\n- Advanced Training: 5-10% WER reduction\n- Total Expected WER: 8-12% (vs current 17.66%)\n","output_type":"stream"}],"execution_count":48},{"id":"1afb9e62-1baa-4505-b605-84e5178e8887","cell_type":"code","source":"# Complete Enhanced Training Pipeline\ndef train_enhanced_model():\n    \"\"\"Train the enhanced model with all improvements.\"\"\"\n    \n    # Initialize enhanced preprocessor\n    enhanced_preprocessor = EnhancedAudioPreprocessor(\n        model_name=ENHANCED_CONFIG['model_name'],\n        target_sample_rate=ENHANCED_CONFIG['target_sample_rate'],\n        max_audio_length=ENHANCED_CONFIG['max_audio_length']\n    )\n    \n    # Load larger dataset\n    enhanced_data_loader = DataLoader(\n        dataset_name=ENHANCED_CONFIG['dataset_name'],\n        num_samples=ENHANCED_CONFIG['num_samples'],\n        train_split_ratio=ENHANCED_CONFIG['train_split_ratio']\n    )\n    \n    # Load and preprocess data\n    print(\"Loading enhanced dataset...\")\n    samples = enhanced_data_loader.load_dataset_subset()\n    train_dataset, val_dataset = enhanced_data_loader.create_train_val_split(samples)\n    \n    # Apply enhanced preprocessing with augmentation\n    print(\"Applying enhanced preprocessing with augmentation...\")\n    train_dataset = train_dataset.map(\n        lambda batch: enhanced_preprocessor.preprocess_batch(batch, augment=True),\n        batched=True,\n        batch_size=8,\n        remove_columns=train_dataset.column_names\n    )\n    \n    val_dataset = val_dataset.map(\n        lambda batch: enhanced_preprocessor.preprocess_batch(batch, augment=False),\n        batched=True,\n        batch_size=8,\n        remove_columns=val_dataset.column_names\n    )\n    \n    # Initialize enhanced trainer\n    enhanced_trainer = WhisperTrainer(\n        model_name=ENHANCED_CONFIG['model_name'],\n        output_dir=ENHANCED_CONFIG['output_dir']\n    )\n    \n    # Train enhanced model\n    print(\"Starting enhanced training...\")\n    trainer = enhanced_trainer.train(train_dataset, val_dataset, ENHANCED_CONFIG)\n    \n    return trainer\n\n# Run enhanced training\nenhanced_trainer = train_enhanced_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T08:17:35.861422Z","iopub.execute_input":"2025-08-18T08:17:35.862233Z","iopub.status.idle":"2025-08-18T08:19:27.707679Z","shell.execute_reply.started":"2025-08-18T08:17:35.862206Z","shell.execute_reply":"2025-08-18T08:19:27.706482Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Loading Whisper processor for openai/whisper-tiny...\nProcessor loaded. Target sample rate: 16000Hz\nLoading enhanced dataset...\nLoading 1000 samples from Shamus/United-Syn-Med...\nAttempting direct parquet loading...\nFound 33 parquet files\nLoading from data/train-00000-of-00033.parquet...\nLoaded 31 samples so far...\nLoading from data/train-00001-of-00033.parquet...\nLoaded 62 samples so far...\nLoading from data/train-00002-of-00033.parquet...\nLoaded 93 samples so far...\nLoading from data/train-00003-of-00033.parquet...\nLoaded 124 samples so far...\nLoading from data/train-00004-of-00033.parquet...\nLoaded 155 samples so far...\nLoading from data/train-00005-of-00033.parquet...\nLoaded 186 samples so far...\nLoading from data/train-00006-of-00033.parquet...\nLoaded 217 samples so far...\nLoading from data/train-00007-of-00033.parquet...\nLoaded 248 samples so far...\nLoading from data/train-00008-of-00033.parquet...\nLoaded 279 samples so far...\nLoading from data/train-00009-of-00033.parquet...\nLoaded 310 samples so far...\nLoading from data/train-00010-of-00033.parquet...\nLoaded 341 samples so far...\nLoading from data/train-00011-of-00033.parquet...\nLoaded 372 samples so far...\nLoading from data/train-00012-of-00033.parquet...\nLoaded 403 samples so far...\nLoading from data/train-00013-of-00033.parquet...\nLoaded 434 samples so far...\nLoading from data/train-00014-of-00033.parquet...\nLoaded 465 samples so far...\nLoading from data/train-00015-of-00033.parquet...\nLoaded 496 samples so far...\nLoading from data/train-00016-of-00033.parquet...\nLoaded 527 samples so far...\nLoading from data/train-00017-of-00033.parquet...\nLoaded 558 samples so far...\nLoading from data/train-00018-of-00033.parquet...\nLoaded 589 samples so far...\nLoading from data/train-00019-of-00033.parquet...\nLoaded 620 samples so far...\nLoading from data/train-00020-of-00033.parquet...\nLoaded 651 samples so far...\nLoading from data/train-00021-of-00033.parquet...\nLoaded 682 samples so far...\nLoading from data/train-00022-of-00033.parquet...\nLoaded 713 samples so far...\nLoading from data/train-00023-of-00033.parquet...\nLoaded 744 samples so far...\nLoading from data/train-00024-of-00033.parquet...\nLoaded 775 samples so far...\nLoading from data/train-00025-of-00033.parquet...\nLoaded 806 samples so far...\nLoading from data/train-00026-of-00033.parquet...\nLoaded 837 samples so far...\nLoading from data/train-00027-of-00033.parquet...\nLoaded 868 samples so far...\nLoading from data/train-00028-of-00033.parquet...\nLoaded 899 samples so far...\nLoading from data/train-00029-of-00033.parquet...\nLoaded 930 samples so far...\nLoading from data/train-00030-of-00033.parquet...\nLoaded 961 samples so far...\nLoading from data/train-00031-of-00033.parquet...\nLoaded 992 samples so far...\nLoading from data/train-00032-of-00033.parquet...\nLoaded 1000 samples so far...\nTrain samples: 850\nValidation samples: 150\nApplying enhanced preprocessing with augmentation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/850 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98d48997d8d04245aee04e567c1a9de9"}},"metadata":{}},{"name":"stdout","text":"Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68ed12b1e46344f19e6c91f46dfe087f"}},"metadata":{}},{"name":"stdout","text":"Error processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 6: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 7: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 0: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 1: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 2: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 3: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 4: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nError processing sample 5: module 'torchaudio.transforms' has no attribute 'HighpassBiquad'\nLoading Whisper model: openai/whisper-tiny...\nModel loaded. Parameters: 37,760,640\nStarting enhanced training...\nSetting up trainer...\nStarting training...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/336461442.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Run enhanced training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0menhanced_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_enhanced_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/336461442.py\u001b[0m in \u001b[0;36mtrain_enhanced_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Train enhanced model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting enhanced training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhanced_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENHANCED_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/99557485.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, config)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving final model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3744\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3745\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3747\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3810\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3811\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Caught ValueError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1694, in forward\n    outputs = self.model(\n              ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1513, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 882, in forward\n    raise ValueError(\nValueError: Whisper expects the mel input features to be of length 3000, but found 2500. Make sure to pad the input mel features to 3000.\n"],"ename":"ValueError","evalue":"Caught ValueError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1694, in forward\n    outputs = self.model(\n              ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1513, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 882, in forward\n    raise ValueError(\nValueError: Whisper expects the mel input features to be of length 3000, but found 2500. Make sure to pad the input mel features to 3000.\n","output_type":"error"}],"execution_count":50}]}